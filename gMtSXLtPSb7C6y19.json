{
  "active": false,
  "connections": {
    "Parse Markdown Pages": {
      "main": [
        [
          {
            "node": "Append or update row (page) in sheet",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "When clicking ‘Execute workflow’": {
      "main": [
        [
          {
            "node": "List files",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "List files": {
      "main": [
        [
          {
            "node": "Filter the book with his Name",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Get a file": {
      "main": [
        [
          {
            "node": "Parse Markdown Pages",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Append or update row (page) in sheet": {
      "main": [
        []
      ]
    },
    "Filter the book with his Name": {
      "main": [
        [
          {
            "node": "Get a file",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "createdAt": "2025-09-06T11:02:24.018Z",
  "id": "gMtSXLtPSb7C6y19",
  "isArchived": false,
  "meta": {
    "templateCredsSetupCompleted": true
  },
  "name": "Book-GPT - WF1.1 - Chunking Markdown vers Google Sheets",
  "nodes": [
    {
      "parameters": {
        "jsCode": "const out = [];\n\nfunction splitByPages(md) {\n  // 1) priorité au délimiteur de page '---'\n  let pages = md.split(/\\n---\\n/g);\n  if (pages.length > 1) return pages;\n\n  // 2) sinon, split par H1 ou H2 en gardant l'en-tête\n  const parts = md.split(/\\n(?=(?:#{1,2}\\s))/g); // avant # ou ##\n  return parts.filter(p => p.trim().length > 0);\n}\n\nfor (const item of items) {\n  const md = item.json.text ?? (item.binary?.data ? Buffer.from(item.binary.data.data, item.binary.data.encoding || 'base64').toString('utf8') : '');\n\n  if (!md) continue;\n\n  const rawPages = splitByPages(md);\n\n  // petit packer pour éviter > ~2k mots par chunk (sécurité)\n  const MAX_CHARS = 12000; // ajustable\n  let pageIndex = 0;\n\n  for (let p of rawPages) {\n    // ne pas couper dans les fences ```…```\n    const segments = [];\n    let buf = '';\n    let inFence = false;\n    for (const line of p.split('\\n')) {\n      const isFence = line.trim().startsWith('```');\n      if (isFence) inFence = !inFence;\n\n      if ((buf + '\\n' + line).length > MAX_CHARS && !inFence) {\n        segments.push(buf.trim());\n        buf = line;\n      } else {\n        buf += (buf ? '\\n' : '') + line;\n      }\n    }\n    if (buf.trim()) segments.push(buf.trim());\n\n    for (const seg of segments) {\n      const pageId = 'PAGE' + String(++pageIndex).padStart(3, '0') + '_' + Date.now();\n      out.push({\n        json: {\n          pageId,\n          text: seg\n        }\n      });\n    }\n  }\n}\n\nreturn out;\n"
      },
      "id": "1d37a253-9e4c-4155-9b88-247a3ac3beb9",
      "name": "Parse Markdown Pages",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        128,
        -448
      ]
    },
    {
      "parameters": {},
      "type": "n8n-nodes-base.manualTrigger",
      "typeVersion": 1,
      "position": [
        -768,
        -448
      ],
      "id": "3fe19cb5-4b90-470b-bf60-a7ca3e058bda",
      "name": "When clicking ‘Execute workflow’"
    },
    {
      "parameters": {
        "authentication": "oAuth2",
        "resource": "file",
        "operation": "list",
        "owner": {
          "__rl": true,
          "value": "gauthiervasseur",
          "mode": "name"
        },
        "repository": {
          "__rl": true,
          "value": "Md_Books",
          "mode": "list",
          "cachedResultName": "Md_Books",
          "cachedResultUrl": "https://github.com/gauthiervasseur/Md_Books"
        },
        "filePath": "data pionnier"
      },
      "type": "n8n-nodes-base.github",
      "typeVersion": 1.1,
      "position": [
        -544,
        -448
      ],
      "id": "9a74c34f-6e44-44b6-a2d0-80f3eebcbbf9",
      "name": "List files",
      "webhookId": "d3036f1d-bb55-4e08-b331-8c25fb308a6e",
      "credentials": {
        "githubOAuth2Api": {
          "id": "EUHRCVUYC415lQGe",
          "name": "GitHub account Abqarimind"
        }
      }
    },
    {
      "parameters": {
        "authentication": "oAuth2",
        "resource": "file",
        "operation": "get",
        "owner": {
          "__rl": true,
          "value": "gauthiervasseur",
          "mode": "name"
        },
        "repository": {
          "__rl": true,
          "value": "Md_Books",
          "mode": "name"
        },
        "filePath": "={{ $json.path }}",
        "additionalParameters": {}
      },
      "type": "n8n-nodes-base.github",
      "typeVersion": 1.1,
      "position": [
        -96,
        -448
      ],
      "id": "24af630a-e5de-432a-aa23-86ec9581010d",
      "name": "Get a file",
      "webhookId": "d85a7af6-cbe1-45e4-a7cd-07185ba6a076",
      "credentials": {
        "githubOAuth2Api": {
          "id": "EUHRCVUYC415lQGe",
          "name": "GitHub account Abqarimind"
        }
      }
    },
    {
      "parameters": {
        "operation": "appendOrUpdate",
        "documentId": {
          "__rl": true,
          "value": "1eS8vwCbPnqtZ5xkUW4pLvINq0MiGWFRrFOwAvgEA7K4",
          "mode": "list",
          "cachedResultName": "New_Book_Processing",
          "cachedResultUrl": "https://docs.google.com/spreadsheets/d/1eS8vwCbPnqtZ5xkUW4pLvINq0MiGWFRrFOwAvgEA7K4/edit?usp=drivesdk"
        },
        "sheetName": {
          "__rl": true,
          "value": 1074483520,
          "mode": "list",
          "cachedResultName": "Book to Translate",
          "cachedResultUrl": "https://docs.google.com/spreadsheets/d/1eS8vwCbPnqtZ5xkUW4pLvINq0MiGWFRrFOwAvgEA7K4/edit#gid=1074483520"
        },
        "columns": {
          "mappingMode": "defineBelow",
          "value": {
            "Page ID": "={{ $json.pageId }}",
            "Status": "Ready For Refine",
            "Text": "={{ $json.text }}",
            "Created At": "={{$now.toISO()}}",
            "Updated At": "={{$now.toISO()}}"
          },
          "matchingColumns": [
            "Page ID"
          ],
          "schema": [
            {
              "id": "Page ID",
              "displayName": "Page ID",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true,
              "removed": false
            },
            {
              "id": "Status",
              "displayName": "Status",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true
            },
            {
              "id": "Text",
              "displayName": "Text",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true
            },
            {
              "id": "New Text",
              "displayName": "New Text",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true,
              "removed": true
            },
            {
              "id": "Updated At",
              "displayName": "Updated At",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true
            },
            {
              "id": "Created At",
              "displayName": "Created At",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "canBeUsedToMatch": true
            }
          ],
          "attemptToConvertTypes": false,
          "convertFieldsToString": false
        },
        "options": {}
      },
      "type": "n8n-nodes-base.googleSheets",
      "typeVersion": 4.7,
      "position": [
        352,
        -448
      ],
      "id": "4d32d11b-351a-4552-8de3-a3db416be4e8",
      "name": "Append or update row (page) in sheet",
      "credentials": {
        "googleSheetsOAuth2Api": {
          "id": "L3VVYCipjZMZeLe6",
          "name": "Google Sheets Ayoub"
        }
      }
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 2
          },
          "conditions": [
            {
              "id": "b692b417-4917-4336-958d-0d6313442f96",
              "leftValue": "={{ $json.name }}",
              "rightValue": "Vasseur_Data pionnier.md",
              "operator": {
                "type": "string",
                "operation": "equals",
                "name": "filter.operator.equals"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.filter",
      "typeVersion": 2.2,
      "position": [
        -320,
        -448
      ],
      "id": "e9718da6-a84e-4109-9aa2-cd42d9e4f2ba",
      "name": "Filter the book with his Name"
    },
    {
      "parameters": {
        "content": "## Documentation Ressource Link\n[**Google Sheet Documentation**](https://docs.n8n.io/integrations/builtin/credentials/google/oauth-single-service/?utm_source=n8n_app&utm_medium=credential_settings&utm_campaign=create_new_credentials_modal)\n[**Github Documentation**](https://docs.n8n.io/integrations/builtin/credentials/github/?utm_source=n8n_app&utm_medium=credential_settings&utm_campaign=create_new_credentials_modal#using-oauth2)\n[**OpenAI Documentation**](https://docs.n8n.io/integrations/builtin/credentials/openai/?utm_source=n8n_app&utm_medium=credential_settings&utm_campaign=create_new_credentials_modal#related-resources)",
        "width": 1200
      },
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        -768,
        -736
      ],
      "id": "4d98ef53-8b82-4cc8-aa84-d56052061169",
      "name": "Sticky Note"
    }
  ],
  "origin": "n8n",
  "pinData": {
    "Append or update row (page) in sheet": [
      {
        "json": {
          "Page ID": "PAGE001_1757157753081",
          "Status": "Ready For Refine",
          "Text": "![](Vasseur_Data pionnier_media/Images/1.png)\n\nDevenez un data pionnier !\n\nGauthier Vasseur\n\nDevenez un data pionnier !\n\nComprendre et exploiter les données en entreprise\n\nAdapté des classes de l’auteur\n\n« Become a Data CEO » du programme Continuing Studies Program de Stanford University\n\n« Seven keys to Master Data » de L’Alliance for Inclusive AI à L’Université de Berkeley en Californie\n\nÀ Marie\n\nAvant-propos\n\nCe livre, j’aurais dû l’écrire il y a vingt ans, quand mon parcours professionnel a croisé celui de Philippe, de Salah, de Marie, et un peu plus tard de Nazhin et de Burton.\n\nJ’avais embrassé une carrière en finance, un peu par passion, certainement beaucoup par convention. J’aimais le côté technique du sujet, ses aspects processus et optimisation. J’y rencontrais toutefois beaucoup de frustrations : je passais beaucoup trop de mon temps à faire des choses que je n’aimais pas, telles que de la collecte manuelle de données, des traitements sous tableurs à rallonge. J’avais aussi le sentiment de ne jamais vraiment être en contrôle : toujours à la limite des échéances de reporting, à la merci d’une erreur ou souvent bien trop loin de la réalité opérationnelle, tout enfoui que j’étais dans mes fichiers. À coups d’effort, de week-ends écourtés et de nuits blanches, j’arrivais à continuer à progresser, à faire de nouvelles choses et à casser la routine et la monotonie d’un hamster dans sa roue. Mais la lumière au bout du tunnel ressemblait plus à une faible ampoule, vue par mes yeux fatigués, qu’à un éclat d’espoir.\n\nParfois, il faut un peu de chance, que j’ai eue et que j’ai combinée avec l’énergie (du désespoir) et une insatiable curiosité. En quelques minutes, j’ai réalisé que je pouvais bouleverser ce statu quo, lorsque deux consultants et experts en analytique sont arrivés dans mon bureau : Philippe et Salah. En recherche de projets, ils avaient été orientés jusqu’à moi par mon directeur informatique, soit par pitié pour moi soit par intérêt de se débarrasser d’eux. À leur rencontre, j’ai été bien inspiré de me défaire des certitudes que m’avaient inculquées mes études. En leur ouvrant mes fichiers Excel (mon cœur) et en partageant mes peines, j’ai lu dans leurs yeux la compréhension profonde de mes souffrances et, surtout, cette étincelle de confiance qui semblait me dire : « Tu connais ton métier, on connaît la data, on est tous prêts à fournir les efforts… C’est déjà gagné ! » Trois ans, une quinzaine d’articles et autant de conférences plus tard, mon rôle s’est transformé. Mieux encore, j’ai vu autour de moi grandir et apprendre une équipe qui avait trouvé cette même passion pour l’analytique et la data. Rien ne nous résistait : le passage à l’an 2000, à l’euro, les normes comptables, le multidevise, le juridique, les dérivés, la trésorerie, la dette, le cash, les budgets : nous transposions tous ces challenges en des processus finement huilés qui nous laissaient le luxe de la réflexion et du temps libre. Marie était comme moi novice en la matière et, malgré plus de trente ans de maison, elle fut la première à me rejoindre dans cette aventure. Nous avons appris ensemble, en combinant ma précipitation à son organisation, mon sens de la vision globale à son attention au détail, mon engouement parfois débridé à sa patience. Marie nous a quittés une veille de noël il y a quelques années. Elle restera pour moi l’une des meilleurs analystes avec lesquels j’ai travaillé, mais surtout la représentation avant l’heure de ce que les profils data sont en train de devenir : une harmonie de compétences pour servir des questions et des challenges métier.\n\nEn 2004, décidément à court de vision long terme, je perdais un peu de ma passion dans une routine qui finissait par s’installer, clôture après clôture, budget après budget. Ma route a croisé Nazhin et Burton, cadres exécutifs d’une société en logiciel décisionnel, Hyperion (rachetée par Oracle en 2006). C’est à une conférence qu’ils m’ont demandé pourquoi je ne ferai pas mon métier de cette passion pour la data. Ils avaient mis le doigt sur ce que j’avais toujours eu en face de moi, mais que j’avais refusé de voir : la data et la technologie étaient mes vraies passions. Depuis mon premier PC en 1983, mes premiers prix en programmation, mon premier jeu publié, mes premiers devoirs rendus sous traitement de texte (qui m’avait valu une convocation chez le proviseur à l’époque), mes nuits à faire des petits boulots, à câbler des PC dans des salles de marché bancaires (pour m’offrir un disque dur ou de la mémoire), la technologie avait toujours été autour de moi. Et je dois cela à mon papa, ingénieur, qui m’avait dès le plus jeune âge associé à ses projets.\n\nC’est alors que j’ai « basculé ». En quelques mois, j’ai abandonné douze ans de carrière en finance pour rejoindre la Silicon Valley, sans garantie, sans réelle visibilité, mais avec la certitude de réaliser un rêve caché en moi depuis si longtemps.\n\nQuinze ans et cinq aventures plus tard entre Hyperion (Business Intelligence), Oracle, Google, TriNet (services RH en ligne), Semarchy (solution de Master Data Management) et un bel échec dans une start-up européenne, je comprenais que la richesse de tout ce que j’avais vécu et l’inspiration de professionnels exceptionnels que j’avais croisés devaient être partagées. De mes premiers cours donnés à l’université de Stanford et à l’Association des financiers professionnels (l’AFP), puis le mentorat avec le Google Launchpad jusqu’à aujour­d’hui, je n’ai cessé de me passionner pour l’enseignement. J’avais vu au Fisher Center for Business Analytics de l’Université de Berkeley tant de mes collègues ou employés grandir et se révéler grâce à la maîtrise de l’analytique : je voulais maintenant partager tout ce que j’avais appris et qui m’avait tant apporté. Et je dois cela à ma maman, institutrice qui m’a certainement donné l’engouement pédagogique.\n\nIntroduction\n\nDans un monde volatile, la capacité des entreprises à se réinventer, à agir vite et à s’adapter constamment est une des clés de survie. Les leviers de cette agilité sont le temps et la visibilité. Le temps permet de se poser les bonnes questions, de réfléchir et d’agir. La visibilité permet de se situer, de savoir d’où l’on vient et où l’on doit se diriger. La transformation digitale n’est donc pas un but en soi. Elle est, par la maîtrise des flux d’information et des gains de temps qu’elle procure, un formidable atout pour réussir (chapitre 1).\n\nPionnier de la data, ou Data Pioneer, n’est pas une terminologie officielle, universelle ou approuvée par tous. C’est une expression que j’ai choisie parce qu’elle représente toutes les facettes de l’analyste d’aujourd’hui.\n\nCe chemin du data pionnier est devenu clé dans un parcours professionnel. Dès lors que les organisations vont acquérir une culture data plus forte, les analystes d’hier devront élever leur niveau de jeu pour répondre à des populations plus au fait et en attente des promesses du digital. Ils devront devenir des pionniers, moteur de progrès, d’amélioration, de découverte. Les analyses jusqu’à présent concentrées au niveau des équipes data vont naturellement être reprises par les salariés devenus Data Citizen, laissant aux analystes des responsabilités nouvelles sur des domaines plus en pointe.\n\nComment pouvons-nous aborder ce tournant et nous équiper des compétences clés pour réussir dans ce rôle émergeant de Pionnier ?\n\nCet ouvrage rassemble tout ce qui m’a permis de bâtir ma connaissance en data et en analytique. Il se concentre sur les fondements impératifs à connaître, il intègre les derniers paradigmes et livre des cas réels, vécus. Il ne s’attardera pas sur les modes et les gadgets non nécessaires au quotidien des professionnels. Il passera également sur les querelles de terminologie et de clocher qui ne sont d’aucune utilité pour ce qui nous rassemblera au cours de ces pages, à savoir résoudre les questions et les challenges du quotidien de manière pragmatique, efficace, éthique et durable.\n\nCe livre a aussi pour but de secouer nos schémas de pensée par des prises de position parfois volontairement paradoxales. En nous étirant de part et d’autre d’une pensée molle et convenue de la data, nous allons pouvoir recalibrer nos expériences vers des positions plus claires et plus saines.\n\nLa principale erreur en analytique est de ne pas savoir ce que l’on cherche. Maîtriser son analytique ne sert à rien si on l’applique à la mauvaise question ou à une question mal posée. Cet exercice délicat s’appuie sur l’expérience, la compétence, la curiosité et une dose de courage tout en maîtrisant ses biais cognitifs et de raisonnement (chapitre 2).\n\nAu cœur de l’analytique est la donnée (chapitre 3). C’est la sève, le sang de tout process. La définir avec précision, en connaître sa nature et les conditions de sa fluidité est la condition sine qua non pour une analytique efficace et pertinente.\n\nL’analytique peut se passer de technologie (chapitre 4), mais les volumes et la complexité de la donnée aujourd’hui nécessitent des outils dédiés. Comprendre les solutions qui nous entourent, choisir et mettre en place celles sur lesquelles nous allons nous appuyer sera nécessaire pour développer des processus performants.\n\nLa donnée est une matière première. Sa capture, son organisation, sa valorisation et son analyse font appel à des techniques à la portée de chacun. 80 % de l’analytique se gagne dans ces phases de préparation de données (chapitre 5).\n\nL’élément le plus complexe de la chaîne analytique, c’est l’humain (chapitre 6). La gestion du changement, l’adaptation de notre leadership, la connaissance de nos biais font partie des outils essentiels de tout projet data.\n\nLorsque technologie, données et humains sont prêts, mettre en harmonie leurs interactions est la clé des processus légers, efficaces et durables (chapitre 7). Les processus Creeper paralysent nos entreprises. Des processus lean font gagner des heures chaque semaine pour nous permettre de nous concentrer sur les missions à valeur ajoutée.\n\nVous avez tous les outils en main. Il faut maintenant commencer. Le dernier pas, c’est avec votre cœur que vous le prendrez. Il vous faudra du cœur pour franchir les obstacles du quotidien de la data, pour faire preuve d’humilité, de curiosité et de collaboration, et pour commencer ce cheminement d’apprentissage. Il vous faudra du cœur pour grandir en architecte et chef d’orchestre de votre donnée et pour faire de votre maîtrise du temps et de l’insight un formidable tremplin pour l’innovation.\n\nLe succès en data repose sur la compréhension de ces fondamentaux. Il s’appuie aussi sur la pratique. L’excellence ne viendra que par la répétition continue de ces gestes et de ces réflexes. Enfin, le progrès que nous ferons sera une fonction de notre engouement, de notre appétit d’apprendre, de notre curiosité et de notre courage devant les défis et les nouveautés que nous rencontrerons.\n\nSi vous avez ce livre, vous démontrez déjà ces qualités. Vous vous démarquez déjà d’une majorité de professionnels qui se contentent de lectures rapides et de mots à la mode. Vous faites la différence entre le dire et le faire. Vous vous présentez aux côtés de ceux qui bâtissent ce que sera la data de demain dans nos entreprises, mais également dans nos sociétés.\n\nAlors, pour votre engagement, pour votre passion, bravo et bienvenue dans un monde de découverte et d’apprentissage sans fin. Bienvenue chez les Pionniers de la data.\n\nCHAPITRE 1  \nLes racines de la transformation digitale\n\nLa direction générale annonce son nouveau projet digital.",
          "Created At": "2025-09-06T13:22:34.020+02:00",
          "Updated At": "2025-09-06T13:22:34.021+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE002_1757157753082",
          "Status": "Ready For Refine",
          "Text": "Ne sommes-nous pas souvent frustrés quand nous voyons quelqu’un justifier une décision, une action ou un comportement… parce qu’elle est « importante », parce qu’elle est « bonne », parce que « tout le monde le fait », parce que « tout le monde en parle » ? La data, l’analytique, la transformation digitale sont des domaines riches en ce type de dogmes. Ces affirmations ne sont pas mauvaises et ont le mérite d’être en ligne avec une tendance observée, validée et pertinente.\n\nNéanmoins, ces raccourcis et simplifications peuvent s’avérer contre-productifs dans une dynamique de changement et d’adhésion au changement. Si, dans le meilleur des cas, nous avons la profondeur de compréhension du sujet et n’éprouvons plus le besoin de nous répéter les détails, la perception de nos propos par notre entourage, elle, va s’appuyer sur ces seuls termes simplistes. Cela peut être suffisant pour commencer une transition, mais cela n’aura plus guère de poids lorsque les premières difficultés apparaîtront.\n\nSi, tout au long de notre courbe d’apprentissage, notre seule motivation est guidée par « parce que c’est important », nous risquons rapidement de lâcher prise. Le « c’est important » doit avoir une substance que nous pouvons comprendre, que nous pouvons appliquer à ce que nous faisons, et surtout un ancrage dans des éléments stratégiques ou tactiques pertinents pour justifier nos efforts. L’objectif de ce chapitre est de fonder l’impératif de la data dans une logique claire et d’apporter des arguments et des exemples pour l’illustrer, afin que nous soyons les meilleurs ambassadeurs de la data.\n\nDepuis mes premiers pas en data, j’ai constamment remis en cause la raison d’être de ma passion. Je voulais être sûr que je ne lui succombais pas en perdant mon pragmatisme et mes obligations professionnelles pour suivre une mode. Je ne suis jamais parvenu à changer d’opinion et j’ai d’ailleurs pris le risque de m’engager dans ce domaine en laissant beaucoup derrière moi, en quittant ce pour quoi j’avais été formé pendant cinq années d’études, puis en me tournant complètement vers l’éducation, abandonnant ma carrière d’entrepreneur. Je ne faisais pas de la data parce que c’était dans l’air du temps, mais parce que j’avais la conviction que c’était un sujet clé, bien avant qu’il devienne Big (Data).\n\nPour comprendre l’importance de la maîtrise de la donnée et de l’analytique aujourd’hui, il faut prendre quelques pas de recul.",
          "Created At": "2025-09-06T13:22:34.022+02:00",
          "Updated At": "2025-09-06T13:22:34.022+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE003_1757157753082",
          "Status": "Ready For Refine",
          "Text": "# Les défis d’un monde en constante évolution\n\nL’annonce du cygne noir.\n\nQuels sont les éléments de succès d’une organisation ? Nous pouvons penser à ses profits, à son cash flow, à la satisfaction client ou à sa domination mondiale. Mais il y a un élément plus fondamental : sa survie ou son développement durable. Qui en effet lancerait une entreprise sans objectif de durer, ou au moins d’avoir la maîtrise de sa durée de vie ?\n\nCe point évident a pendant longtemps été couplé avec la relative assurance que, une fois sur des rails et une taille critique atteinte, une entreprise pouvait se considérer hors de danger. C’était le fameux too big to fail (trop gros pour échouer) qui donnait aux grands groupes mondiaux l’assurance qu’ils vivraient toujours.\n\nC’était sans compter sur l’accélération des économies, sur l’émergence de catastrophes climatiques, politiques, terroristes, boursières, sanitaires plus fréquentes, sur la révolution digitale et ses opportunités exponentielles, et sur l’émergence de nouveaux modèles insufflés par des start-up audacieuses. En quelques décennies, l’environnement de l’entreprise s’est considérablement transformé et s’est rempli d’incertitudes.",
          "Created At": "2025-09-06T13:22:34.023+02:00",
          "Updated At": "2025-09-06T13:22:34.023+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE004_1757157753082",
          "Status": "Ready For Refine",
          "Text": "# Le darwinisme digital\n\nLe darwinisme digital est un concept développé par Bryan Solis. Il transpose la théorie de l’évolution de Charles Darwin au monde de l’entreprise et postule que cette dernière est comme un être vivant : elle doit s’adapter, évoluer, se transformer, si elle veut survivre. Il ajoute également que cette transformation digitale ne doit pas être limitée à la technologie ou à la data, mais qu’elle doit également toucher les effectifs de l’entreprise, le leadership et la culture.\n\nLa fin d’un règne\n\nUne histoire naturelle qui illustre bien l’aspect caduc d’une domination par la taille et la puissance est celle du Mégalodon. Ce requin, aujourd’hui disparu, mesurait plus de 20 mètres de long. C’est certainement le plus grand prédateur que les océans aient connu. Et pourtant, après vingt à trente millions d’années de règne, il s’est éteint. Ce n’est pas la collision de notre planète avec le funeste astéroïde il y a 66 millions d’années qui a mis un terme à son existence. Les paléontologues estiment que c’est sa physionomie et sa physiologie qui sont en cause. Il était trop lent pour capturer suffisamment de nourriture pour sa taille. Les requins de taille plus réduite ainsi que d’autres poissons ou mammifères marins plus agiles ont probablement contribué à limiter son accès à des sources de nourriture. Enfin, sa mobilité plus réduite l’a peut-être empêché de migrer vers des eaux à température viable lors de grands épisodes climatiques de notre Terre.\n\n→ Mais où sont-ils ?\n\nÀ la fin de mes études, me destinant à une carrière financière, je contemplais de nombreuses entreprises alors au pinacle du secteur pour bon nombre de jeunes professionnels. Arthur Andersen et Lehmann Brothers étaient parmi les sociétés les plus en vue. Je m’amuse de voir certains de mes étudiants aujourd’hui ne même pas se souvenir de ces noms. Une de mes statistiques préférées est l’évolution du nombre d’entreprises du Fortune 500 (500 plus grandes entreprises) de 1955 jusqu’à aujourd’hui. Il ne reste que 71 de ces groupes de l’époque dans le classement actuel. Les autres ont été soit dépassés par des évolutions trop rapides ou les aléas de notre monde, soit réduits à néant par la compétition.",
          "Created At": "2025-09-06T13:22:34.027+02:00",
          "Updated At": "2025-09-06T13:22:34.027+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE005_1757157753082",
          "Status": "Ready For Refine",
          "Text": "# Les clés de la survie\n\nDe quelles armes disposent les entreprises pour assurer leur survie ? Même si la tentation est grande de dire « la data », si nous remontons à un niveau plus stratégique, nous nous rendons compte que ce ne sont pas les analyses qui vont sauver les organisations (sauf si l’analytique est leur produit ou service final). Ce qui va compter, qui semble évident et qui pourtant est régulièrement occulté par les mirages de la technologie, c’est :\n\n– l’innovation ;\n\n– l’agilité ;\n\n– la résilience ;\n\n– l’efficacité ;\n\n– la qualité des produits et des services.\n\nNous pouvons facilement nous accorder sur ces points, même si souvent nous demandons à des cabinets de consulting de nous les rappeler à grands frais. Le problème est que, si cette constatation est évidente, voire triviale, engager une organisation sur ces sujets ne se décrète pas. On peut certes apporter des éléments de confort ou de support propices à les encourager ou rédiger de grandes chartes, voire investir dans des toboggans et des baby-foot, mais c‘est rarement suffisant pour déclencher des engagements durables. Travailler la culture de l’entreprise va également être un facteur clé pour engager ces énergies, mais cela reste un travail de longue haleine souvent délicat.\n\nIl existe deux leviers immédiats pour déclencher ces comportements innovants, audacieux et collaboratifs :\n\n– garantir une visibilité de tous les instants sur notre situation ;\n\n– gagner le temps pour réfléchir, échanger et agir.\n\nSi nous savons où nous sommes, d’où nous venons et où nous pouvons aller, si nous avons le temps de nous poser les bonnes questions, de mettre en place une équipe et surtout de prédisposer notre esprit à une réflexion calme et posée, alors nous allons naturellement commencer à entrevoir des solutions nouvelles et certainement meilleures.\n\nMême en n’identifiant des axes de progrès ou des solutions que marginalement supérieurs, la récurrence de ce processus va nous donner un vrai avantage. Les succès sont rarement bâtis sur des coups de génie : ils cachent souvent un travail d’équipe, de multiples échecs et des réajustements de stratégie douloureux. Ce travail itératif est une des clés de la réussite dans toutes les start-up que j’ai connues. Cette même logique s’applique aux équipes, aux départements ou aux divisions d’organisations : nous ne gérons pas avec des coups de poker, mais nous progressons constamment en nous adaptant à ce que nous découvrons et en apprenant de nos échecs.\n\nSi le concept est simple et plutôt intuitif, la question est de savoir comment on déclenche ces comportements et ces dynamiques. Il est évident que l’approche par diktat peut fonctionner à court terme : forcer les équipes dans des réunions de brainstorming, récompenser les nouvelles idées ou encore sanctionner le manque de résultat est un processus simple à mettre en œuvre. Mais qu’en est-il de la substance créée ? La vraie bonne idée ne vient pas d’un cerveau fatigué et sous pression. Elle vient d’une vague d’itérations rapides de pensées d’un groupe collaboratif et se raffine à l’épreuve du temps. C’est toute la valeur qu’apporte une donnée maîtrisée que nous avons vue plus haut : une visibilité claire et pragmatique et des gains de temps significatifs.\n\nVoilà pourquoi grandir nos compétences data et analytiques est clé. En tant qu’individu, le temps et la visibilité nous permettent de valoriser notre expérience. En tant qu’organisation, nous pouvons nous reposer sur des équipes vraiment engagées dans les défis stratégiques, parce que libérées du pensum du data crunching (bricolage manuel de la donnée).\n\n→ Des gains de 90 %\n\nDès mes premiers pas en analytique, je me suis intéressé aux gains de temps produits par le design de processus optimisés (grâce aux principes partagés dans cet ouvrage). Depuis vingt ans, je retrouve les mêmes mesures. Même des processus manuels considérés comme courts (souvent sur tableur), perçus comme efficaces et ne nécessitant que « quelques minutes », finissaient souvent par prendre au moins trente minutes de labeur. Leur version optimisée était opérée sous la barre des trois minutes sans efforts et surtout sans risque de mauvaise manipulation. 90 % de temps de réduction est devenu une norme à laquelle je me suis habitué et que j’ai systématique réinvesti pour continuer à apprendre.",
          "Created At": "2025-09-06T13:22:34.028+02:00",
          "Updated At": "2025-09-06T13:22:34.028+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE006_1757157753082",
          "Status": "Ready For Refine",
          "Text": "# La data est-elle vraiment le nouvel or noir ?\n\nLa data, ce n’est pas toujours le nouvel « or noir ».\n\nCette métaphore est doublement intéressante pour notre réflexion.\n\nBien que séduisante, elle induit une conception erronée, voire contre-productive de la data. Celle-ci en tant que telle ne vaut rien. Nous sommes entourés d’une infinité de données et la majorité ne sont d’aucune utilité à un instant t. Cette illusion de l’Eldorado de la data a conduit de nombreuses entreprises à se lancer dans des courses frénétiques de centralisation d’informations sans but particulier, si ce n’est d’apporter un rassurant : « Nous l’aurons au cas où. » Après des années de collecte et des millions d’euros ou de dollars engloutis, ces lacs de données sont devenus des marais inextricables. Les données jamais contrôlées ou alignées ont perdu de leur pertinence.\n\nAvec le temps, l’intérêt statistique perd de sa valeur et ce ne sont pas toujours les plus grands échantillons qui font la pertinence d’un résultat statistique, car ils introduisent souvent des biais historiques et des problèmes de consistance dans le temps.\n\nEn réalité, la data, comme le pétrole brut au sortir du puits, n’est d’un intérêt que très limité. Si ce dernier n’est pas utilisé, sa valeur devient même négative car son stockage entraîne des coûts. Ce qui va donner sa valeur à l’or noir comme à la data, c’est notre capacité :\n\n– à la transformer et à la raffiner efficacement pour la rendre utilisable par les différents acteurs de la chaîne de décision. Le pétrole brut n’a intrinsèquement que peu d’applications. L’embaumage de momie ou l’étanchéisation de coque de bateau de bois n’étant plus très porteurs, c’est grâce aux techniques de raffinage que l’or noir a pu livrer tant de dérivés utiles à l’industrie et à notre quotidien. La data n’est pas différente. Seule une data préparée, nettoyée, arrangée, agrégée ou connectée peut servir l’analytique. Sans capacité de transformer de manière efficace, durable et précise la data, une organisation ne pourra en tirer qu’une valeur réduite, et ce, au prix d’efforts opérationnels coûteux ;\n\n– à trouver des applications concrètes. Le pétrole brut et les hydrocarbures sont incontournables, car ils sont devenus des sources d’énergie, des briques pour la chimie de composants complexes ou des produits dérivés. On les retrouve ainsi omniprésents dans des applications de notre quotidien. De la même façon, la donnée ne crée de la valeur qu’à partir du moment où elle peut être appliquée à une problématique. Si elle n’est pas attachée à un objectif, elle ne peut être rattachée à une valeur potentielle. Elle devient rapidement un centre de perte.\n\nLe voyageur et le guide\n\nCette fable illustre les paradoxes de certaines courses à la data.  \nPour qui aime voyager, découvrir de nouveaux pays, vivre l’expérience de nouvelles cultures, préparer son voyage est une phase importante. Ce que chacun va chercher dépend de ses centres d’intérêt, de la saison de la visite, du budget et du temps sur place. Un voyage de quinze jours va prendre des mois de préparation et de collecte d’informations.  \nImaginons maintenant une approche similaire à celle que certaines organisations ont choisie à grands frais.  \n« Dès aujourd’hui, je vais récupérer toutes les informations de tous les pays à toutes les saisons, parce que, sait-on jamais, je pourrais un jour être amené à les visiter. Autrement dit, je vais chaque année acheter tous les guides touristiques de la planète que je vais ranger consciencieusement dans ma bibliothèque achetée à cet effet. Je vais même développer un système de gestion de ces ouvrages tant il y a de pays, de régions et d’éditeurs à mettre à jour chaque millésime.  \nLes années passent et je me décide enfin à partir quelques semaines visiter une partie de l’Afrique occidentale. Avec fierté, et non sans en parler à tous les dîners entre amis et collègues, je me mets à compulser les nombreuses années de guides et les multiples éditeurs différents sur le sujet. Une belle masse d’informations qui, sans nul doute, me permettra de faire le voyage parfait.  \nMais la réalité est différente. Les vieux guides sont périmés, certains éditeurs se concentrent sur des thèmes qui ne sont dans mes centres d’intérêt et puis je n’ai pas le temps de faire une analyse en profondeur de dizaines de milliers de pages. Je comprends que mon retour sur investissement temps est nettement plus intéressant si je me concentre sur une sélection récente de guides et que j’emprunte ou achète quelques ouvrages vraiment spécialisés sur ce que je veux voir : la faune, l’histoire, l’art, etc.  \nUn grand sentiment de frustration m’envahit alors : tout cet argent dépensé pour ces milliers de livres, tant de papier gâché. L’in­for­mation pertinente dont j’ai besoin tient finalement sur ma table de nuit et ne m’aura coûté que quelques dizaines d’euros. J’essaie de me rassurer en me disant qu’avec une décennie de guides j’ai accès aux tendances, aux évolutions du pays : mais est-ce là une donnée vraiment pertinente pour ma décision de voyage ?  \nN’est-ce pas une donnée que je peux récupérer en ligne sur des services spécialisés à payer quand j’en ai besoin ? Cette donnée va-t-elle vraiment influencer mes choix de visites ? Finalement, n’est-ce pas ma passion de visiter ce qui me plaît, indépendamment de toute donnée socio-économique ou autre ?  \nDe façon pragmatique, la décision et l’organisation de mon voyage dépendront de facteurs beaucoup plus terre à terre, voire d’éléments d’imprévus qui font partie de toute expédition. Les statistiques et les simulations pointues tomberont rapidement devant la réalité de terrain (cela me fait penser aux plans de bataille à l’armée !). Le succès de mon voyage dépendra principalement d’une information fraîche, ciblée et facilement accessible pour être adaptée aux situations changeantes.  \nAlors, sans rien dire à personne, je vais garder près de moi ces quel­ques guides et préparer mon voyage. Je ne manquerai pas néanmoins de faire visiter ma collection massive de guides “tous les pays depuis quinze ans” dans ma bibliothèque en noyer massif équipée de son système informatique dernier cri d’indexation et de recherche, et de régulièrement publier photos et commentaires pour assurer au monde que tout cela était une bonne idée ! »",
          "Created At": "2025-09-06T13:22:34.029+02:00",
          "Updated At": "2025-09-06T13:22:34.029+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE007_1757157753082",
          "Status": "Ready For Refine",
          "Text": "# Trois illustrations non conventionnelles des bénéfices de la maîtrise de la data\n\nOn ramène souvent l’exercice de la data à celui de l’analyse financière, de la Data Science ou de la mesure d’une performance. La maîtrise de la chaîne analytique a une portée beaucoup plus large illustrée par les trois exemples ci-après.",
          "Created At": "2025-09-06T13:22:34.029+02:00",
          "Updated At": "2025-09-06T13:22:34.030+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE008_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## Protéger ses dirigeants de la prison (Keep your execs out of jail)\n\nPendant plusieurs années, c’était l’objectif numéro 1 des équipes financières d’un grand groupe de la Silicon Valley qui n’avait clairement pas à se soucier de problème de rentabilité. Cette mission l’emportait sur la performance des processus budgétaires (objectif numéro 2) ou la rapidité des clôtures comptables (objectif numéro 3). La maîtrise de la donnée devait se concentrer sur la transparence des processus, l’application des normes et surtout la remontée des informations régulières permettant d’identifier les expositions aux risques pénaux.",
          "Created At": "2025-09-06T13:22:34.030+02:00",
          "Updated At": "2025-09-06T13:22:34.030+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE009_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## Préparer sa survie aux cygnes noirs\n\nLes cygnes noirs sont des événements aléatoires, hautement improbables, qui jalonnent notre vie : ils ont un impact énorme et sont presque impossibles à prévoir1.\n\nLa préparation aux cygnes noirs est par définition impossible. Il convient donc, pour mieux les appréhender, de bâtir des processus super agiles et des analytiques rapides. Par exemple, pour les compagnies aériennes, le 11 septembre aura été un cygne noir, compte tenu des disruptions qu’il a entraînées en termes de baisse du trafic, d’annulations de vols, de hausse des prix des carburants, etc. Comment la data peut-elle jouer un rôle dans ce contexte ? Pour une compagnie aérienne américaine, Southwest Airlines, c’est l’agilité avec laquelle elle a su modéliser ses modèles économiques et simuler ses choix tactiques qui lui ont permis de conserver une activité bénéficiaire. Même lors de la pandémie de Covid-19, la société, qui n’a cette fois pas pu éviter des pertes, s’est appuyée sur son agilité pour s’approprier des itinéraires délaissés par des compagnies en souffrance.",
          "Created At": "2025-09-06T13:22:34.031+02:00",
          "Updated At": "2025-09-06T13:22:34.031+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE010_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## Faire de la data un non-sujet\n\nCette approche a priori paradoxale illustre parfaitement le concept détaillé plus haut : la data n’a intrinsèquement pas de valeur si elle n’est pas appliquée à un problème. Dans le cas de ce manufacturier d’armes du Royaume-Uni, le point de sa directrice financière était de dire que son activité ne présentait pas vraiment d’aléas économiques ou de recouvrement. Par conséquent, les reportings mensuels, les analyses budgétaires et les beaux tableaux PowerPoint n’étaient pas son obsession. Son risque, comme celui de l’entreprise, était la sécurité de la donnée : le vol d’un fichier clients aurait des conséquences graves, de sécurité de portée nationale. Avec son équipe, elle avait fait de la data un « non-problème ». Elle avait réduit le processus à des automatisations parfaitement huilées. Ses équipes passaient un temps minimal à « faire de la data » pour se concentrer sur ce qui comptait : l’assurance que les données de l’entreprise restaient confidentielles et protégées tout au long des transactions.\n\nCes trois exemples illustrent à quel point les enjeux de la data doivent être considérés au-delà de la simple maîtrise de l’exercice analytique. Ce qui nous amène à devenir d’excellents analystes doit rester la résolution des challenges de nos organisations de manière durable.",
          "Created At": "2025-09-06T13:22:34.031+02:00",
          "Updated At": "2025-09-06T13:22:34.032+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE011_1757157753082",
          "Status": "Ready For Refine",
          "Text": "# Comprendre la data dans sa globalité\n\nL’ancrage de la donnée dans une logique métier implique une définition plus large de la data. Réduire ce monde à la composante information est trop réducteur et ne reflète pas sa richesse.\n\nLa data est un monde à cinq facettes indissociables :\n\n– la donnée : les informations qui nous entourent ;\n\n– la technologie qui va nous permettre de traiter les données de manière efficace et durable ;\n\n– les humains qui les bâtissent et leur donnent une raison d’être ;\n\n– les processus qui les orchestrent et les supportent ;\n\n– l’objectif métier ou personnel qui les unit au sein d’un projet.\n\nCette définition simple possède une autre vertu : elle nous rappelle que le succès de nos initiatives digitales repose sur l’équilibre et l’harmonie de ces cinq piliers. Ces derniers nous évitent les visions tunnel qui nous enferment dans des logiques trop simplistes.",
          "Created At": "2025-09-06T13:22:34.032+02:00",
          "Updated At": "2025-09-06T13:22:34.032+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE012_1757157753082",
          "Status": "Ready For Refine",
          "Text": "# Sommes-nous prêts ?\n\nNous pouvons commencer notre cheminement. Nous avons compris que les enjeux de cette quête digitale sont des enjeux pour les sociétés et la société. Les problématiques économiques et humaines seront de plus en plus complexes à résoudre et notre contribution devra être active et éclairée.\n\nSi vous lisez ce livre, vous êtes certainement parmi ceux qui ont compris ces défis. Votre rôle à ce stade est de devenir un ambassadeur de ces idées.\n\nJ’ai longtemps négligé cette mission en me jetant tête baissée dans des problématiques analytiques. Je viens de partager avec vous les recettes qui m’ont aidé à créer l’engouement, l’engagement et surtout la confiance. À vous de jouer !\n\n------------------------------------------------------------------------\n\n1. Taleb N.N., Le cygne noir, Les Belles Lettres, 2011.\n\nCHAPITRE 2  \nCommencer sur de bonnes bases\n\nLa pression pour trouver la bonne question.\n\nAu démarrage, dans la data, nous pouvons nous sentir un peu désarmés devant l’étendue du sujet et la perspective d’un apprentissage sans fin. Néanmoins, même si la destination se dessine au fur et à mesure et que la ligne d’horizon semble s’éloigner sans cesse, la bonne nouvelle est que le point de départ, lui, reste constant dans sa définition.\n\nToute initiative analytique doit commencer par la formulation d’une question ou d’une problématique précise. En apparence simple, ce principe est en fait souvent négligé. Nous allons voir son importance, les raisons qui nous poussent à court-circuiter cette étape et enfin les outils qui permettent de mieux l’aborder.",
          "Created At": "2025-09-06T13:22:34.033+02:00",
          "Updated At": "2025-09-06T13:22:34.033+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE013_1757157753082",
          "Status": "Ready For Refine",
          "Text": "# Pourquoi est-ce capital de poser une question pour commencer ?\n\nIl ne nous viendrait pas souvent à l’idée d’entreprendre une recette de cuisine ou un bricolage sans but précis. Même si un projet peut évoluer dans sa mise en œuvre du fait de contraintes ou d’opportunités, savoir ce pour quoi nous commençons à travailler semble assez naturel. Et pourtant, en pratique, cette règle est régulièrement mise de côté dans les projets analytiques : « Donnez-nous la data, on trouvera bien quelque chose », ou « On va tout stocker pour avoir plus de chance de trouver quelque chose », ou enfin « On va mettre de l’intelligence artificielle et elle nous dira ce qui est important ». Si commencer tout de suite pour rendre des rapports tous azimuts est tentant, car cela démontre une certaine maestria et expose vite des « résultats », cette approche aboutit rarement à une vraie création de valeur. Nous produisons certes des mesures, des tableaux et des graphiques intéressants, mais, comme ils ne répondent pas à des besoins concrets, leur valeur est limitée. On devine rapidement le retour sur investissement négatif de ces projets malgré les apparences.",
          "Created At": "2025-09-06T13:22:34.034+02:00",
          "Updated At": "2025-09-06T13:22:34.034+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE014_1757157753082",
          "Status": "Ready For Refine",
          "Text": "# Pourquoi est-ce difficile de poser une question ?\n\nDe nombreux facteurs de différente nature rendent cet exercice délicat.",
          "Created At": "2025-09-06T13:22:34.034+02:00",
          "Updated At": "2025-09-06T13:22:34.034+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE015_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## L’obstacle des biais cognitifs\n\n« La plupart d’entre nous posons systématiquement les mêmes types de questions, ce qui ne représente que 15 % des possibilités, et ceci afin d’obtenir des réponses qui nous conviennent, ou auxquelles nous sommes habitués », écrit Fabrice Anguenot2.\n\nLes principaux ennemis de la bonne question sont notre habitude, notre routine et notre champ d’expérience.\n\nLes biais cognitifs sont des mécanismes de pensée qui sont à l’origine d’une altération du jugement en nous faisant perdre notre rationnel et notre raisonnement. Lorsque ces biais se produisent, nous ne capturons plus l’information de manière impartiale ou nous ne la traitons plus de manière logique. Ils sont souvent liés à une compréhension limitée d’une situation par manque de temps, d’informations ou de capacités, ou à des facteurs plus personnels tels que des croyances, des habitudes ou de la complaisance avec soi-même3.\n\nNous retrouvons souvent les biais suivants en analytique.\n\n\\> L’aversion à l’ambiguïté ou à l’incertitude\n\nCe biais va nous conduire à favoriser les options que nous connaissons avec leurs risques associés, sans envisager des alternatives à risques inconnus, qui pourraient être moins importants.\n\nCe biais est illustré par le paradoxe d’Ellsberg. Supposons qu’il y ait deux sacs contenant chacun un mélange de 100 boules rouges et noires. Dans le premier sac, nous savons qu’il y a le même nombre de boules de chaque couleur. Dans le second sac, la proportion rouges/noires est inconnue. Nous sommes invités à tirer une boulle dans l’un des deux sacs avec pour objectif de tirer la couleur rouge. Notre aversion à l’ambiguïté et à l’incertitude va nous conduire à préférer tirer la boule du premier sac avec le mélange connu.\n\n\\> L’ancrage\n\nL’ancrage apparaît lorsque nous sommes initialement exposés à une référence qui va ensuite influencer notre jugement.\n\nSi nous sommes dans des discussions budgétaires et que des ordres de grandeur de certains projets marketing que nous analysons sont de l’ordre du million, alors nous serons tentés d’aligner les estimations des autres projets sur des bases similaires.\n\n\\> L’argumentum ad novitatem ou ad antiquitatem\n\nIl s’agit typiquement des raisonnements fallacieux qui nous conduisent à penser qu’une idée est meilleure parce qu’elle est nouvelle et moderne ou l’inverse. Nous allons avoir tendance à sur­évaluer le potentiel ou la qualité de ce qui est récent et à dénigrer les statu quo existants depuis plus longtemps dans le cas du ad novitatem ou de nous retrancher derrière ces mêmes traditions et habitudes dans le cas du ad antiquitatem.\n\nLes organisations qui se veulent tournées vers l’avenir sont des proies faciles pour l’argumentum ad novitatem. Elles vont souvent venir battre en brèche des sagesses validées au profit d’hypothèses futuristes et séduisantes.\n\nL’importance de considérer les biais cognitifs en analytique  \n\nJean Doridot est ingénieur et docteur en psychologie. Il partage son temps professionnel entre son activité de psychologue libéral, ses interventions en entreprise et le développement de l’application de méditation Zenfie, qu’il a créée en 2015. Auteur de plusieurs ouvrages de référence, il enseigne la psychologie en école de commerce et intervient régulièrement dans les médias.  \nQu’est-ce qu’un biais cognitif ?  \nNous avons dans notre crâne un extraordinaire ordinateur. Cette machine formidable calcule incroyablement vite. Elle est capable de mener de front des milliards d’opérations simultanées. Néanmoins, elle est faillible. Sa plus grande faiblesse, c’est également, parfois, sa plus grande force : l’intuition. Cette espèce de faculté à comprendre, et même à prévoir, ce qui se passe et ce qui va se passer.  \nLe problème est qu’il arrive que cette intuition soit complètement fausse. Sur quoi repose le « il est sûr et certain que ce projet va marcher » ? Sur l’analyse complète et exhaustive de données observables, mesurables et vérifiables ? Ou alors sur le désir aveugle d’un entrepreneur plein d’enthousiasme aux convictions communicatives ?  \nIl nous paraît évident que c’est la première option qui doit toujours l’emporter sur la seconde. Et il n’est pas nécessaire d’être un expert pour avoir la conviction que, dès lors que nous entrons dans le monde des affaires, les bons indicateurs, les KPI’s (key performance indicator), indicateurs clé de performance tels que l’EBITDA, devraient prendre le pas sur nos intuitions.  \nVoici une histoire récente qui nous rappelle à une réalité différente.  \nUn jeune homme, sportif, sympa, a un jour la vision de comment le travail de demain va s’organiser. Il envisage des travailleurs nomades et connectés, avec laptop et smartphone comme tout bagage. Bref, il invente les espaces de coworking et crée WeWork. Adam Neumann réussit à lever des sommes colossales, avant que tout ne s’écroule lors de son entrée en Bourse4. Les investisseurs qui ont engagé des sommes folles dans WeWork étaient certainement tous intelligents et rationnels. Ils se fondaient sur des chiffres observables, mesurables et vérifiables. Ils ont sans doute multiplié leurs analyses avant de prendre leurs décisions.  \nCette histoire n’est pas unique. Que s’est-il passé pour les investisseurs de WeWork ? La réponse tient en quelques mots : leurs raisonnements étaient biaisés. L’objectif de cet encart est de détailler un peu plus l’art, o combien délicat, de détecter les biais, et idéalement, de ne pas en faire l’objet  \nLes trois erreurs fondamentales à éviter ou la sainte trinité du data pionnier vigilant  \nBiais 1 : je crois ce que je vois  \nC’est notre plus grande faiblesse. Pour comprendre l’importance de ce biais, imaginez la situation suivante : un étudiant fait les cent pas dehors, juste devant son université. Il vient de terminer son grand oral et attend que les membres du jury l’invitent à entrer pour con­naître la décision : reçu ou recalé. Fébrile, il regarde sa montre, puis allume une cigarette, sur laquelle il tire une profonde et longue bouffée. Imaginons maintenant que nous nous demandions pourquoi ce jeune homme allume une cigarette à ce moment-là ? D’expérience, par ordre d’importance, nous pourrions dire que c’est parce qu’il est stressé, ou qu’il trouve le temps long, ou encore qu’il ne sait pas quoi faire d’autre.  \nÉvidemment, aucune de ces réponses n’est juste. La seule raison qui explique vraiment pourquoi ce jeune allume une cigarette, est la suivante : c’est parce qu’il s’agit d’un fumeur. Car ce même jeune homme, non-fumeur, son jumeau, dans la même situation, par définition, ne fume pas.  \nEt si nous travaillons sur cet exemple, c’est parce que les tabacologues ont prouvé depuis longtemps que le tabac est une plante qui crée du stress, et non le contraire…  \nQue s’est-il passé ? Nous avons vu une personne, en situation de stress, allumer une cigarette. À ce moment-là, notre cerveau, qui cherche toujours des explications à tout, a déduit que c’est ce stress qui a fait allumer une cigarette à ce pauvre impétrant. Alors qu’en réalité le jeune homme en question fume aussi quand il est heureux et qu’il fait la fête avec des amis. Il fume quand il réfléchit sur un problème compliqué. Il fume quand il a du chagrin à cause d’une peine de cœur. En fait, il fume tout le temps, justement parce qu’il s’agit d’un fumeur.  \nNous noterons que, même après cet exemple, nous aurons peut-être du mal à nous convaincre qu’il n’y a aucun rapport entre le stress et la consommation d’une cigarette. C’est une autre caractéristique des biais cognitifs : même quand nous comprenons l’erreur, il est difficile de s’en défaire.  \nExplications  \nLe biais du « Je crois ce que je vois » est très utilisé en marketing et en publicité. Encore aujourd’hui, les marques paient très cher des sportifs, des stars et des influenceurs pour qu’ils s’affichent simplement avec leurs produits. Pourtant, tout le monde sait qu’une paire de baskets ne devient pas meilleure le jour où un grand footballeur signe un accord avec son nouveau sponsor. Mais, ça marche quand même.  \nDans notre métier de data pionnier, la situation sans doute la plus typique des pièges du « Je crois ce que je vois » est la corrélation illusoire. Rien de pire que deux courbes qui nous regardent dans le blanc des yeux, et qui nous prouvent noir sur blanc qu’on ne peut rien en déduire, car, nous le savons certainement, une corrélation ne prouve pas une relation de causalité.  \nAutre exemple, en France, plus les enfants passent du temps devant les écrans, plus leur QI est bas. Cela a été observé maintes et maintes fois. C’est qu’il doit bien y avoir quelque chose, non ? Sans doute, bientôt, l’effet toxique des écrans sur le développement cognitif des enfants sera prouvé, c’est une simple question de temps. Mais ajoutons juste une petite précision avant la suite : à Singapour, c’est le contraire qui est prouvé.  \nSolutions  \nNous l’aurons compris, il est très difficile de lutter contre un biais cognitif. Pour ce qui est du « Je crois ce que je vois », souvenons-nous justement qu’une corrélation n’est en rien une preuve de quoi que ce soit. C’est en ayant conscience que ce biais est bel et bien là que nous développerons la vigilance nécessaire. Bref, souvenons-nous que, si la sagesse populaire s’échine depuis toujours à nous dire que l’habit ne fait pas le moine, c’est parce que, justement, pour notre cerveau, l’habit fait le moine.  \nBiais 2 : l’erreur fondamentale d’attribution  \nC’est sans doute le biais le plus important après le précédent. Retournons pour cet exemple dans les années 1960, lorsqu’un psychologue demande à des psychiatres quelle serait la proportion d’individus, dans la toute la population, capables d’agir en tortionnaires.  \nLes psychiatres de l’époque ont répondu unanimement qu’il ne pouvait s’agir que de personnes pathologiquement perverses et sadi­ques, donc, Dieu merci, moins de 1 %. Le psychologue en question a alors monté une expérimentation dans laquelle plus de 6 personnes sur 10 infligeaient des sévices terribles à de parfaits inconnus pendant une heure, en échange de quelques dollars. Ce psychologue s’appelait Stanley Milgram, et son expérience a fait date. Deux petites précisions supplémentaires :  \n– Stanley Milgram avait demandé à des psychiatres d’évaluer la personnalité de chacun des participants à son expérience. Ils furent déclarés parfaitement équilibrés, ni pervers ni sadiques ;  \n– l’expérimentation en question a été répliquée de nombreuses fois. Récemment, en Pologne, la proportion de sujets allant jus­qu’au bout (une décharge de 450 volts administrée au participant malchanceux) était de 90 %.  \nNous avons tendance à attribuer les actes des uns et des autres à leur personnalité. Pourtant, bien souvent, comme le démontre l’expérience de Milgram, c’est la situation qui explique le mieux ce qui se passe. Ce biais d’attribution peut se décliner de différentes façons :  \n– le biais du champion : il suffit qu’un manager obtienne un résultat exceptionnel pour qu’il soit rapidement débauché par la concurrence. L’année suivante, le succès n’est pas au rendez-vous. La personne recrutée est pourtant la même que l’année précédente. Seul le contexte est différent ;  \n– le produit qu’il vous faut : certaines entreprises font le choix de se doter de solutions qui ne leur conviennent pas, simplement parce que le produit en question est leader sur son marché.  \nSolutions  \nCherchons toujours ce qui ne se voit pas. Lorsque nous analysons des données, amusons-nous à imaginer des paramètres absents, qui pourraient avoir une influence sur les tableaux que nous observons. Notamment, souvenons-nous que la façon la plus sûre d’analyser des données est de recueillir les chiffres de façon expérimentale, avec au moins deux conditions différentes : une condition qui teste notre hypothèse et une condition qui la contrôle.",
          "Created At": "2025-09-06T13:22:34.035+02:00",
          "Updated At": "2025-09-06T13:22:34.035+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE016_1757157753082",
          "Status": "Ready For Refine",
          "Text": "Dans la vraie vie, il n’y a jamais de groupe contrôle. C’est pour cela qu’il est si difficile de devenir un bon data pionnier. Lorsque nous analysons des données, c’est le contexte qui explique souvent le mieux ce qui se passe : succès ou échec d’un produit, taux d’absentéisme, croissance, etc. Cherchons des contextes à peu près similaires et faisons des comparaisons. Même si toutes choses ne sont pas égales par ailleurs, nous aurons quand même un semblant de groupe con­trôle. C’est mieux que rien !  \nBiais 3 : l’influence majoritaire  \nL’influence sociale est un « gros morceau » de la psychologie. Com­ment et pourquoi un individu change d’avis fascine les psys depuis longtemps. Parmi les différentes formes que peut prendre cette influence, la plus simple, et sans doute aussi la plus efficace, reste l’influence majoritaire.  \n  \nDans les années 1950, Solomon Hasch a monté une expérimentation très astucieuse. Il montrait à des groupes de sept étudiants trois segments de droite de tailles différentes, à côté d’un quatrième qui avait, clairement et indiscutablement, la même taille qu’un des trois autres. Plusieurs planches étaient projetées, et chaque sujet devait dire à voix haute quel segment, A, B ou C, était de même taille que le segment anonyme. Au début, tout allait bien, mais, après quelques essais, les six premiers sujets de l’expérimentation donnaient une réponse clairement erronée. Ils étaient en réalité des com­pères de l’expérimentateur, et seul le septième sujet était un sujet naïf. Bien sûr, chaque compère donnait la même mauvaise réponse, comme si cela crevait les yeux. Contre toute attente, trois sujets sur quatre se sont soumis au moins une fois à l’influence majoritaire. Dans plus d’un tiers des cas, les réponses données par les sujets naïfs étaient conformes aux réponses, fausses, données par la majorité.  \nExplications  \nL’homme est un animal, et c’est un animal social. L’une de ses plus grandes angoisses est d’être exclu du groupe, mis à l’écart. Aussi, les participants à l’expérience de Hasch ne sont pas devenus subitement atteints de cécité. Ils ont simplement constaté que quelque chose ne tournait pas rond et que, s’ils ne voulaient pas se mettre à l’écart du groupe, ils avaient intérêt à dire la même chose que tout le monde.  \nDans le monde de l’entreprise, ce biais est souvent désigné par le problème de la « pensée de groupe ». À la problématique du nombre viennent s’ajouter les difficultés des rapports hiérarchiques, et des enjeux économiques et politiques qui rythment la vie de toute entreprise.  \nSolutions  \nAcceptons d’être seuls contre tous. Cultivons en permanence notre esprit critique. Et, surtout, trouvons des alliés. Les expérimentations ont observé que l’influence majoritaire disparaît complètement dès lors qu’il n’y a plus un sujet naïf mais deux. Si nous sommes curieux, penchons-nous sur le sujet de l’influence minoritaire. Nous comprendrons qu’il est souvent plus facile d’influencer une réunion de 50 personnes à deux que de lutter seul contre l’avis de trois autres.  \nMes recommandations pour mitiger nos biais  \nNous devons absolument retenir que les biais cognitifs sont comme les effets d’optique. Même lorsque nous sommes au courant, l’illusion est toujours là. Personne n’en est exempt.  \nÉtablissons des check-lists, avant toute prise de décision. Lorsque nous tentons d’expliquer des résultats, recherchons systématiquement trois histoires cohérentes et différentes. Cela forcera notre esprit à élargir sa vision et à aller chercher ce qui ne se voit pas. N’hésitons pas à aller chercher des résultats fiables, et contradictoires les uns par rapport aux autres. Intéressons-nous aux succès des grandes entreprises, et aussi à leurs échecs les plus cuisants. Dans les deux cas, faisons l’inventaire des biais qui ont été à l’œuvre.  \nMes conseils aux data pionniers  \nUne erreur terrible est de croire que celui qui peut expliquer le passé peut faire des prévisions sûres et certaines sur l’avenir. Il n’y a rien de plus faux. Tout le monde aujourd’hui peut se dire qu’il n’est pas si compliqué que ça d’inventer la roue. Il a pourtant fallu à nos ancêtres plus de 60 000 ans pour y parvenir… et 1 000 ans supplémentaires pour penser à faire une brouette.  \nDans le quotidien du data pionnier, nous allons devoir souvent expliquer le passé. Nous allons devoir aussi prévoir l’avenir, le prédire. Souvenons-nous par conséquent que les mêmes causes produisent toujours les mêmes effets. Mais restons vigilants, et ne cherchons pas à déterminer trop vite les causes.  \nJe terminerai par un conte oriental. À la tombée de la nuit, un homme scrute le sol très attentivement, à quatre pattes sous un lampadaire.  \nUn mendiant qui passe par là lui demande ce qu’il fait. Il lui répond : « J’ai perdu les clés de chez moi, alors je les cherche pour entrer dans ma maison. Aide-moi donc ! » Tandis que les deux hommes sont maintenant en train de regarder partout autour du lampadaire, le mendiant demande encore : « Tu es sûr que tu les as perdues là tes clés ? » « Pas du tout », répond l’autre homme. « Mais alors pourquoi tu les cherches ici ? » Et l’homme de répondre : « C’est parce que sous le lampadaire est le seul endroit où il y a de la lumière. »",
          "Created At": "2025-09-06T13:22:34.036+02:00",
          "Updated At": "2025-09-06T13:22:34.036+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE017_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## La perception des données disponibles\n\nParce que certaines questions requièrent des données nouvelles ou inhabituelles, nous sommes tentés de ne pas les poser. Ou de manière plus sournoise, nous posons la question en nous efforçant d’y faire correspondre une mauvaise réponse.\n\n→ Les mauvaises données pour une bonne question\n\nDans un groupe international de grande distribution et de restauration, le contrôle de gestion produit voulait vérifier les opérations de lancement pour un nouveau produit. Alors que le responsable avait commencé par une question ouverte pour lancer la réflexion, « comment allons-nous mesurer l’exécution opérationnelle de ce lancement ? », il fut interrompu par les deux jeunes analystes dédiés à la marque qui claquèrent une réponse au ton définitif : « Il suffit de mesurer le chiffre d’affaires et la marge générée, c’est simple. » Une analyse rapide de cette évidence montrait qu’elle ne correspondait pas à la problématique posée. Ce n’était pas les ventes qui intéressaient le responsable, même si un lancement réussi a un impact favorable sur les métriques financières. Il voulait savoir si le lancement était correctement mené. Son objectif était donc de mesurer si les moyens mis en œuvre étaient suffisants pour atteindre les cibles définies par le marketing en termes de démographie et de géographie. La mesure des ventes ne serait connue que dans plusieurs semaines, voire des mois, trop tard pour corriger une campagne mal gérée.\n\nLes analystes insistèrent d’un ton un peu plus vindicatif : « On ne va pas y passer la journée, on mesure les ventes et voilà ; de toute façon, nous n’avons que cette donnée-là facilement accessible. » Le vice-président de la marque se tourna alors vers ses deux collaborateurs et leur demanda froidement : « Attendez, vous êtes en train de me dire que vous allez répondre à une question par la mauvaise réponse, parce que les éléments qui pourraient donner la bonne ne sont pas directement accessibles ? » Un grand blanc s’installa dans la salle. À la suite de quelques échanges destinés à ne faire perdre la face à personne, le groupe arriva finalement à trouver les bons indicateurs.\n\n» Qu’auriez-vous proposé ? Auriez-vous choisi d’aller contre un statu quo imposé si rapidement dans la réunion ?\n\nVoici quelques-uns des indicateurs qui ont été finalement arrêtés pour ce lancement de produit de grande distribution et de restauration/café/club :\n\n– nombre de commerciaux sur le terrain et leur répartition géographique pendant la campagne ;\n\n– nombre de commerces et de distributeurs démarchés pour la présentation du nouveau produit ;\n\n– type de commerces visités ;\n\n– taille des zones de chalandise touchées ;\n\n– nombre d’échantillons distribués ;\n\n– nombre de promotions laissées sur le lieu de vente.\n\nCes données ont semblé a priori impossibles à collecter du fait de leur originalité ou de l’absence de prise en charge dans les systèmes marketing. Avec une simple compréhension des fonctionnalités de base des outils analytiques et de la nature de ces data, le problème de collecte, de consolidation et de reporting (voir chapitre 5) a été résolu dans l’après-midi qui a suivi, sans nécessiter d’investissement.",
          "Created At": "2025-09-06T13:22:34.036+02:00",
          "Updated At": "2025-09-06T13:22:34.037+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE018_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## La compréhension des mécaniques analytiques\n\nLorsque l’analytique est considérée comme une boîte noire, il est délicat d’en connaître ses limites et ses opportunités. Le cas précédent souffrait un peu de cette lacune de connaissance de la part des protagonistes. Ces limitations créent d’autres entraves à la formulation de questions pertinentes. Voici une liste non exhaustive de schémas de pensée autolimitants qui sont pourtant résolus avec des dispositifs d’analyse simples et accessibles à tous.\n\nTableau 1. Quelques schémas de pensée autolimitants faciles à corriger\n\nBien sûr, dans ces exemples, la réponse d’un analyste formé mériterait d’être argumentée et pourrait ne pas couvrir certaines exceptions. C’est néanmoins le niveau de confiance et celui de compétences auxquels nous arriverons par la mise en pratique des éléments de cet ouvrage. Ce bagage de compétences et de connaissance va libérer notre questionnement. Les seuls challenges impossibles seront ceux pour lesquels la donnée est trop chère à acquérir ou interdite d’accès, l’équipement trop onéreux, la maturité de l’équipe insuffisante ou le temps imparti trop court (voir chapitres 4, 5, 6 et 7).",
          "Created At": "2025-09-06T13:22:34.037+02:00",
          "Updated At": "2025-09-06T13:22:34.037+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE019_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## La pression de l’habitude\n\nPartiellement illustré dans un exemple précédent, cet obstacle pernicieux nous empêche de formaliser des questionnements nouveaux de peur de choquer ou de déstabiliser une organisation adepte de la routine. Pourquoi bousculer les habitudes et éveiller les susceptibilités quand des générations d’analystes se sont contentées, avec un certain succès, de reproduire les schémas passés ?\n\n» Avez-vous déjà passé en revue les reportings de votre équipe, minutieusement publiés mois après mois, année après année ? Avez-vous déjà essayé de les faire évoluer ?",
          "Created At": "2025-09-06T13:22:34.038+02:00",
          "Updated At": "2025-09-06T13:22:34.038+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE020_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## Le courage (ou son manque)\n\nPoser une nouvelle question ou surtout une question pertinente évitée jusqu’à présent revêt deux risques :\n\n– elle va déranger, être considérée comme impertinente par ceux que le statu quo arrangeait ;\n\n– elle nous expose en nous mettant dans l’obligation d’y répondre au risque de mettre au grand jour des conclusions non désirées.\n\n→ Des conclusions parfois dangereuses\n\nUn contrôleur financier faillit perdre son emploi pour avoir osé suggérer la mesure de la rentabilité d’instruments financiers sur le long terme, alors que ces derniers avaient apporté gloire et bonus à leur initiateur. Sa motivation était pure : son rôle était de suivre la valorisation des actifs du groupe. Dès le premier jour de mise en place des analytiques de suivi, il sentit une pression croissante sur ces rapports (de nos jours, on appellerait cela du harcèlement). Il n’eut pas le droit de les publier en dehors de l’équipe, et pour cause : trois mois après, les bénéfices des instruments avaient disparu et les appels de marge se mesuraient en millions. La révélation de ces chiffres aurait pu lui coûter son emploi : tout cela pour une question peut-être courageuse, et certainement fondée.",
          "Created At": "2025-09-06T13:22:34.039+02:00",
          "Updated At": "2025-09-06T13:22:34.039+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE021_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## La tentation du voisin et de la bonne pratique\n\nLes projets analytiques ont besoin d’une reconnaissance rapide. Parfois nous manquons d’imagination, de temps et de courage pour poser la bonne question (voir supra, Le courage (ou son manque)). Pour pallier cela, nous pouvons céder à la tentation de produire des analyses et des indicateurs en masse au détriment de leur pertinence. Il suffit d’aller voir des confrères dans l’industrie et de copier les analyses standards du métier. Cela procure rapidement du travail pour les architectes de la donnée et donne l’illusion de produire de la valeur. Il y a bien sûr des métriques classiques que nous devrions toujours avoir, mais cette approche comporte aussi des risques : leur production peut créer une vision tunnel et un sentiment de travail accompli. Elle risque de nous limiter dans une zone de confort et de ne pas développer des analyses qui comptent.\n\nLa vision tunnel (tunel vision)\n\nLa vision tunnel décrit la manière avec laquelle nous pouvons nous engager dans des décisions en renonçant à aller au-delà de notre perspective individuelle.\n\nLes analyses à valeur ajoutée seront celles qui répondent vraiment à une problématique de notre entreprise, pas à celle de notre voisin.\n\n→ Nos besoins ne sont pas ceux du voisin\n\nDes équipes d’analystes avaient développé des analyses en pointe dans leur domaine. Le temps gagné grâce à l’efficacité de leurs processus était réinvesti en partie dans l’investigation de nouveaux problèmes. Leurs modèles de données étaient extrêmement raffinés et précis. À de nombreuses reprises, des confrères et des consultants vinrent leur demander de partager leurs KPI et leur structure de données. L’entreprise avait volontiers partagé certaines de ces informations (sans les données dedans bien évidemment), tout en sachant que cela risquait fort d’être contre-productif pour leur acquéreur. Sa situation économique (très précaire à l’époque), sa stratégie, ses contraintes humaines, data, techniques et ses processus l’avaient conduite à développer une analytique très spécifique, avec des biais volontaires en termes de choix de métrique et d’organisation de données. Elle était focalisée sur la gestion de la dette avec ses marchés principaux en Asie, et des processus étaient très en pointe sur la partie bancaire. C’était là une situation singulière, absolument pas applicable aux confrères du domaine.\n\nL’entreprise acquéreur était en excédent de trésorerie, en plein développement aux États-Unis, très manuelle, en grand manque de compétences. Cette copie d’analytique n’a jamais pris, faute de pertinence, mais elle a occupé tout le monde pendant dix-huit mois !\n\nPoser la bonne question nécessite l’exercice de nombreuses qualités professionnelles et personnelles. Se connaître soi-même, maîtriser sa data et sa chaîne analytique et une petite dose de courage sont des fondements que nous devons acquérir pour challenger les statu quo.\n\n→ Commencer par chercher ce que l’on recherche\n\nLe centre que je dirige à Berkeley (Fisher Center for Business Analytics) collabore à de nombreux projets d’étude ou de recher­che avec des entreprises partenaires. Le point d’achoppement est toujours celui de la question. Même dans le contexte de recherche, il est impératif de savoir ce que l’on recherche. Des professeurs de premier plan, qui pourraient sans doute faire de la magie en analytique, s’obstinent avec raison à obtenir cette fameuse question avant de démarrer tout projet. Bien souvent, ce qui paraît un exercice évident est beaucoup plus complexe. Il n’est pas rare de refuser des projets ou d’aboutir à de faux problèmes en raison de l’absence de questions pertinentes.",
          "Created At": "2025-09-06T13:22:34.040+02:00",
          "Updated At": "2025-09-06T13:22:34.040+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE022_1757157753082",
          "Status": "Ready For Refine",
          "Text": "# Comment poser la bonne question\n\nLa nature et le contexte des questions changeant constamment, les techniques de questiologie, autrement dit l’art de poser la bonne question, pour optimiser la formulation des questions peuvent nous aider.\n\nLe pouvoir des questions  \n\nSophie Chamayou Degoix est Executive coach certifiée HEC, spécialisée en intelligence émotionnelle, questiologie et programmation neuro-linguistique (PNL). Après un parcours de plus de vingt ans en négociation, marketing et communication, elle décide de mettre ses compétences au service des dirigeants et managers et cofonde Intuitik, cabinet de coaching, formation et conseils. Son accompagnement est basé sur la conviction que chaque être humain peut exprimer son plein potentiel et générer un impact positif sur son environnement et sur le monde.  \nComment pouvez-vous aider votre interlocuteur à grandir, à s’enrichir, à changer de posture ? Comment pouvez-vous modifier le comportement et l’engagement de vos collaborateurs, de vos clients ? Comment pouvez-vous obtenir des résultats plus impactants ?  \nL’une des clés est de formuler des questions suffisamment puis­santes et confrontantes pour provoquer chez votre interlocuteur un « bug cérébral » qui va lui permettre de reconsidérer ses certitudes et d’explorer d’autres chemins.  \nSocrate, avec la maïeutique (l’art d’accoucher les âmes), ne cessait de questionner ses disciples afin de les inciter à remettre en question leurs idées habituelles. Ses questions démontaient les concepts qui sous-entendaient les argumentations pour pousser encore plus loin les réflexions et développer la pensée critique. D’ailleurs, Socrate offrait ses questions comme des cadeaux à ses disciples, ce qui les rendait plus importantes que les réponses elles-mêmes.  \nEinstein, également, portait une attention particulière aux questionnements : « Si je disposais d’une heure pour résoudre un problème et que ma vie en dépende, je consacrerais les 55 premières minutes à définir la question appropriée à poser, car, une fois cela fait, je pourrais résoudre le problème en 5 minutes. »  \nIl a, ainsi, découvert la théorie de la relativité en sortant des schémas de pensée habituels et en se posant une question qui a complètement modifié notre compréhension de l’univers et de la gravité : « À quoi l’univers ressemblerait-il si je me déplaçais à la vitesse de la lumière ? »  \nUne question pertinemment posée peut donc bousculer notre mode de pensée et être à l’origine d’une découverte révolutionnaire.  \nComment distinguer une question classique d’une question puissante ?  \nTout d’abord, une question puissante est une question ouverte, c’est-à-dire qu’elle donne la possibilité d’avoir une infinité de réponses. Avec de puissantes questions ouvertes, ni le demandeur ni le répondant ne connaissent la réponse. Elles permettent, donc, si elles sont aidantes, de faire émerger une idée inédite en activant la créativité de l’interlocuteur. Elles impliquent les personnes et développent la coopération et l’engagement.  \nMais, force est de constater que, dans notre quotidien, 80 % de nos questions sont fermées. Elles commencent soit par un verbe, soit par « Est-ce que » ou « Y a-t-il », ou bien encore par une assertion négative « Ne pensez-vous pas que ». Ce sont des questions qui attendent une réponse par « oui » ou « non » et qui illustrent, la plupart du temps, l’expression de nos propres pensées.  \nEn posant des questions fermées, votre intention est de valider votre mode de pensée (« Ne pensez-vous pas que le télétravail est source de responsabilisation ? »), de prendre le contrôle (« Pouvez-vous relancer vos clients ? ») ou de fixer un cadre (« Êtes-vous d’accord ? »). Ce type de questionnement ne vous permet pas d’élargir votre compréhension du monde, et ne développe pas non plus le niveau de réflexion de votre interlocuteur, voire il le déresponsabilise et le désengage.  \nEn transformant vos questions fermées en questions ouvertes, vous transformez également la posture de votre interlocuteur. Par exemple, « Pouvez-vous relancer vos clients ? » induit une réponse déresponsabilisante avec un « oui » ou un « non ». Par contre, si on la modifie en « Qu’allez-vous mettre en place comme actions pour ne plus avoir à relancer vos clients ? », alors elle prend une autre dimension et redonne du pouvoir à l’interlocuteur. De simple exécutante, la personne qui répond va modifier son état d’esprit et utiliser ses capacités de réflexion et de créativité pour trouver sa propre solution au problème.  \nUne question puissante va ainsi vous permettre de sortir de vos schémas de pensée habituels et d’envisager d’autres champs des possibles. Si vous posez toujours les mêmes questions, vous obtiendrez toujours les mêmes réponses et vous serez limité dans vos possibilités. Alors que, en questionnant vos certitudes, vous bannissez vos réponses définitives, celles qui vous confortent dans vos croyances, dans votre vision du monde et vous limitent dans votre développement. Vous découvrez qu’il n’y a plus une seule vérité mais plusieurs, à partir desquelles en jaillissent d’autres. La bonne question devient un catalyseur de développement d’idées, stimulatrice de créativité.  \nIl est d’ailleurs frappant de constater que les leaders les plus créatifs, ceux qui contribuent aux changements du monde, sont ceux qui (se) posent le plus de questions. Elon Musk, par exemple, privilégie le questionnement aux réponses : pour aborder l’innovation, il commence par formuler la question qu’il souhaite adresser. Il développe ensuite des hypothèses, qu’il reconsidère une nouvelle fois par une autre question, jusqu’à ce que la réponse soit « probablement vraie ». Il remet donc sans cesse en question ses idées, ses réponses, ses conclusions, sans tenir compte des évidences, des croyances et des habitudes.  \nUn questionnement donnant-donnant  \nUn bon questionnement va déterminer également la qualité de vos relations. Questionner va vous permettre de rentrer en contact avec quelqu’un, d’appréhender son monde et d’enrichir le vôtre. Cela vous permet de comprendre qu’il n’y a pas une seule vérité, que chacun a des perceptions différentes de la réalité, et vous accueillez celles des autres comme point de départ d’une nouvelle réflexion. En privilégiant la connexion et les échanges constructifs, vous comprenez mieux les besoins et les attentes de l’autre personne et vous pouvez donc mieux y répondre. Vous développez votre empathie et votre tolérance.  \nUn autre objectif du questionnement puissant est de développer votre capacité à impliquer votre interlocuteur et à le mettre en mouvement. Les questions posées vont l’aider à trouver son chemin et à faire son premier pas. Par exemple, avec des questions confrontantes, vous pouvez faire bouger les référentiels : « Quelles nouvelles habitudes allez-vous mettre en place pour développer un état d’esprit positif ? » ou « Comment votre singularité va contribuer à rendre ce projet exceptionnel ? ».  \nD’autre part, pour éviter les justifications, vous devez limiter les questions commençant par « Pourquoi », sauf si vous demandez à votre interlocuteur d’y répondre avec « pour » (et non pas « parce que »). En faisant cela, vous allez chercher sa motivation, son sens et non pas une justification qu’il se sent obligé de donner et qui le place en position défensive. Par exemple, à la question « Pourquoi faites-vous comme ça ? », préférez « Quel est le bénéfice de faire comme ça ? ».  \nPour développer sa motivation et aller chercher ses valeurs, vous pouvez poser une autre question puissante comme « Qu’est-ce qui est important pour vous ? » ou « Quand vous aurez atteint votre objectif, qu’est-ce que cela va vous apporter de plus d’important ? ». Vous obtiendrez ainsi la valeur essentielle pour la personne, celle à laquelle elle va se raccrocher et se mobiliser pour atteindre son objectif.  \nDe l’importance du temps du verbe dans la question  \nLe temps utilisé dans les verbes des questions est également très important et peut renforcer leur impact. Bannissez le conditionnel qui ne donnera que des réponses conditionnelles et donc pas engageantes ni responsabilisantes. Privilégiez le futur qui permet de se projeter dans un état désiré. Au lieu de demander « Que feriez-vous si vous n’aviez plus peur ? », préférez « Que ferez-vous quand vous n’aurez plus peur ? ». Projeter quelqu’un dans un futur de réussite avec une visualisation est extrêmement puissant pour l’inciter à l’action et développer sa motivation : « Comment saurez-vous que vous avez réussi ? Que ressentirez-vous ? Qu’observerez-vous ? Que vous direz-vous ? Avec qui serez-vous ? »  \nEn posant des questions au passé, vous permettez à votre interlocuteur d’identifier et de comprendre les causes de son problème.  \nEnfin, les questions au présent vont inciter votre interlocuteur à mettre en place les actions nécessaires pour concrétiser son objectif.  \nDonc, si vous souhaitez débloquer une situation et trouver des pistes d’actions concrètes, commencez par poser des questions au passé (les causes), puis projetez la personne dans la réussite de son objectif avec des questions posées au futur (la visualisation) et, enfin, ramenez-la au présent en statuant sur les actions à mener.  \nLa Questiologie  \nUne autre technique pour poser des questions puissantes est la méthode de la Questiologie© créée par Frédéric Falisse qui, après avoir modélisé 3 800 questions intéressantes, en a extrait une logique et des critères communs. Cette méthodologie propose de développer une stratégie de questionnement en vous adaptant, dans un premier temps, au profil de votre interlocuteur (les « quatre locus ») :  \n– « observateur » : celui qui va observer, analyser ;  \n– « méta » : celui qui va réfléchir, modéliser, théoriser ;  \n– « introspectif » : celui qui va ressentir des émotions ;  \n– « acteur » : celui qui va agir.  \nGénéralement, nous avons chacun un ou deux locus de prédilection.  \nEn vous adaptant au profil de votre interlocuteur, vous développez une meilleure connexion avec lui, vous débloquez des situations et vous lui permettez de trouver des idées novatrices. Vous développez ainsi votre agilité relationnelle.  \nSi, par exemple, un collaborateur vient vous voir, très en colère, en vous disant qu’il ne supporte plus la relation avec son manager, l’objectif, dans un premier temps, sera de diminuer sa charge émotionnelle en lui faisant prendre de la distance par rapport à son problème et de lui permettre de mieux réfléchir. Puisqu’il est dans un locus « introspectif » (il ressent une émotion), vous allez l’emmener vers un locus « observateur » (analyser la situation factuellement) : « Pour vous, c’est quoi une bonne collaboration ? » Puis, pour l’aider à prendre encore plus de hauteur et à développer sa réflexion, vous allez lui poser une question du locus « méta » : « À quoi pensez-vous quand vous imaginez cette collaboration ? » L’interlocuteur, qui a pu ainsi prendre de la distance par rapport à son émotion, peut maintenant envisager des actions concrètes et réfléchies. Vous pouvez alors décider de lui poser une question « d’acteur » qui va l’engager : « Que pouvez-vous faire pour obtenir une bonne collaboration ? » Et, enfin, pour ancrer sa décision et évaluer sa charge émotionnelle, vous pouvez finir par une question de type « introspectif » : « Comment vous sentez-vous avec ça ? » Avec cette stratégie, vous incitez votre interlocuteur à modifier ses schémas de pensée habituels et à trouver de nouvelles pistes de réflexion.",
          "Created At": "2025-09-06T13:22:34.043+02:00",
          "Updated At": "2025-09-06T13:22:34.043+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE023_1757157753082",
          "Status": "Ready For Refine",
          "Text": "Vous pouvez aussi adapter votre stratégie de questionnement en fonction des cinq étapes de raisonnement de votre interlocuteur. Comme ce ne sont pas les mêmes connexions neuronales qui sont sollicitées, cela permet de développer différentes capacités réflexives. En posant des questions sur chacune de ces étapes de raisonnement, appelées « gestes mentaux », vous allez « bousculer » son cerveau pour qu’une nouvelle connexion se mette en place et génère une nouvelle réponse :  \n– description des choses ;  \n– perception, appropriation des choses ;  \n– évaluation des options ;  \n– sélection des options, choix ;  \n– intégration des solutions dans une perspective plus large.  \nEn questionnant sur le même sujet mais de manière différente, en adaptant vos questions au profil de l’interlocuteur, vous multipliez votre propension à poser des questions puissantes et vous démultipliez donc votre capacité à obtenir des réponses différentes, plus réfléchies, plus créatives, plus impliquantes.  \nPour résumer  \nSavoir poser des questions puissantes va donc vous permettre de :  \n– créer du lien ;  \n– d’enrichir la perception du monde de chacun ;  \n– de clarifier un objectif ;  \n– de motiver, d’engager ;  \n– de donner du sens ;  \n– de mettre en mouvement ;  \n– de stimuler une réflexion profonde ;  \n– d’explorer les options ;  \n– de développer l’intelligence collective.  \nIl s’agit donc d’un véritable enjeu de communication, de leadership et de développement humain à la fois individuel et collectif.  \nL’art de poser des questions, et donc d’obtenir des réponses plus impactantes, est ce qui fait la différence dans un monde de plus en plus incertain où l’agilité, l’engagement, la créativité, l’empathie et la coopération sont des compétences comportementales (soft skills) très recherchées.\n\nIl est également important de privilégier les dynamiques de réflexion qui s’adaptent rapidement. Pour poser la bonne question, il s’agit de :\n\n– rester humble. La première étape peut être de se mettre en situation de ne pas savoir. Cela va éviter de s’enferrer dans un biais dès le départ et forcer l’ouverture à des suggestions extérieures ;\n\n– poser les cinq pourquoi. Cette pratique empruntée à l’industrie automobile part d’un problème observé pour remonter vers la cause initiale (root cause) par une succession de « pourquoi » ;\n\n– aller Gemba (qui signifie « le lieu où les choses se passent » en japonais) ! Ce concept lui aussi emprunté à l’industrie automobile consiste à se rapprocher au plus près du terrain et, dans notre contexte, à affiner la question auprès de ceux qui font et qui savent ;\n\n– s’appuyer sur la diversité. S’entourer d’une grande diversité de profils minimise les biais et enrichit la réflexion avec le maximum d’idées ;\n\n– viser le cygne noir. Confronter la pertinence de la question à l’interrogation « Qu’est-ce qui pourrait nous tuer (figurativement) demain ? » permet de se concentrer a priori sur ces risques fatals. Si nous constatons que la problématique que nous cherchons à résoudre est complètement déconnectée des risques qui pourraient précipiter la faillite, est-ce bien cette problématique que nous devons résoudre en premier ?\n\n– cartographier notre savoir. Le champ des possibles est contenu dans cette matrice que Donald Rumsfeld, secrétaire d’État sous George Bush Jr, avait rendue célèbre.\n\nFigure 1. Matrice de Rumsfeld\n\nCe que nous savons que nous connaissons est notre zone de confort et fait partie de notre routine. Nous devons éviter de nous y complaire. Nous nous contenterons d’en contrôler régulièrement la validité par des reportings automatisés pour mieux nous consacrer à ce que nous savons que nous ne connaissons pas. Ce sont des questions qui nous attendent et sur lesquelles nous n’avons pas encore su ou eu le temps de nous pencher. Enfin, il y a ce que nous ne savons pas, que nous en soyons conscients ou non. Nous pouvons le découvrir en faisant appel à des processus analytiques empiriques, mais surtout par des échanges, des collaborations avec nos confrères, nos mentors, par nos lectures en dehors de nos domaines de pré­dilections. Ce temps, gagné grâce à l’efficacité de nos processus analytiques, est notre meilleur moteur de créativité dans tous les domaines, car il nous permettra de nous poser et de créer ces moments de réflexion pour aller découvrir ce que nous ne savons pas que nous ne savons pas.",
          "Created At": "2025-09-06T13:22:34.044+02:00",
          "Updated At": "2025-09-06T13:22:34.044+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE024_1757157753082",
          "Status": "Ready For Refine",
          "Text": "# Mettre la question au service de l’action\n\nJustifier un investissement en temps, voire technologique, dans le domaine décisionnel est toujours délicat. Une solution, une question ou un challenge opérationnel peut être valorisé en fonction d’un gain ou d’une économie souvent quantifiable. S’il n’a pas de valeur, le projet n’a pas de raison d’être. Si un projet analytique résout un problème concret, une valeur peut être estimée et il sera alors parfaitement possible de faire le calcul du retour sur investissement à l’envers et d’en déduire l’investissement maximal. S’il résout un problème à 1 000 000 d’euros, un projet à 200 000 euros sera plus facilement acceptable. Si le processus manuel ou sur tableur peut être réduit de cinq heures par semaine, alors un projet d’une quarantaine d’heures pour l’optimiser peut sembler acceptable. Ces logiques ne sont pas encore souvent mises en œuvre et l’histoire est jonchée d’échecs de projets data cuisants et coûteux, car trop centrés sur la technologie ou la data.",
          "Created At": "2025-09-06T13:22:34.045+02:00",
          "Updated At": "2025-09-06T13:22:34.045+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE025_1757157753082",
          "Status": "Ready For Refine",
          "Text": "# Pour conclure\n\nNous voilà équipés pour commencer nos projets analytiques sur le bon pied. Parce que cet exercice de questionnement est délicat et qu’il ne peut pas être résolu à coups d’achat de logiciels ou d’accumulation de données, nous pouvons être tentés de le négliger. De plus, le temps passé à réfléchir ne se matérialise pas aux yeux d’un management impatient, au contraire d’un ballet de feuilles de tableur et de graphiques qui, même sans pertinence, font toujours leur effet lors des réunions d’équipe.\n\nCette première étape conditionne pourtant les résultats futurs parce qu’il ne sert à rien de trouver la bonne réponse à la mauvaise question.\n\n------------------------------------------------------------------------\n\n2. Anguenot F., La questiologie ou l’art de se poser des questions, [www.lettreducadre.fr](http://www.lettreducadre.fr), 21 janvier 2015.\n\n3. Voir aussi : Meyer O, Daniel Kahneman et les biais cognitifs, RSE Magazine, 21/02/2020.\n\n4. Un quotidien français, Libération, a alors publié un papier titré « Le baratineur qui valait 47 milliards » le 16 novembre 2019.\n\nCHAPITRE 3  \nComprendre la donnée dans l’entreprise\n\nL’annonce de la perte de toutes les données de l’entreprise.\n\nQuelle est donc cette fameuse donnée ? On nous parle de Big Data, de données clients, de données privées, mais savons-nous bien visualiser ce qu’elle est vraiment ? Ce chapitre va clarifier le concept, présenter sa structure quasi immuable, les différentes méthodes de base pour la traiter, et enfin comment respecter les normes et l’éthique autour de la data.",
          "Created At": "2025-09-06T13:22:34.045+02:00",
          "Updated At": "2025-09-06T13:22:34.046+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE026_1757157753082",
          "Status": "Ready For Refine",
          "Text": "# Qu’est-ce que la donnée pour l’analytique en entreprise ?\n\nLa réponse à cette question a bien évolué depuis un quart de siècle. Les noms ou expressions qui vont suivre ont tous été utilisés pour définir la data, sans jamais vraiment le faire correctement. L’évolution des technologies et les biais des mentalités dans notre domaine ont régulièrement empêché de considérer la data pour ce qu’elle est.\n\nDans les années 1980, la data en analytique était souvent considérée comme l’information contenue dans les logiciels. De nature financière ou opérationnelle, son utilisation était principalement limitée au monde de l’entreprise ou de la science. Elle restait principalement un domaine de techniciens. Les mondes financiers, marketing, commerciaux, voire même parfois opérationnels, regardaient avec dédain ceux qui y mettaient les mains. Les premières solutions d’aide à la décision par l’analytique avaient une adoption limitée.\n\nLe début des années 2000 a mis la data sur le devant de la scène en lui affublant un qualificatif aussi imposant qu’effrayant : le Big Data. De grandes entreprises telles que Google, Yahoo et autres acteurs de l’Internet avaient commencé à collecter et à analyser toutes les données du monde, quelles qu’elles soient, pour ensuite en bâtir des propositions de valeur pour le grand public. En l’espace de quelques années, la data prit un caractère plus universel. Elle pouvait être beaucoup de choses, et le terme de « données non struc­turées » permettait de savamment faire référence aussi bien aux photos, aux voix et aux textes.\n\nLa mode du Big Data s’est ensuite estompée. Aujourd’hui, l’intelligence artificielle et sa cohorte de termes comme Machine Learning, Deep Learning (voir chapitre 5, Les domaines du Big Data) sont venus supplanter la notion de Data dans les esprits. La data est moins ce qu’elle est, mais plus ce que l’on en fait.\n\nTout au long de cette évolution, la plupart du temps, seuls les techniciens avaient une compréhension pragmatique de ce qu’est la data. Les métiers se contentaient souvent de mentionner ces termes plus sexy, sans jamais vraiment apprendre leur nature. Dans les paragraphes qui vont suivre, nous allons combler ces manques et démontrer que non seulement cette compréhension est accessible à tous, mais qu’elle est aussi nécessaire pour l’analytique.",
          "Created At": "2025-09-06T13:22:34.046+02:00",
          "Updated At": "2025-09-06T13:22:34.046+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE027_1757157753082",
          "Status": "Ready For Refine",
          "Text": "# Comment appréhender la data",
          "Created At": "2025-09-06T13:22:34.047+02:00",
          "Updated At": "2025-09-06T13:22:34.047+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE028_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## La data, c’est « tout »\n\nJ’aime à me remémorer cette magnifique chanson des Pink Floyd, extrait de l’album Dark Side of the Moon, dont les paroles donnent une définition de la data très juste (sans le faire exprès). Il y a une infinité de datas, où que nous soyons.\n\nLa data doit être considérée comme pouvant être tout ce qu’il y a de visible et d’invisible dans notre univers. En évitant toute limitation a priori, nous évitons la réduction du champ des possibles dans la sélection de nos éléments de réponse. Nous évitons ainsi de ne retenir que les réponses pour lesquelles nous avons de la donnée. À la question « combien y a-t-il de data dans une pièce, dans une entreprise, dans un pays ? », la réponse doit être par principe « une infinité ».\n\n→ Les données qui nous entourent\n\nPar exemple dans un bureau nous trouverons :\n\n– le nombre de prises électriques ;\n\n– la température de l’air ;\n\n– le volume de papier ;\n\n– la couleur des livres sur le bureau ;\n\n– le compte des caractères du brouillon du dernier e-mail ;\n\n– l’âge moyen des collaborateurs passés nous voir dans la journée, etc.\n\nLa liste est sans fin.\n\n» Quelle est votre liste de données ? Prenez le temps de considérer les données les plus simples ou les plus farfelues autour de vous.",
          "Created At": "2025-09-06T13:22:34.048+02:00",
          "Updated At": "2025-09-06T13:22:34.048+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE029_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## Identifier quelle data est importante\n\nLa clé est dans le point de départ en analytique : la question.\n\nOui, il y aura toujours une infinité de données autour de nous, mais ce champ se réduira fortement dès que notre question sera posée. Ensuite, le temps et le retour sur investissement réduiront encore un peu plus le champ des possibles, car certaines données seront trop coûteuses à extraire ou à raffiner. Cependant, grâce à notre connaissance et à notre pratique du sujet, nous ne nous autolimiterons pas à notre petite zone de confort et nous pourrons aller capturer des données au-delà des limites précédentes.\n\nReprenons l’exemple des données dans un bureau, choisissons-en quelques-unes et regardons a posteriori quelle question pourrait les rendre pertinentes et si leur collecte était économiquement valide. La position retenue sera bien sûr très personnelle et pas forcément pertinente pour tous.\n\nTableau 2. Comment choisir la collecte des données dans un bureau ?\n\nPour chacun de ces exemples, nous avons pu rapidement nous concentrer sur une partie palpable de l’univers de data autour de nous, grâce à la question posée.\n\n→ Comment expliquer l’échouage de cétacés ?\n\nIl y a quelques années, les côtes océaniques européennes avaient connu une recrudescence d’échouage de cétacés qui mouraient par dizaines sans raison apparente. Un journaliste demanda à un scientifique s’il allait faire du Big Data et de l’IA pour tenter d’expliquer ces événements. La réponse fut un exemple de pragmatisme : « je connais mon métier, on connaît les grands facteurs qui désorientent les cétacés, on va déjà mesurer ces éléments. Pas la peine de faire bouillir l’océan. »\n\nAucune donnée ne saurait être écartée de nos champs d’analyse si elle est pertinente. Le problème est que toutes les datas ne sont pas facilement et économiquement accessibles, voire pas du tout, du fait de leur nature (ce que nous avons en tête par exemple) ou de leur complexité (les interactions entre les masses d’air, les océans dans les études climatiques, par exemple). Pour être valorisée, une donnée devra être captée dans un format utilisable.",
          "Created At": "2025-09-06T13:22:34.048+02:00",
          "Updated At": "2025-09-06T13:22:34.049+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE030_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## Collecter une donnée pour une utilisation efficace, précise et durable\n\nLa diversité des formats est quasi infinie, par exemple du plus classique au moins conventionnel :\n\n– le rapport papier qui peut contenir textes et images ;\n\n– le format électronique avec ses différents types de fichiers (texte, images, vidéo, etc.) ;\n\n– le support spécifique : le film, la voix, le son, le mouvement, la pierre, l’argile et tout ce qui peut être imprimé ;\n\n– la donnée cérébrale : nos pensées (que l’on pourrait ramener à des impulsions électriques ?).\n\nEn 2015, les principes FAIR ont établi les caractéristiques requises pour qu’une donnée puisse être traitée, échangée ou analysée avec un minimum de friction. L’acronyme FAIR représente quatre termes :\n\n– Findable (trouvable) : les données doivent pouvoir être identifiées sans ambiguïté, correctement décrites, simples à chercher ;\n\n– Accessible : les données peuvent être extraites avec des protocoles standardisés, (idéalement) universels, qui permettent si nécessaire l’application de protocoles d’identification ;\n\n– Interopérable : les données utilisent un langage formel, accessible, partagé et largement applicable pour la représentation des connaissances ;\n\n– Réutilisable : les données sont publiées avec une licence d’utilisation des données claire et accessible. Les données sont associées à une provenance détaillée et répondent à des normes communautaires pertinentes pour le domaine.\n\nCette caractéristique FAIR commence par une première nécessité : la donnée doit être stockée dans une forme de base de données, soit-elle basique.\n\nPour pouvoir être traitée et analysée, la donnée doit présenter un format que l’individu ou la machine peut comprendre. Pour être transportée, échangée, mise à jour, modifiée, elle doit également rester fluide. Dans une vision pragmatique, applicable à nos environnements standards, le seul conteneur qui assure cette capacité d’analyse et cette fluidité est celui de la base de données.",
          "Created At": "2025-09-06T13:22:34.049+02:00",
          "Updated At": "2025-09-06T13:22:34.049+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE031_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## Qu’est-ce qu’une base de données ?\n\nUne base de données n’est ni plus ni moins qu’un endroit où nous pouvons entreposer de la donnée. Cette solution assure, selon sa forme, des services de contrôle d’accès, de sauvegarde, de formatage, d’organisation, d’indexation, voire de prétraitement.\n\nLa base de données : une histoire antique\n\nLe concept de base de données est assez naturel. Depuis l’antiquité, de grandes bibliothèques (comme celles d’Alexandrie ou d’Assurbanipal à Ninive) centralisent, sécurisent et donnent accès à des savoirs, à des témoignages, à des relevés. Ces bibliothèques maintenaient en permanence le classement, la mise à jour et l’ajout de nouveaux ouvrages, comme les bases de données actuelles.\n\nLe concept naturel de dépôt central de données a toutefois dû être adapté aux contraintes de volume, de partage, de rapidité et de fluidité exigées par l’entreprise. Nous avons assisté, depuis trois décennies, à la numérisation progressive de ces « bibliothèques » qui sont devenues tout ou partie digitales. Cette transformation a permis de libérer l’information et de la rendre progressivement accessible à tous, partout et en masse.\n\nAujourd’hui, la digitalisation d’une grande partie de notre vie a accéléré cette mise en base de données et, par la même occasion, a permis d’analyser encore plus de données, plus facilement. Nos transactions, nos messages, nos photos, nos réactions (sur les réseaux sociaux), notre voix (sur les assistants vocaux), notre vie sociale (sur nos smartphones) sont nativement dans un format numérique et par conséquent beaucoup plus propices à l’analyse.\n\nCette numérisation de la donnée et son stockage en base de données sont devenus la norme en analytique. Et pourtant, de nombreuses organisations continuent à stocker de la donnée sur du papier, dans des tableurs ou autres documents électroniques bureautiques où les caractéristiques FAIR disparaissent.\n\nPapier contre silicone\n\nPourquoi certaines données sont-elles toujours conservées dans la mémoire commune ou sur des médias personnels tels que des calepins ou des cahiers ? Certes, le support physique confère à la donnée une indépendance à la technologie et à l’électronique, mais il introduit une friction quand il s’agit de partager, de consolider et d’analyser cette information autrement que manuellement.\n\nMis à part certaines technologies avancées, la représentation logique des bases de données que nous allons rencontrer le plus souvent est une collection de tables de données. Ces tables, tout à fait similaires à une liste de courses, à un tableau de points au tarot ou à un menu de restaurant, sont composées de lignes et de colonnes. Bien souvent, les noms dans la première ligne apportent des indi­cations sur le type ou la nature des données de chaque colonne. Chaque ligne est un enregistrement dont les caractéristiques et les valeurs apparaîtront dans les colonnes.\n\n» Combien trouvez-vous de ces structures tabulaires autour de vous ? Vous constatez à quel point cette structure est naturelle pour collecter de l’information.\n\nFigure 2. Structure d’une base de données\n\nL’approche semble évidente et, pourtant, bon nombre de nos données sont conservées dans des tableaux complexes, alambiqués, délicats à retravailler. Pour des finalités de communication, il est normal d’ajouter des totaux, de la couleur et des graphiques, mais, pour les processus intermédiaires d’échange et de transformation de la donnée, ces formats « cosmétisés » sont hautement inefficaces (voir chapitre 7, Chasser les processus Creeper).\n\nIl ne faut pas confondre les objectifs de stockage/préparation et d’échange de données brutes avec les besoins d’analyse Ces deux étapes se suivent, mais ne peuvent être mélangées sous peine de faire perdre à la data sa fluidité et son aspect FAIR, et à l’analyse son adaptabilité et son agilité.",
          "Created At": "2025-09-06T13:22:34.050+02:00",
          "Updated At": "2025-09-06T13:22:34.050+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE032_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## Les fichiers plats : le plus simple des formats FAIR\n\nSi la logique tabulaire est assez simple, une question vient rapidement à l’esprit. Comment utiliser des tables si :\n\n– nous ne sommes pas équipés de bases de données à proprement parler ;\n\n– nous n’avons de budget pour investir dans ces technologies ;\n\n– les services informatiques n’ont pas de temps à nous consacrer.\n\nCe qui pourrait constituer un obstacle incontournable ou une excuse pour ne pas avancer a en fait une solution universelle, accessible à tous sans aucun investissement. Nous avons tous accès à des tables de données capables de stocker des millions de lignes, d’être analysées avec performance, d’être partagées sans difficulté et de nourrir des algorithmes de dernière génération. Nous pouvons tous les mettre en œuvre indépendamment du système d’exploitation de nos ordinateurs, et ce, sans aucun investissement. Ce médium universel, ce couteau suisse de la data n’est autre que le fichier plat, aussi connu sous le nom de fichier texte, ou fichier CSV.\n\nSa mécanique est la plus simple qu’il soit pour définir une table :\n\n– les enregistrements sont représentés par des lignes avec un retour à la ligne à la fin de chacune (retour chariot/entrée) ;\n\n– les colonnes sont indiquées par un délimiteur que toute solution data et analytique saura interpréter comme un changement de colonne. Ce délimiteur est souvent une virgule (voir illustration 1) pour les fichiers CSV (Comma Separated Value), une tabulation pour les fichiers TSV (Tab Separated Value) ou tout autre séparateur qui puisse être identifié sans ambiguïté.\n\nDans ce dernier cas, on parle par extension de CSV, même si le délimiteur est un point-virgule, un pipe (\\|), voire parfois un espace.\n\nIllustration 1. Exemple de fichier plat CSV : extrait du manifeste d’un bateau au destin funeste\n\nUn CSV ou un fichier texte ne ressemble à rien quand il est lu de manière brute. Notre première impression est souvent celle du rejet : « C’est moche, ça à l’air complexe et mon patron ne comprendra jamais. »\n\nMais, si l’on garde en tête les exigences de fluidité de la donnée et la séparation des processus de préparation et de stockage avec celle de l’analyse, on comprend que ces tables sont la partie immergée de l’iceberg sur laquelle reposera la partie émergée de l’analyse et du reporting. Et comme la partie immergée d’un bloc de glace, elle représente la majeure partie du travail et de la valeur du processus analytique. La capture de la donnée, sa préparation, sa modélisation et sa mise à disposition représentent 80 % du travail.\n\nIllustration 2. Exemple de fichier issu de l’interprétation par une solution analytique du fichier CSV de l’illustration 1\n\nConserver les données en tables et a minima en fichiers plats, c’est garantir plusieurs choses.\n\n\\> Un média quasiment sans limite en termes de volume\n\nLe fichier CSV n’a pas de limitation autre que l’espace disque disponible sur notre lecteur ou une limitation de la taille du fichier de notre système d’exploitation. Les formats tableurs présentent rapidement des contraintes de volume et de performance.\n\nTableau 3. Contraintes de volume des tableurs\n\n\\> Un format plus performant au chargement et qui se compresse mieux\n\n\\* La solution 1 est locale. \\*\\* La solution 2 est sur le Cloud.\n\nNote : exemple pour des fichiers de 950 671 lignes × 18 colonnes, soit 17 112 078 cellules (2 150 604 vides, 14 961 474 remplies) traité en format fichier plat et XLSX.\n\nTableau 4. Mesures de volumétrie et de performance pour des fichiers Excel et texte sur un ordinateur portable de travail\n\n\\> Un format d’échange universel\n\nLa majorité des solutions savent aujourd’hui importer et exporter des fichiers plats sans effort. On ne retrouve pas de problème de version de logiciel. Le seul point d’attention est le séparateur qui doit être correctement indiqué dans les phases d’intégration. En effet, si des champs contiennent le séparateur, alors la machine ne pourra pas appliquer sa logique pour retrouver les colonnes.\n\nUn piège classique est celui des CSV dans des contextes internationaux où les nombres peuvent s’écrire avec des virgules ou des points comme marqueur de décimales et dont créer des problèmes suivant l’origine de l’export. Il est préférable dans ce cas d’utiliser des tabulations, voire des pipes (\\|).\n\n\\> Un format sur lequel on peut travailler directement\n\nCes fichiers plats peuvent être immédiatement utilisés pour des analyses, des visualisations avancées, des algorithmes de Machine Learning.",
          "Created At": "2025-09-06T13:22:34.051+02:00",
          "Updated At": "2025-09-06T13:22:34.051+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE033_1757157753082",
          "Status": "Ready For Refine",
          "Text": "# Les trois règles cardinales en analytique\n\nTout ce qui précède nous amène à poser une première fondation dans notre pratique de la donnée :\n\n1\\. récupérons, échangeons et stockons la donnée sous forme de tables ;\n\n2\\. pour des raisons techniques, pratiques ou budgétaires, nous pouvons nous contenter d’un fichier plat ;\n\n3\\. procédons toujours en trois étapes : capturer, préparer, puis analyser la donnée.\n\nLes tableurs sont de formidables outils d’analyse de données. Utilisés selon les règles que nous venons d’énoncer, ils peuvent offrir un niveau acceptable de fluidité, de partage ou d’agilité. Ils ouvrent néanmoins la porte à la tentation de mélanger préparation et analyse, parce que c’est facile et pratique. Ils vont être limités en performance sur des gros volumes de data. Ils vont être moins faciles à partager, car ils nécessiteront toujours une phase de compréhension de leur mécanisme. Enfin, s’ils contiennent des fonctions avancées de transformation, de calcul et de visualisation, ils n’atteindront pas la performance d’une combinaison : base de données + solution d’analytique + bibliothèque de Machine Learning. Les tableurs sont d’excellents outils de prototypage, même si, avec l’expérience, on devient beaucoup plus agile avec des outils analytiques dédiés. Ils apportent également des possibilités d’automatisation de processus intéressantes avec les macros (bien qu’elles aboutissent souvent à des usines à gaz), même si les approches en Python ou en développement de procédures stockées au niveau de la base de données (voir chapitre 5, Optimiser le choix du moteur de calcul) sont beaucoup plus rapides.\n\nCHAPITRE 4  \nMettre les systèmes au service du data pionnier\n\nOn a encore « bidouillé » dans le transactionnel.\n\nAfin de mettre les technologies à notre service, nous allons dans ce chapitre :\n\n– comprendre pourquoi et quand la technologie est pertinente dans un projet analytique ;\n\n– reconnaître et qualifier les deux grandes familles de logiciels ;\n\n– détailler les solutions dédiées à l’analytique ;\n\n– lister les pratiques d’implémentation nécessaires pour des mises en œuvre réussies.",
          "Created At": "2025-09-06T13:22:34.051+02:00",
          "Updated At": "2025-09-06T13:22:34.052+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE034_1757157753082",
          "Status": "Ready For Refine",
          "Text": "# Quelle est l’utilité de la technologie en data ?\n\nAvant de nous précipiter sur la technologie comme remède à tous les problèmes data, répondons à une première question : peut-on faire de l’analytique sans système ? Soyons encore plus extrêmes : peut-on faire de la data sans électricité ? Qu’en pensez-vous ?\n\nLa réponse est sans appel : on peut collecter, stocker, analyser et reporter de la donnée sans aucune technologie particulière. L’humanité fait cela depuis des millénaires soit par la voix et la mémoire, soit par l’écrit sur les supports les plus variés : pierre, tablette d’argile, papyrus, bambou, parchemin ou encore papier. Il est donc possible de gérer une chaîne analytique sans aucun outil informatique.\n\nLa data antique\n\nIl y a plus de trois mille ans, les souverains administraient leur empire avec des relevés réguliers de population, de réserve de métaux ou de grain… Un recensement civil avait pour objectif spécifique d’établir quel rôle les hommes de chaque localité joueraient en matière de mobilisation. Le premier recensement connu a été effectué en 1776 (avant notre ère), sous le règne de Yasmah-Addu, et un autre en 1770, sous le règne de Zimri-Lim. À ces deux occasions, ils se sont produits à la fin d’une période de guerre… Il était évidemment nécessaire de contrôler les effectifs disponibles pour recruter des troupes pour lutter contre Élam5.\n\nSi nous pouvons faire des analyses sans systèmes, pourquoi ces derniers semblent-ils aujourd’hui s’imposer dans tous les projets ? Pourquoi avons-nous besoin de technologies dans nos processus data ? La réponse nous donne la raison d’être des solutions que nous mettons en place. Elle tient en cinq points :\n\n– aller plus vite ;\n\n– voir plus loin et plus en détail ;\n\n– traiter plus d’informations ;\n\n– collaborer avec agilité ;\n\n– bâtir des processus durables.\n\nCela paraît évident, et pourtant ces préceptes ne sont pas toujours partagés et appliqués en entreprise où l’implémentation de technologie est parfois une raison en elle-même.\n\n→ Quand le manuel fait mieux que le 2.0\n\nDans les milieux de la finance, les modes, le qu’en-dira-t-on et l’influence des confrères pèsent lourd dans la balance des investissements logiciels. La mise en place de solutions digitales peut devenir non seulement d’une utilité relative, mais également chronophage par rapport au processus qu’elles sont censées supporter. Voici un exemple vécu à la fin des années 1990.\n\nLa solution en question visait à faire passer dans le 2.0 la confirmation de certaines transactions. Manuelle à l’époque, elle consistait en une série de tâches simples :\n\n– extraire les fichiers de confirmation ;\n\n– imprimer le lot ;\n\n– les séparer par institution financière ;\n\n– les faxer (il y a deux décennies, cette technologie était la seule reconnue comme valeur contractuelle) en chargeant liasse par liasse les documents et en laissant ensuite la machine les envoyer.\n\nUne collaboratrice se chargeait chaque matin de cette tâche. Malgré son obsession pour l’efficacité et la rapidité, le responsable avait choisi de ne pas remettre en cause ce processus qui présentait beaucoup d’avantages :\n\n– les tâches manuelles ne prenaient pas plus de cinq à sept minutes ;\n\n– elles comportaient peu de risques d’erreur, si ce n’est aucun ;\n\n– le fax pouvait prendre un quart d’heure, mais cette tâche n’avait pas besoin de supervision humaine, sauf pour une comparaison pages chargées/pages envoyées à la fin de l’envoi ;\n\n– la collaboratrice profitait de la durée d’envoi des documents pour se préparer un café et fumer une cigarette, ce qui constituait une pause quotidienne en milieu de matinée ;\n\n– et surtout, elle venait rejoindre son responsable avec une tasse du café qu’elle avait fait couler, l’esprit détendu par sa pause, pour faire la revue des opérations de la veille et des points d’attention sur les reportings quotidiens des filiales. Ce moment était précieux, car il présentait un temps de réflexion apaisé et calme au milieu de journées chargées. C’était la possibilité de traiter de vrais problèmes, de réfléchir à des points stratégiques.\n\nLorsque le processus passa en 2.0 par le truchement de la confir­mation électronique, la collaboratrice perdit du jour au lendemain sa pause-café-cigarette et le responsable un moment précieux d’échange. Il n’avait pas été consulté sur le projet, une migration contre-productive qui faisait s’évaporer beaucoup de valeur. Le processus était devenu le suivant :\n\n– extraire un à un les fichiers par banque ;\n\n– les stocker sur une clé USB ;\n\n– les recopier sur un poste de communication dédié ;\n\n– imprimer ces fichiers pour revue et validation interne ;\n\n– les assigner à chacune des banques ;\n\n– lancer le transfert des fichiers ;\n\n– relancer les transferts non passés la première fois.\n\nPour la collaboratrice, le processus prenait maintenant une bonne trentaine de minutes et elle devait enchaîner des manipulations informatiques non triviales. Elle ne pouvait prendre ces minutes de pause, de préparation de son café et encore moins profiter d’une cigarette. Elle n’avait de surcroît plus qu’une dizaine de minutes à consacrer aux sessions de travail avec son manager.\n\nCerise sur le gâteau, cette avancée vers le 2.0 avait coûté l’équivalent de 200 000 euros pour perdre 10 minutes par jour et faire disparaître du calendrier une séance de travail à forte valeur ajoutée.\n\nLa technologie n’est pas l’arme absolue du digital. Dans l’exemple ci-dessus, les choix avaient été dictés par la mode du 2.0, la pression d’un éditeur et la volonté d’épater les confrères, au détriment de toute efficacité… et de l’humain. Le projet avait été dirigé par des professionnels sans réelles connaissances techniques. Il a été possible de modifier le processus, d’adapter les solutions tout au long de la chaîne et finalement de livrer un niveau de performance et d’automatisation optimal… qui faisait gagner quatre minutes sur le traitement initial. Cet exemple illustre à quel point il est facile d’oublier la raison pour laquelle nous utilisons des outils. Le monde de l’analytique est très prompt à mettre la charrue avant les bœufs.\n\nNous allons voir maintenant les familles de systèmes que nous retrouvons toujours dans nos organisations pour ensuite nous concentrer sur les technologies qui supportent l’analytique.\n\nL’objectif est de comprendre le rôle et le positionnement de chacune des familles de solutions dans la chaîne analytique. C’est ainsi que les personnels du métier peuvent devenir des interlocuteurs éclairés des services techniques et les équipes informatiques, des éducateurs pédagogues vis-à-vis de leurs collègues du terrain, voire des architectes de solutions à leur niveau.\n\n« Chaque science, chaque étude, a son jargon inintelligible, qui semble n’être inventé que pour en défendre les approches. » Voltaire\n\nNous pouvons en général regrouper l’ensemble des systèmes de l’entreprise en deux catégories : les systèmes transactionnels et les systèmes décisionnels, chacun avec sa raison d’être. Leur nature et leurs fonctions diffèrent et ne pas savoir les distinguer ou les utiliser l’un pour l’autre peut conduire à des erreurs coûteuses.",
          "Created At": "2025-09-06T13:22:34.052+02:00",
          "Updated At": "2025-09-06T13:22:34.052+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE035_1757157753082",
          "Status": "Ready For Refine",
          "Text": "# Les solutions transactionnelles\n\nLes solutions transactionnelles sont celles qui enregistrent les… transactions. C’est simple, non ?\n\nComment peut-on les définir un peu mieux ? Elles ont les objectifs suivants :\n\n– automatiser les opérations commerciales de base telles que les prises de commandes, les rentrées et les sorties en stock, les livraisons, la facturation, les règlements et les encaissements, etc. Elles remplacent les multiples saisies manuelles et les chaînes administratives papier tout au long du cycle ;\n\n– monitorer les étapes du cycle d’exploitation de la commande fournisseur au règlement client en passant par la mise en stock ou la production :\n\n• pour aider les employés à faire leur travail plus efficacement en supprimant les barrières entre les unités commerciales ;\n\n• pour donner une vue en temps réel du cycle afin de permettre aux entreprises de répondre aux problèmes de manière pro­active et d’optimiser en permanence leurs opérations ;\n\n– améliorer la conformité avec les normes réglementaires en encadrant, voire en forçant, la capture d’éléments obligatoires et en assurant une gestion relativement stricte de leur administration.\n\nLes systèmes transactionnels font d’ailleurs souvent l’objet d’audits d’intérim par les équipes qui veillent à ce que les transactions réelles y soient bien enregistrées, puis correctement portées d’une étape à une autre. Avec leur extension à tous les domaines de l’entreprise, les systèmes transactionnels gèrent souvent la majorité des flux de l’activité. Certaines de ces solutions, souvent appelées ERP (Enterprise Ressource Planning), se retrouvent par exemple dans les services :\n\n– achats ;\n\n– marketing en ligne ;\n\n– support clients ;\n\n– ressources humaines ;\n\n– ventes.\n\nLes données qu’ils collectent sont fondamentales pour l’établissement des comptes financiers et des déclarations fiscales.\n\n» Sauriez-vous identifier les systèmes transactionnels ou ERP de votre entreprise ou de votre département ? Sauriez-vous repérer celui qui se cache dans ces photos (attention, il y a un piège sur la troisième) ?\n\nLa caisse enregistreuse, ici dans sa forme la plus moderne, est le système transactionnel qui capture les transactions d’achats et leurs encaissements.\n\nLe petit boîtier à la ceinture de ce livreur scanne les colis livrés et enregistre l’acte de dépôt chez le récipiendaire.\n\nIci, pas de technologie digitale, mais le tableau de score enregistre les coups et la performance de la golfeuse. Ce dernier collecte les « transactions » du parcours. On peut imaginer que ces données seront consolidées avec d’au­tres pour suivre à la fois la progression de la golfeuse, mais également les performances globales des membres du club. Considérer que cette étape est très manuelle et relativement peu efficace pourrait amener à adopter une solution de capture plus intégrée pour gagner en temps, en précision, voire en richesse de données captées (température, heure exacte, biométrie, etc.), à moins que le retour sur investissement ne soit trop élevé ou que les challenges métier soient suffisamment couverts par cette approche « papier ».",
          "Created At": "2025-09-06T13:22:34.053+02:00",
          "Updated At": "2025-09-06T13:22:34.053+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE036_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## Une brève histoire de l’ERP\n\nLe terme ERP a été inventé en 1990 par la firme d’analystes Gartner, mais ses origines remontent aux années 1960. Le concept s’appliquait alors à la gestion et au contrôle des stocks dans le secteur manufacturier. Les ingénieurs logiciels ont créé des programmes pour surveiller les stocks, rapprocher les soldes, etc. Dans les années 1970, cela avait évolué vers des systèmes de planification des besoins en matériaux (MRP) pour les processus de production.\n\nDans les années 1980, le MRP s’est développé pour englober davantage de processus de fabrication, ce qui a incité beaucoup de gens à l’appeler MRP-II ou Manufacturing Resource Planning. En 1990, ces systèmes se sont étendus, au-delà du contrôle des stocks et d’autres processus opérationnels, à d’autres fonctions de back-office, comme la comptabilité et les ressources humaines, ouvrant la voie à l’ERP.\n\nAujourd’hui, l’ERP gère également les fonctions de front office telles que l’automatisation des forces de vente (SFA), l’automatisation du marketing et le commerce électronique. Devant ces avancées de produits et leurs réussites, un large éventail d’entreprises, de la distribution en gros au commerce électronique, utilisent désormais des solutions ERP6.",
          "Created At": "2025-09-06T13:22:34.054+02:00",
          "Updated At": "2025-09-06T13:22:34.054+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE037_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## Les forces des ERP sont leurs faiblesses pour l’analytique\n\nLorsque l’on comprend la finalité des systèmes transactionnels, on saisit mieux certaines de leurs caractéristiques intrinsèques :\n\n– ils sont limités à un groupe de tâches défini. Chaque module est développé avec les spécificités du processus qu’il couvre ;\n\n– ils capturent la donnée au plus haut niveau de granularité. Chaque transaction, chaque virement, chaque pièce produite, chaque envoi, chaque détail de facture, etc. nécessaire à l’exécution de la transaction est enregistré, sans exception ;\n\n– l’information est brute dans le détail le plus fin ;\n\n– leur agilité n’est pas innée. Les processus transactionnels s’appuient tous sur une base standard et commune à tous les acteurs économiques, au moins au niveau de chaque industrie. Ils doivent bien sûr s’adapter aux spécificités de chaque entreprise, mais, en dehors de ces customisations, une facture reste une facture, un bordereau de livraison reste un bordereau de livraison, etc. ;\n\n– ils affichent un certain manque de transparence et un côté « boîte noire ». Ces logiciels ne sont pas faits pour être désossés. Ils contiennent par ailleurs la propriété intellectuelle de leurs éditeurs. Par conséquent, s’aventurer à customiser les rouages sans contrôle est risqué. Il est souvent préférable de ne pas trop interférer avec leur mécanisme ;\n\n– ils doivent garantir de hauts niveaux de disponibilité et de performance. Le niveau de service peut être de 24 heures sur 24, 365 jours sur 365 et les besoins de réponse en temps réel ;\n\n– ils intègrent de fortes contraintes d’intégrité pour garantir des transactions justes, opposables à des tiers ou respectant des contraintes légales. On ne saurait être tenté de venir manuellement ajouter de la donnée d’un logiciel de facturation ou de gommer une liste de livraisons reçues.\n\n→ Oui, mais chez nous c’est différent\n\nLa tentation de customiser les outils transactionnels est parfois grande. Combien de fois avons-nous pu entendre « nos processus sont différents et nous devons fortement adapter les solutions du marché » ? S’il faut respecter l’expérience et l’expertise de chacun dans l’émission de ces propos, il peut être pertinent de réfléchir également à des approches moins extrêmes où les processus et les outils peuvent être adaptés de manière synergétique et moins coûteuse. Cela peut permettre d’éviter :\n\n– des coûts importants de transformation de l’outil, car les compétences sur ce genre d’intervention sont souvent onéreuses ;\n\n– une complexification de la solution avec les problèmes de maintenance et de performance qui peuvent en résulter ;\n\n– de tomber dans une logique pernicieuse autant en processus transactionnels qu’en analytique qui conduit à bâtir des usines à gaz : « Nous savons mieux que les autres et, si le reste du monde fait différemment, ils ont probablement tort. »\n\nSi le retour sur investissement de l’insertion de ces customisations est clair et qu’il prend bien en compte les éléments ci-dessus, il n’y a pas de discussion. Dans les autres cas, il est peut-être plus sage de concentrer son énergie sur des éléments de gestion qui apporteront de la valeur et de la pérennité.",
          "Created At": "2025-09-06T13:22:34.054+02:00",
          "Updated At": "2025-09-06T13:22:34.054+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE038_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## L’analytique transactionnelle n’est pas l’analytique décisionnelle\n\nLa plupart des solutions ERP intègrent leurs analytiques et leurs tableaux de bord. Elles sont dédiées, spécifiques et en prise directe sur l’activité. Elles sont au plus proche du temps réel, ce qui est nécessaire pour les équipes chargées de l’exploitation qui ont besoin d’identifier les problèmes rapidement pour éviter des conséquences graves sur l’activité. Par exemple, la mesure en continu de la qualité de pièces produites, de conditions climatiques, de risque de fraude est nécessaire, car tout délai dans l’arrêt d’une machine, dans la protection d’infrastructures ou dans le blocage d’un compte pourrait être dommageable.\n\nLorsque les questions métier deviennent plus larges, nous allons avoir besoin de données plus diverses et de leur appliquer des traitements qui varieront en fonction des réponses recherchées et qui restitueront les résultats sous des formes diverses.\n\nLes solutions transactionnelles trouvent rapidement leurs limites pour centraliser ces données, pour proposer des analyses spécifiques et des rapports partageables avec des audiences larges.\n\nPersister à traiter ces questions décisionnelles dans le transactionnel entraîne quatre grands types de conséquences.\n\n\\> La naissance de monstres\n\nLa customisation du transactionnel aboutit à la naissance d’un système de plus en plus complexe et de moins en moins gérable. Ces adaptations sont de surcroît coûteuses, car elles requièrent des intervenants éditeurs ultraqualifiés.\n\n→ La progéniture des docteurs Frankenstein\n\nIl y a quelques années, un fabricant de composants réseau de la Silicon Valley s’est retrouvé confronté de manière brutale à une situation inextricable avec son ERP global. Pendant près d’une décennie de customisations à tout va, les équipes informatiques avaient évité les mises à jour de leur solution afin de ne pas avoir à redévelopper leur modification. Mais l’inéluctable est arrivé, lorsque l’éditeur a annoncé la fin de la maintenance de la version ancienne de son outil, il fallait mettre à jour un système qui ne ressemblait plus du tout à une installation standard. Une analyse des coûts aboutit rapidement à la conclusion que repartir de zéro était la seule option viable. Le groupe décida d’entreprendre une nouvelle implémentation et choisit cette fois-ci l’éditeur concurrent. L’implémentation dura deux ans et fut extrêmement douloureuse pour les équipes et pour l’activité qui dut être arrêtée pendant plus d’un mois pendant la migration. Rétrospectivement, ces problèmes auraient pu être évités si les équipes en charge avaient respecté leur ERP.\n\n\\> La mutation en escargot\n\nL’utilisation du transactionnel pour des analyses régulières a un impact sur la performance. Ces solutions sont calibrées pour un volume d’activités transactionnelles et l’ajout d’une charge analytique supplémentaire peut considérablement les ralentir, surtout lors de phases de charge intenses en production.\n\n→ La fièvre du vendredi soir\n\nLa scène prend place dans un beau magasin de vêtements féminins et de décoration, d’une chaîne haut de gamme, un vendredi en fin d’après-midi, dans le très select Stanford Shopping Center. Dans la douce odeur des parfums artificiels du magasin, la musique parfaitement choisie et un éclairage digne d’un studio de cinéma, une cliente se rend à la caisse avec un peu plus de 2 000 dollars d’articles. Il est 17 heures 10, elle sera à la maison pour 18 heures, juste à temps pour commencer une belle soirée. L’agent de caisse l’attend avec le sourire et commence à scanner les articles. Un premier beep rassurant, puis un deuxième… puis plus rien… : les articles ne passent plus. Le caissier tente une saisie manuelle du code-barres. La caisse enregistreuse ne répond pas. Les autres caissiers sont dans la même situation. Il est maintenant 17 heures 20 et la file aux caisses s’est allongée. Les clientes du vendredi soir s’impatientent. Elles regar­dent nerveusement les trois caissiers qui, malgré l’embarras de la situation, semblent habitués au problème. Certaines commencent à partir, laissant par terre leurs sacs remplis de vêtements et de bibelots. D’autres interpellent le personnel du magasin avec des menaces à peine voilées de plainte auprès du directeur. Mais toutes ont leur téléphone à la main, sur leurs réseaux sociaux préférés, critiquant à tour de bras le service de cette enseigne de renom.\n\nQue se passe-t-il pour que tous les vendredis à 17 heures les systèmes du magasin se mettent à ralentir au point de causer de tels retards aux caisses ?\n\nLa solution se trouve à 4 700 km de là, sur la côte est. Il est 20 heures là-bas et c’est la fin de semaine. Un analyste prépare activement les rapports du vendredi en commettant deux erreurs cardinales. Il attaque massivement le système transactionnel de facturation par des requêtes de données et fait tourner de gros calculs d’agrégation sur ceux-ci. Ses extractions sont par ailleurs mal programmées et mal ajustées à ses besoins : elles extraient beaucoup trop de détails. Parce qu’il est en prise direct sur le transactionnel, l’impact est immédiat : les systèmes commencent à ralentir notablement, entraînant les problèmes aux caisses.\n\nFaudrait-il investir dans des serveurs plus puissants ? Faudrait-il changer d’ERP ? Avec des ressources et du temps, toutes ces solutions seraient possibles. Mais l’approche la plus raisonnable ne serait-elle pas d’éviter de mettre le transactionnel à genoux sur des périodes de fonctionnement de magasin ? Une compréhension des technologies, des mécaniques de la donnée et des processus pourrait permettre de résoudre le problème simplement et sans grand surcoût.\n\n» Que feriez-vous pour alléger la charge du vendredi soir ?\n\nNous pourrions par exemple :\n\n– lisser la capture de données tout au long de la journée et de la semaine ;\n\n– valider le périmètre de la capture pour éviter une surextraction d’informations ;\n\n– effectuer les calculs sur un autre serveur que celui du transac­tionnel ;\n\n– extraire moins de détails pour le reporting du vendredi soir : les grandes lignes peuvent suffire dans un premier temps. Qui prendrait une décision d’urgence à cet instant-là ?\n\n– extraire toute la data une fois que tous les magasins sont fermés : les analyses détaillées ne pourront être faites que lundi matin de toute façon ;\n\n– ou, de manière plus élégante, n’extraire qu’un échantillon représentatif et inférer une tendance dès la mi-journée, pour permettre, sur la journée du vendredi, un vrai travail de réflexion et d’analyse.\n\n\\> La création de cocktails explosifs\n\nLes solutions transactionnelles ne sont pas organisées pour faciliter l’accès de l’utilisateur à la donnée. Les modèles de données visent la performance et la granularité. Les ouvrir à des analystes généralistes, non formés aux arcanes des ERP, est la garantie de gros problèmes, tant dans la qualité de la donnée extraite que dans la stabilité du système. En effet, le nombre élevé de tables ajouté à la complexité des conditions à définir pour une extraction précise peut donner des requêtes complètement fausses.\n\n\\> Le péché cardinal de la requête sans fin\n\nLes systèmes transactionnels ont des structures extrêmement complexes. Non seulement les données sont organisées de manière non naturelle pour un utilisateur métier, mais elles apparaissent en plusieurs exemplaires, sous des en-têtes incompréhensibles et requièrent des requêtes très complexes pour être extraites. Pourquoi faire compliqué ? L’objectif est ici la performance et l’intégrité du traitement des opérations : faciliter le travail de l’analyste n’est pas dans le cahier des charges.\n\nTous ces risques contribuent à augmenter la charge sur le transactionnel et à diminuer sa performance opérationnelle.\n\n» Cherchez autour de vous des exemples de dérogation à cette règle et constatez les conséquences de ces choix.\n\nRespecter l’intégrité du transactionnel est une règle quasi cardinale et nous pouvons comprendre les résistances justifiées des équipes informatiques, d’autant qu’il existe des solutions pour traiter exactement ces problèmes dans les règles de l’art.",
          "Created At": "2025-09-06T13:22:34.055+02:00",
          "Updated At": "2025-09-06T13:22:34.055+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE039_1757157753082",
          "Status": "Ready For Refine",
          "Text": "# Les nouveaux besoins analytiques\n\nNous avons vu que, jusqu’à présent, les solutions transactionnelles contribuent à notre excellence opérationnelle. Mais, aujourd’hui, cet avantage concurrentiel peut ne plus être suffisant. Nous devons aussi être équipés pour l’excellence décisionnelle, celle qui va nous permettre de prendre des décisions rapides et pragmatiques sur des pans entiers de l’organisation, dans un monde en constante évolution.",
          "Created At": "2025-09-06T13:22:34.056+02:00",
          "Updated At": "2025-09-06T13:22:34.056+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE040_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## Le cycle de management\n\nÀ la maîtrise des cycles d’exploitation est venue s’ajouter celle du cycle de management. Il fallait apporter intégration et automa­tisation des processus de planification stratégique, de budget, de mesure de performance, d’analyse d’écart, et d’investigation de problème et de reporting.\n\nFigure 3. Cycle du management de la plupart des organisations\n\nLe cycle « stratégie-planification-action-mesure-ajustement » s’appuie sur l’ensemble des modules transactionnels tout autour du cercle. Les solutions décisionnelles au centre viennent unifier la donnée, pour une vision globale de l’entreprise.\n\nL’une des difficultés venait de ce que l’exercice devait être réalisé au niveau du périmètre entier des activités de l’entreprise. Il n’était plus question de chiffres détaillés au centime près et de mesure de chaque transaction, mais d’une approche plus globale en tendance et en agrégation. Il n’était plus question de se concentrer sur la production, la gestion de la paie ou les stocks, mais de faire la synthèse de tous ces éléments. Les solutions transactionnelles n’avaient pas été bâties pour cet exercice. Lorsque les métiers ont dû centraliser, préparer, analyser et diffuser leurs informations, ils se sont tournés vers la solution qu’ils maîtrisaient : les tableurs en général et Excel en particulier.\n\nLe besoin de suivre des données de plus en plus diverses, de plus en plus vite, a conduit à l’essor de l’analytique sous Excel et, dans bien des cas, à des sous et surexploitations de l’outil mal maîtrisées. Des jungles de feuilles de tableur ont ainsi proliféré, apportant à court terme des solutions rapides mais créant à moyen terme des processus creeper (voir chapitre 7, Chasser les processus Creeper). C’est ce qui a été communément appelé le Spreadsheet Hell (l’enfer des tableurs).",
          "Created At": "2025-09-06T13:22:34.056+02:00",
          "Updated At": "2025-09-06T13:22:34.056+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE041_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## La naissance de la Business Intelligence\n\nLes outils de BI (Business Intelligence) sont venus combler le manque de solutions durables au-dessus des ERP et éviter ces proliférations anarchiques d’outils tableurs. Le terme apparaît dès les années 1950. On le repère dans un papier technique du IBM Journal of Research and Development de 1958.\n\nLa Business Intelligence… il y a soixante ans\n\nUn système automatique est en cours de développement pour diffuser des informations aux différentes sections de toute organisation industrielle, scientifique ou gouvernementale. Ce système de renseignement utilisera des machines de traitement des données pour réaliser des synthèses de documents et pour créer des profils d’intérêt pour chacun des « points d’action » d’une organisation. Les documents entrants et générés en interne sont automatiquement extraits, caractérisés par un modèle de mot, et envoyés automatiquement aux points d’action appropriés. Cet article montre la flexibilité d’un tel système pour identifier les informations connues, pour trouver qui a besoin de les connaître et pour les diffuser efficacement sous forme résumée ou en tant que document complet7.\n\nCe n’était pas très vendeur, mais le concept était là.\n\nEn 1989, Howard Dresner a proposé Business Intelligence comme terme générique pour décrire « les concepts et les méthodes pour améliorer la prise de décision commerciale en utilisant des systèmes de soutien basés sur des faits ».\n\nDans les années 1990, les solutions décisionnelles sont apparues avec des outils pour combler les manques des solutions transactionnelles. Après la vague ERP, le monde était prêt pour celle du décisionnel.\n\nMais là où les solutions transactionnelles avaient concurrencé des tâches manuelles et papier sans difficulté, l’adoption des solutions décisionnelles était confrontée à un existant bureautique tenace et à un manque de compréhension « data » de ses utilisateurs cibles. La population d’analystes était d’une grande diversité de profil, de niveau et d’appétence pour les chiffres.\n\nLes solutions décisionnelles apportent des fonctions jusqu’à présent gérées manuellement et devenues critiques :\n\n– centraliser des données de différents domaines pour avoir une vue d’ensemble ;\n\n– analyser de manière agile pour répondre à des besoins changeants ;\n\n– gérer efficacement des cycles budgétaires ou prévisionnels ;\n\n– diffuser l’information aux acteurs de l’entreprise ;\n\n– et, plus récemment, appliquer des mathématiques, des statistiques et des apprentissages par la machine (Machine Learning) pour analyser ou prévoir avec plus de précision. Ce qui est souvent rangé sous les vocables : Data Science, intelligence artificielle ou Machine Learning.\n\nPour y répondre, les solutions BI se sont organisées autour de quatre fonctions :\n\n– la capture et la préparation de données ;\n\n– le stockage orienté métier des informations ;\n\n– l’analytique ;\n\n– le reporting.\n\nFigure 4. Pile décisionnelle\n\nNous allons détailler les technologies à travers ce prisme.\n\nAttention terrain miné\n\nCher lecteur, vous entrez dans un domaine sensible pour beaucoup de personnes. Pendant des décennies, il a été précieusement gardé afin d’éviter la démocratisation de ses concepts. Des barricades de jargon technico-marketing ont été régulièrement érigées pour noyer les simples mortels comme nous. Une vingtaine d’années d’expérience dans le secteur et une quinzaine passée dans l’enseignement supérieur et en mentorat m’ont appris que ces concepts peuvent être présentés de manière simple, sans pour autant être simpliste.  \nNous ne ferons pas l’économie de quatre termes barbares et un tantinet rebutants, car il faut à un moment donné appeler les choses par leur nom. Une fois que vous aurez lu les quelques lignes qui les décrivent et surtout leur raison d’être, vous ne pourrez vous empêcher de lâcher un profond soupir et de vous dire : « Ah, c’est juste ça dont il est question ! » Nos quatre vilains mots (ou acronymes) sont :  \n– Middleware ;  \n– ETL (Extract, Transform, Load) ;  \n– MDM (Master Data Management) ;  \n– Base de données ou Database.",
          "Created At": "2025-09-06T13:22:34.057+02:00",
          "Updated At": "2025-09-06T13:22:34.057+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE042_1757157753082",
          "Status": "Ready For Refine",
          "Text": "# Comprendre les solutions BI ou décisionnelles\n\nAfin de mieux comprendre les différents composants décisionnels et leurs interactions, remettons-les dans leur contexte. Le but du décisionnel est de répondre rapidement et pragmatiquement à des questions ou à des challenges métier grâce à une chaîne de production d’analytique efficace.\n\nQu’il y ait ou non des solutions logicielles dans ce processus n’est pas le point important. Ce qui compte est de comprendre que cette supply chain de la donnée doit constamment transformer de la donnée brute en information pertinente pour répondre aux problématiques métier. Elle doit le faire avec :\n\n– rapidité pour répondre sans délai aux problèmes ;\n\n– agilité pour s’ajuster à un environnement changeant ;\n\n– fiabilité pour éviter des erreurs grossières ;\n\n– durabilité pour qu’elle devienne un vrai instrument de gestion.\n\nCela ne vous rappelle-t-il pas les critères de fluidité évoqués plus haut (voir chapitre 3, Qu’est-ce que la donnée pour l’analytique en entreprise ?) ? Cette chaîne a été pensée pour répondre à ces exigences. Ce n’est pas forcément une solution logicielle, c’est avant tout un enchaînement logique de tâches que soit l’homme soit la machine devra réaliser.",
          "Created At": "2025-09-06T13:22:34.058+02:00",
          "Updated At": "2025-09-06T13:22:34.058+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE043_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## Le Middleware\n\nUne fois la question métier posée, tout processus analytique commence par de l’acquisition de données. Sans cette phase, rien ne peut commencer. Si la donnée est déjà à disposition comme c’est le cas sur des processus récurrents, c’est que cette tâche a déjà eu lieu en amont. Un copier-coller, une récupération de documents papier, un questionnaire, une saisie manuelle : toutes ces étapes peuvent servir à collecter une donnée que nous allons analyser.\n\nLe groupe de solutions en charge de cette étape est appelé dans le jargon le Middleware ou intergiciel en français.\n\nCe terme, néologisme aux sonorités barbares, est en fait plus simple qu’il n’en a l’air. En un mot, c’est comme Tupperware, mais on a mis « Middle » à la place ; et au lieu de boîtes en plastique, il s’agit de solutions techniques que l’on a placées pour faire le lien, dans le cas du décisionnel, entre les solutions transactionnelles et les sources données nécessaires à l’analyse. Ce sont bien les solutions qui sont au milieu (Middle) de deux technologies.\n\nCes solutions sont fondamentales parce que l’acquisition et la préparation des données sont les maillons les plus importants de la chaîne analytique. Tout analyste ou data scientist vous le dira : la préparation de données, c’est 80 % du travail. D’où ma recommandation de considérer cette phase critique comme une phase préliminaire à part entière et de ne pas la mélanger complètement avec la phase d’analyse. On peut certes faire des préparations de données de dernière minute, mais, si l’on veut des processus efficaces et durables, ce n’est pas la meilleure option à terme.\n\nÀ ce niveau, nous pouvons considérer qu’il y a trois grandes fonctions à réaliser :\n\n– centraliser l’information ;\n\n– aligner et normaliser les données pour qu’elles soient cohérentes entre elles ;\n\n– nettoyer et préparer pour pouvoir travailler sur une donnée un minimum propre.\n\n\\> Récupérer et préparer la donnée\n\nLe premier groupe de solutions récupère de l’information. Il va permettre d’accélérer, d’automatiser et de mettre à l’échelle cette phase, souvent couverte par des importations de fichiers ou des copier-coller. C’est le monde des ETL.\n\nETL est un acronyme qui veut simplement dire extraire, transformer, load (charger).\n\nEn clair, ces solutions vont chercher de l’information, la préparer pour qu’elle soit adaptée à l’analyse et la charger dans un endroit accessible (la base de données dans la majorité des cas). Et voilà, vous avez compris un premier mot de jargon absolument clé en analytique.\n\nDétaillons maintenant ces trois étapes. Nous allons voir qu’elles vont apporter beaucoup à notre chaîne analytique en termes de fluidité de données (voir chapitre 7).\n\nExtraire la donnée\n\nCombien de temps passons-nous à exporter de l’information de systèmes transactionnels ? Il nous faut penser à faire les extractions à temps, savoir où aller, passer par des menus à rallonge, se souvenir d’interfaces, penser à changer les filtres d’extraction, attendre, récupérer des extractions dans différents endroits, etc.\n\nCette tâche est totalement automatisée par les ETL. La fonction d’extraction automatique de l’ETL est quasi magique quand on se remémore les heures passées à centraliser manuellement des informations pendant des années. Elle va :\n\n– aller se connecter à toutes les sources de données nécessaires à l’analyse ;\n\n– sélectionner l’exact périmètre de données à extraire avec tous les bons filtres ;\n\n– choisir l’heure d’extraction pour éviter les impacts sur les systèmes transactionnels.\n\nTransformer la donnée\n\nCette possibilité peut être mise à profit pour réaliser bon nombre de calculs, de corrections ou d’agrégations. En les lançant pendant la phase d’ETL, nous améliorons notre processus analytique à plusieurs niveaux :\n\n– concentration sur l’analyse : nous n’avons plus à passer du temps à faire ces calculs ;\n\n– temps de traitement : les calculs sont réalisés sur de puissants serveurs en amont, pas sur notre machine de bureau ;\n\n– qualité de données : les transformations (voir chapitre 5, Opti­miser les analyses par les données calculées/préparées) et les calculs ont été développés en collaboration avec des ingénieurs sur des technologies spécialisées et, une fois les algorithmes validés, le risque d’erreur est nul (sauf si la donnée est mauvaise).\n\nCharger la donnée\n\nÀ ce stade, notre donnée a été extraite et raffinée et l’ETL nous offre la liberté de la déposer où nous le souhaitons. Finis les copier-coller, les importations qui durent des heures ou encore la recherche désespérée du fichier créé par l’informatique. Ce confort se cumule avec un gain de temps à chaque mise à jour de reporting, sans compter l’esprit en paix de savoir que toutes les données sont bien arrivées et à leur place.\n\n→ Commencer sa journée Data Ready\n\nDe tous les grands moments dont se souvient un responsable du contrôle de gestion, celui qui l’a toujours frappé était les réunions quotidiennes à 10 heures du matin avec son équipe. C’était autour d’un café qu’il discutait ouvertement des problèmes, réfléchissait aux solutions, mettait en évidence les défis commerciaux à résoudre et prenait en compte les informations qu’il devait produire pour le directeur financier, la direction et les partenaires commerciaux.\n\nCe qui avait lieu au cours de ces réunions de vingt minutes résumait ce qu’il appréciait le plus dans son travail en finance : des discussions impartiales et calmes fondées sur les informations dont chacun disposait et les analyses que chacun avait ou pouvait facilement produire. Le niveau de confiance et de confort autour de l’équipe était fort, tout comme la confiance dans les données et les rapports. Cette équipe était-elle composée de super-héros ? De personnes des plus grandes universités ? Pas du tout. Étaient-ils des bourreaux de travail sacrifiant leurs nuits et leurs week-ends pour arranger les choses ? Absolument pas. Étaient-ils terrorisés par leur direction ? Non plus. Ce qui faisait la différence, c’est que l’équipe se réveillait chaque jour avec de nouvelles données fraîches qui avaient été saisies, traitées, organisées, connectées pendant la nuit par des systèmes d’ETL efficaces.\n\nLeur première responsabilité n’était plus de trouver les données, ni le nombre de macros qu’il leur faudrait exécuter ou déboguer pour avoir un ensemble de données propres. Il s’agissait de lire les informations comme s’il s’agissait d’un livre ouvert et d’exploiter leur intelligence, leur expérience d’entreprise et leur créativité pour trouver de nouvelles façons de sauver la situation. Ce processus automatisé de collecte de données sans faille peut sembler un petit détail dans la séquence globale de gestion des données, mais il faisait toute la différence. Cela a eu un impact sur les gens, leur comportement, leur niveau de confort et de confiance. La disponibilité d’une solution offrant des données propres et à jour, accessibles de manière transparente par l’équipe, a amélioré de façon exponentielle leur capacité à servir leur organisation.\n\nUn élément essentiel de la vague d’analyse des données que nous traversons tous est la capacité de capturer et de centraliser les points de données clés indépendamment du nombre de systèmes, du volume et de la complexité des sources sous-jacentes. C’est la base d’une analyse précise qui, surtout, permet aux équipes de concentrer leur expérience, leur puissance de travail et leur collaboration sur le problème réel, d’enquêter au lieu de croquer les données.\n\nC’est l’art de l’ETL que nous devons tous apprendre à maîtriser ou du moins à contrôler. Qu’elles soient autonomes ou intégrées, le choix de ces solutions qui nous permettent de préparer les données chaque jour est une des clés du succès.\n\n» Avez-vous souvent l’occasion de profiter de réunions productives, celles où des équipes entières peuvent non seulement servir leur organisation et leurs partenaires commerciaux, mais aussi faire pleinement l’expérience d’effectuer un travail qu’elles apprécient vraiment ?\n\nLe positionnement du T (Transformation), au milieu ou à la fin de l’acronyme, indique à quel niveau se font les calculs. Vont-ils être réalisés sur le serveur source ou sur le serveur cible ? Même sans être technicien, nous pouvons avoir un avis. Il s’agit ici de savoir quel endroit va être le plus performant, causer le moins d’interruptions et dans lequel il sera plus facile de programmer les transformations. Ces réponses ne peuvent venir que d’une discussion constructive et donneront certainement lieu à des arbitrages et à des concessions entre les métiers et les différents groupes de l’informatique en charge des systèmes concernés.\n\nPourquoi évoque-t-on rarement l’ETL dans les processus analytiques ?\n\nTout d’abord, il s’agit d’une problématique peu sexy en comparaison des projets flamboyants de visualisation avancée ou d’analytique prédictive. C’est aussi probablement la discipline qui demande le plus de rigueur, d’organisation et de technique. Ce réseau de tuyaux qui vont acheminer l’information nécessite une bonne connaissance de la gestion de données qui nous fait souvent défaut (voir chapitre 5, Organiser la donnée pour une meilleure analytique). Elle nous met aussi en prise avec les services plus techniques, avec lesquels nous entretenons parfois des relations délicates.\n\n» Identifiez dans votre entourage les solutions d’ETL qui sont utilisées. Sont-elles appelées par le nom de leur éditeur (Talend, Stambia, Oracle, etc.). Sont-elles identifiées par le nom du produit (ODI, SSIS, etc.) ? Sont-elles nommées en fonction d’un projet ou d’un processus ? Ou bien sont-elles effectuées manuellement ?\n\n\\> Les solutions de Master Data Management\n\nLes données que nous avons préparées ne sont pas toutes prêtes à l’emploi. Elles peuvent arriver de plusieurs systèmes qui ont souvent chacun des référentiels différents. Afin de pouvoir par la suite joindre ces informations pour des analyses d’ensemble, il va falloir créer un référentiel commun pour que chaque client, chaque fournisseur, produit ou entité puisse être défini par des références uniques et validées, telles que l’adresse, le nom pour un client, une taille, un poids, une référence pour un produit (voir chapitre 5, Gérer les données maîtres : le Master Data Management (MDM)).\n\n\\> Les solutions de gestion de la qualité des données\n\nLa gestion de la qualité des données est un process complexe qui allie autant l’humain et les processus que la technologie et qui se situe tout au long de la chaîne. Je l’ai positionnée dans le Middleware pour la mettre au côté du Master Data Management dont elle bénéficie et pour montrer son importance en amont de la chaîne si nous souhaitons éviter de multiples corrections une fois les données mises à disposition et diffusées (voir chapitre 5, Établir une qualité holistique de la donnée).\n\n\\> Pour conclure\n\nVous comprenez désormais ce rôle absolument clé du Middleware en décisionnel. Vous en connaissez les termes principaux et vous allez pouvoir engager la discussion avec vos homologues de l’informatique qui se réjouiront d’avoir enfin des métiers moins ignorants sur le sujet.\n\nLe dernier mètre de la préparation de la data\n\nDepuis le début des années 2000, nous observons l’émergence d’outils de préparation de données orientés utilisateurs. Ces derniers permettent de couvrir le dernier mètre de la chaîne de préparation avec des solutions agiles, simples et souvent graphiques. Elles n’ont pas vocation à se substituer aux ETL qui gèrent les transferts massifs de data : elles sourcent le plus souvent leurs informations parmi celles raffinées par ces derniers et produisent des tables finales pour simplifier les analyses à suivre.  \n  \nFigure 5. Exemple de processus couvrant le dernier mètre de la préparation de données par un ETL géré par les analystes",
          "Created At": "2025-09-06T13:22:34.060+02:00",
          "Updated At": "2025-09-06T13:22:34.060+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE044_1757157753082",
          "Status": "Ready For Refine",
          "Text": "Ces solutions peuvent être des logiciels dédiés comme Alteryx ou être intégrées dans les solutions d’analyse comme avec Tableau Prep dans Tableau, PowerBI ou le Modeler de Pyramid Analytics.  \n  \nIllustration 3. Exemple de processus de préparation de données avec injection d’une table propre dans une base de données et dans un fichier texte  \nLe processus d’ETL sur Pyramid Analytics permet d’automatiser sur des grands volumes de nombreuses tâches manuelles que nous faisons traditionnellement sous tableur. Les tâches s’enchaînent de manière automatique et s’appliquent aussi bien à dix lignes qu’à plusieurs millions de lignes.\n\nL’émergence du Python pour la transformation de données\n\nLe Python est un langage créé dans les années 1980 par Guido van Rossum et popularisé au début des années 2000. Le nom a été inspiré par Monty Python’s Flying Circus. Si le data pionnier n’a pas vocation à devenir un développeur, il peut néanmoins s’aventurer dans cette discipline pour couvrir des besoins de transformation de données massives. Grâce à sa librairie de traitement de données « Pandas » (comme l’animal), il donne ainsi accès à une grande variété de fonctions capables de manipuler des tables de dizaines voire de centaines de millions de lignes sans effort. Utilisé de manière ciblée, pour éviter le redéveloppement d’usines à gaz telles que celles bâties par nos macros sous tableur, cette option peut s’avérer très utile pour intégrer une donnée raffinée à nos analyses et à nos visualisations.",
          "Created At": "2025-09-06T13:22:34.060+02:00",
          "Updated At": "2025-09-06T13:22:34.061+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE045_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## La base de données\n\nLa présence de la base de données à la suite de l’ETL est à la fois logique et nécessaire : où mettre cette information si bien préparée ? La base de données en décisionnel peut prendre des noms variés en fonction de sa taille ou de sa fonction. Il n’y a pas de règle formelle selon moi, mais on retrouve souvent :\n\n– Data Mart quand elles sont plus dédiées à un domaine donné ou de petite taille ;\n\n– Data Warehouse quand elles couvrent l’ensemble de l’entreprise ;\n\n– Entrepôt de données quand on veut rester franco-français ;\n\n– Data Lake quand on veut faire plus « digital », quand les volumes de data entreposés sont grands, structurés ou non structurés, avec un raffinage limité. Les Data Lake sont parfois complétés par des Data Mart pour entreposer la donnée une fois préparée pour les métiers ;\n\n– Oracle, AS400 ou encore Hana ou BW pour nommer ces entrepôts par la technologie qu’ils utilisent.\n\nCe qui est important, c’est de regarder au-delà des terminologies et de retenir la nature et le rôle de ces bases de données. En décisionnel, ce sont juste des endroits où l’information est rangée pour une analyse ultérieure. Cette clarification va d’ailleurs permettre de tourner les débats vers ce qui compte réellement pour les analyses qui vont suivre.\n\n\\> L’émergence difficile des bases vraiment décisionnelles\n\nNous avons quitté le monde du transactionnel et les objectifs sont maintenant de livrer des analyses pour répondre aux questions et aux challenges de la vie et de la stratégie de l’entreprise.\n\nOr, une tendance régulièrement observée est de reproduire dans ces bases décisionnelles l’exacte copie des bases transactionnelles. Cela part d’une bonne intention qui vise à soulager le transactionnel d’éventuelles requêtes ultradétaillées. Néanmoins, si l’on s’éloigne des analyses vraiment transactionnelles, peu d’analystes vont vraiment travailler au niveau ultragranulaire.\n\nLa plupart du temps, cette option conduit les utilisateurs à récupérer ces tables géantes pour les réduire à grands frais de macros et de copier-coller afin de recréer des bases sous tableur, chacun de son côté.\n\nUne autre tendance est de laisser le contrôle total de ces bases de données à des équipes techniques qui décident de ce qui y sera chargé et sous quelle forme. L’objectif de gouvernance et de contrôle centralisé est louable, mais bride complètement les utilisateurs finaux dans leur capacité d’organiser leur information en fonction de leurs besoins. Le besoin d’analyse d’une entreprise sera différent de celui d’une entreprise concurrente (voir chapitre 2, La tentation du voisin et de la bonne pratique). Si nous devons travailler à partir de la même base standard préparée par une tierce partie sans connexion avec le quotidien métier, nous devrons à chaque fois réajuster en local notre niveau de granularité et notre sélection de métriques, voire les données jointes à notre modèle. Nous sommes repartis pour des manipulations sans fin.\n\nEnfin, aujourd’hui, le monde de l’analytique tourne encore souvent sur tableur. Outre un déficit de culture data, des parcours de formation pas toujours adaptés et un management qui peine à se séparer de ses rapports Excel, une des raisons de cet enracinement des feuilles de calcul dans le monde décisionnel peut s’expliquer par le manque total d’autonomie laissé aux analystes par les solutions de stockage actuelles.\n\nCe n’est pas tant un manque de fonctionnalités des bases de données, mais un manque total de contrôle des chaînes décisionnelles de la part des métiers qui poussent au retranchement dans des solutions que nous maîtrisons et sur lesquelles nous avons toute liberté : le tableur.\n\n\\> Les qualités intrinsèques d’une bonne base décisionnelle\n\nNous devons nous concentrer sur les besoins métier pour comprendre ce qui va être important dans le choix et la gouvernance de ces bases de données.\n\nAnalystes, data scientists et informaticiens doivent travailler de concert dans l’organisation de ces entrepôts de données. On laissera le choix de la technologie aux techniciens, le budget à la finance et le nom de cette architecture aux pinailleurs.\n\nQuels sont les éléments primordiaux et sur quoi il faudra être inflexible, sous peine de retomber sur des proliférations de feuilles Excel et de temps perdu en aval de la base de données ?\n\nLa capacité de la base de données à stocker le bon volume et à traiter des calculs de préparation de données\n\nSouvenez-vous, l’ETL, ou plutôt l’ELT, peut amener à faire porter à la base de données la charge de nombreux traitements de préparation lors de la récupération de données. Aujourd’hui, le coût de stockage est insignifiant et, si nous nous concentrons vraiment sur nos problématiques métier, nos volumes n’atteindront pas les niveaux du transactionnel. L’accès à des puissances de traitement est facile et peu onéreux grâce au Cloud ou à l’achat de serveurs privés. Par conséquent, ce critère sera facilement atteint.\n\nLes données massives en Data Science\n\nLe cas du traitement de données massives en Data Science peut être considéré différemment. Vouloir privilégier et démocratiser des outils ultraperformants et pointus aux dépens des outils d’analytique accessibles peut finir par laisser les analystes sans solutions et les renvoyer dans les tableurs. C’est un peu comme supprimer les petites berlines pour les trajets quotidiens au profit de Formule 1, parce que l’on a un champion de la course automobile dans l’équipe.\n\nLa rapidité de modélisation et d’adaptation\n\nLa majorité des données de base telles que les ventes, la paie, les stocks, les clics sur le site internet, etc. vont peu évoluer dans leur structure. En tant qu’analystes, nous connaissons les métriques et les attributs nécessaires : nous les avons établis, nous les récupérons à chaque ETL et nos rapports de base peuvent fonctionner. Ce qui va faire l’objet d’évolutions régulières sont :\n\n– les nouvelles informations que nous allons devoir collecter pour enrichir nos analyses et répondre à de nouvelles questions métier ;\n\n– les nouveaux points de vue que nous allons considérer pour regarder une situation sur d’autres angles ;\n\n– de nouvelles préparations de données pour des besoins plus raffinés.\n\nIl faudra que bases de données et processus de maintenance soient suffisamment agiles pour coller aux besoins du terrain et préparer la donnée en conséquence.\n\nLa possibilité d’avoir des zones de prototypage\n\nElles sont souvent appelées bacs à sable (sandbox) pour les utilisateurs. La création de ces espaces a plusieurs effets vertueux :\n\n– tester l’intégration et l’utilisation de nouveaux jeux de données dans un environnement data et non pas dans un enchevêtrement de tableurs ;\n\n– engager les analystes à se bâtir une culture data par une pratique régulière proche des règles de l’art.\n\nCe dernier point risque de picoter un peu, car il y aura à la fois une courbe d’apprentissage et une courbe de changement (voir chapitre 6, La courbe de changement digital), mais c’est cette orientation qu’il faut prendre, sous peine de rester coincé dans l’enfer des tableurs.\n\nElle peut être effectuée en plusieurs temps avec tout de même un premier prototypage rapide sur Excel ou fichiers plats, plus un deuxième plus détaillé dans le bac à sable, avant l’intégration éventuelle dans la base de données décisionnelle pour une mise à disposition plus large. Cette dernière étape n’est pas forcément nécessaire, car certaines analyses peuvent être trop spécifiques pour justifier une industrialisation et un partage large au travers de l’entreprise.\n\nTableur contre prototypage en « bac à sable »\n\nLa première étape sur feuille de calcul permet d’être extrêmement agile dans un environnement que l’on connaît. Cependant, essayer de l’éviter le plus souvent possible accélère l’apprentissage des réflexes « base de données » par l’usage. L’excellence acquise par la répétition permet de prototyper aussi vite dans le Data Warehouse que dans Excel. Ce peut être plus lent au démarrage certes, mais ce retard est plus que comblé, lors des rafraîchissements de données et des calculs, par l’identification précoce de challenges dans l’organisation des informations et par l’économie d’un portage ultérieur du tableur à la base de données. Il permet de plus une application d’outils de reporting dès le départ, évitant ainsi la création de rapports temporaires sur tableurs.\n\nCes prérequis sont rarement pris en compte dans les choix de bases de données. Nous faisons de ces bases décisionnelles une affaire de technologie ou de data brute, alors qu’elles représentent le point de contact entre l’informatique et les métiers et qu’elles devraient être un formidable travail collaboratif… Des Data Warehouse sous-utilisés et des armées de travailleurs sous Excel ruinent les grands objectifs d’une data démocratie.\n\nUn article8 évoque la nécessité de donner aux utilisateurs décisionnels des bases de données qu’ils peuvent vraiment maîtriser, quitte à laisser de côté certaines avancées technologiques et un peu de performance. À quoi servent-elles en effet si on ne peut pas les maîtriser ?\n\nRetour vers le futur\n\nPendant la dernière décennie, les entreprises ont fait l’expérience des bases de données avancées (type NoSQL) (voir chapitre 5, Les bases de données NoSQL (Not Only SQL)) en y voyant une arme absolue pour stocker toujours plus de données et réaliser des calculs toujours plus gourmands en ressources.  \nMais, très rapidement, elles se sont rendu compte que des données entreposées sans contrôle et sans but n’avaient pas de valeur. Elles ont aussi réalisé que ces bases de données, d’accès complexe et technique, était coupées du monde des analystes et n’apportaient pas leurs promesses analytiques car :  \n– elles nécessitaient un support technique chevronné, laissant les métiers sans autonomie et les informaticiens peu formés sans contrôle ;  \n– elles étaient lourdes en codage et en maintenance ;  \n– elles ne pouvaient pas coller à une réalité opérationnelle changeante ;  \n– elles stockaient une donnée peu raffinée et impropre pour une analyse rapide et fiable ;  \n– elles n’étaient pas accessibles avec les langages familiers des analystes tels que le SQL ;  \n– elles n’étaient pas structurées en tables, plus naturelles à comprendre ;  \n– elles n’étaient pas toujours ACID (atomicité, consistance, isolation et durabilité) (voir chapitre 5, Les bases de données relationnelles) ;  \n– enfin, paradoxalement, elles n’étaient pas aussi rapides que certaines solutions conventionnelles, car elles passaient parfois plus te temps à distribuer les données et les traitements sur plusieurs machines qu’à effectivement réaliser les calculs.  \nRapidement les entreprises, même les plus avancée, ont dû réintroduire des approches plus conventionnelles pour maintenir la data et les métiers en prise.\n\n\\> La place du Cloud dans le traitement des données\n\nLa facture des services Cloud est arrivée.\n\nLe Cloud ne change rien aux configurations détaillées plus haut. Comme bien souvent, les nouvelles technologies ne modifient pas les paradigmes de base de la data : sa chaîne de valeur et de traitement est restée la même depuis la naissance de l’humanité. Le Cloud est une formidable révolution, mais, pour notre pratique décisionnelle, s’attarder sur le jargon en se perdant sur des détails techniques n’est pas pertinent. Mon confrère et ami Edward Roske, directeur général d’InterRel (société de consulting en analytique) et un des experts en Business Analytics aux États Unis en donne deux définitions :\n\n– le Cloud, c’est juste un ordinateur avec un très long câble réseau ;\n\n– le Cloud, c’est juste vos applications qui tournent sur un ordinateur qui ne vous appartient pas.",
          "Created At": "2025-09-06T13:22:34.061+02:00",
          "Updated At": "2025-09-06T13:22:34.061+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE046_1757157753082",
          "Status": "Ready For Refine",
          "Text": "Finalement, nous avons tous des expériences similaires au Cloud, lorsque nous nous connectons en interne à nos serveurs d’entreprise. Pour l’utilisateur, ces changements de paradigme technologique ne sauraient justifier l’abandon de la discipline et de la rigueur avec laquelle il doit gérer sa data. Le Cloud n’est pas plus magique que les systèmes en local.\n\nNous verrons dans le chapitre 5 les différents types de base de données et leurs domaines de prédilection.",
          "Created At": "2025-09-06T13:22:34.062+02:00",
          "Updated At": "2025-09-06T13:22:34.062+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE047_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## Les solutions d’analyse\n\nDans ce chapitre sur les outils, j’aurais pu ne parler que de ces merveilleuses solutions d’analyse qui transforment la donnée en information raffinée, en graphiques colorés ou en tableaux riches de calculs en ligne et en colonne.\n\nMais cela aurait été comme parler en premier du glaçage d’un gâteau dans une recette de cuisine : j’aurais omis tout le travail en amont pour réaliser le gâteau. En effet, une fois notre donnée collectée, raffinée, préparée et stockée, le travail d’analyse ne repose plus vraiment sur de la technologie, mais surtout sur notre sens critique et notre curiosité.\n\nLes solutions d’analyse sont aussi appelées solution de Business Intelligence, parfois de reporting ou encore par le nom du vendeur de logiciel ou de son logiciel (tableau, PowerBI, Qlick, Spotfire, Pyramid Analytics… Excel). Encore une fois, ce qui nous intéresse n’est pas le nom et les débats sans fin pour faire la différence entre du Data Analytics, du Mining ou de la Business Intelligence. Nous devons nous concentrer sur l’objectif de cette étape : trouver les pépites d’information nécessaires à la prise de décision.\n\nMême si elles varient d’une solution à l’autre, la plupart des solutions d’analyse offrent les mêmes fonctionnalités : la data étant ce qu’elle est, il n’y a pas cent façons d’en faire des tableaux et des graphes.\n\n\\> La présentation de la donnée\n\nPouvoir prendre des en-têtes de colonnes d’un clic et les déposer dans des cases afin de bâtir les analyses, sans se soucier des risques d’altération de données, d’oubli de cellules et avec une rapidité jusque-là jamais atteinte a changé notre façon de travailler.\n\nNous y gagnons en temps, en précision, en qualité de restitution, en performance et en confort : tout cela en même temps. Seule est sacrifiée l’option de pouvoir retoucher, altérer ou cosmétiser son rapport à la dernière minute par le changement d’une cellule ou l’ajout d’un élément graphique atypique.\n\nIllustration 4. Écran typique d’une solution analytique\n\nLes colonnes disponibles pour l’analyse sont à gauche, les « étagères » pour les cliquer-déposer et former les tableaux et graphiques sont dans la colonne adjacente sur la droite. Les différentes étagères pilotent ce qui va être en ligne, en colonne, en couleur, en taille, en filtre, en infobulle, etc. Le résultat apparaît dynamiquement dans la partie centrale, avec sa légende à droite.\n\nCette représentation a été réalisée en quelques secondes sur la base de plusieurs sources de plusieurs dizaines de milliers de lignes chacune, seulement à la souris9.\n\n\\> Les calculs finaux\n\nIl est toujours préférable de concentrer les efforts de calcul en amont de la phase d’analyse. Néanmoins, certaines opérations devront attendre cette dernière phase :\n\n– les calculs ad hoc visant à répondre à une question précise. Ils peuvent se faire à partir de colonnes, parfois de lignes, avec une grande diversité de formules ;\n\n– les agrégations en somme, moyenne, comptage, minimum/maximum, etc. de la donnée pour obtenir un niveau de granularité pertinent ;\n\n– la transformation de certains champs pour des raisons esthétiques ou pour corriger des problèmes de qualité de dernière minute (voir chapitre 5, Tirer le meilleur de sa donnée avec une bonne préparation).\n\n\\> La création de mini-processus analytiques en local\n\nComme nous l’avons vu plus haut, certaines solutions d’analyse offrent tout ou partie de la panoplie des outils data à leur utilisateur pour lui apporter autonomie et lui donner la possibilité de développer localement des analyses à partir de données brutes.\n\nLes solutions de BI actuelles offrent souvent :\n\n– des capacités d’ETL pour aller chercher de la donnée qui n’a pas encore été prise en compte dans les bases de données ;\n\n– des capacités de préparation pour raffiner localement des tables brutes, sans avoir à passer par des étapes manuelles ou des tableurs ;\n\n– la possibilité de créer en mémoire une mini-base de données pour organiser sa donnée proprement avant l’analyse.\n\nOn pointe souvent le risque de création de shadow IT ou shadow datamart (développement utilisateurs fait en dehors du contrôle de l’informatique et de la gouvernance d’entreprise). Je préfère considérer le potentiel de ces outils qui permettent le maquettage rapide de nouvelles analyses par les utilisateurs, dans un contexte de bonne pratique data et avec des développements qui pourront être facilement portés vers le Data Warehouse.",
          "Created At": "2025-09-06T13:22:34.063+02:00",
          "Updated At": "2025-09-06T13:22:34.063+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE048_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## Le reporting\n\nBien souvent, les solutions analytiques incluent un volet reporting pour connecter directement la production d’analyses à sa diffusion. Cette fonction est particulièrement pratique, car elle permet d’intégrer une couche cosmétique et de diffuser ses résultats sans avoir à changer d’environnement. Elle incite néanmoins parfois à bâtir des usines à gaz pour des présentations parfaites au pixel près et fait perdre de vue l’objectif principal.\n\nCette dernière étape est trop souvent confondue ou incluse avec la phase d’analyse. L’analyste veut se faire plaisir et devient un Picasso du reporting aux dépens de l’analyse et de l’investigation. Il est souvent sous la pression du demandeur du rapport qui peut exiger une œuvre avec laquelle il pourra parader en réunion, sous le regard fier de l’analyste qui affichera avec fierté : « C’est moi qui l’ai fait. »\n\n\\> À quoi sert le reporting ?\n\nSi l’on considère que l’analyse a été bouclée en aval, il ne reste plus qu’à préparer la mise en forme et la diffusion de cette information. Quels sont les éléments qui doivent nous guider pour ne pas rater cette tâche ?\n\nIl faut revenir à la question de base : quel est l’objectif d’un reporting ?\n\nUn reporting efficace et utile doit entraîner :\n\n– une décision : nous comprenons la situation, nous voyons le problème, nous sommes informés et nous décidons ;\n\n– une discussion : nous avons repéré des éléments intéressants, mais nous avons besoin d’être orientés, voire aidés pour continuer notre démarche. Nous déclenchons alors un échange avec notre hiérarchie, nos équipes ou nos confrères ;\n\n– un confort, une confirmation : la situation est sous contrôle et nous pouvons continuer à nous consacrer aux problématiques qui comptent pour l’entreprise.\n\nEn d’autres termes, un bon reporting permet que nos analyses aient un impact et que tout ce travail de préparation-stockage-analyse de données ait un retour sur investissement.\n\nLa taille, l’épaisseur, les couleurs et le degré de raffinement sont-ils importants pour remplir ces objectifs ? Cela va dépendre de l’audience. Choisir le format du reporting doit être directement relié à ce que les lecteurs préfèrent recevoir. Nous avons tous nos sensibilités : certains d’entre nous préfèrent lire, d’autres aiment les images, j’aime personnellement une conversation qui va droit au but. Cer­tains veulent beaucoup de détails pour se rassurer, d’autres demandent juste la synthèse, quant à moi, je préfère une présentation à géométrie variable au tableau blanc. Si nos yeux sont fatigués, il nous faudra de gros caractères ; si nous sommes pinailleurs, il faudra des titres, des sous-titres et des légendes partout ; si nous avons confiance en nos équipes, des informations simples et pertinentes suffiront.\n\nUn bon rapport est un rapport qui « déclenche » une action.\n\nOn peut dès lors se poser la question de la valeur « temps passé » par les équipes à peaufiner des rapports jusqu’au moindre détail de police de caractères. S’il est normal de présenter des documents propres avec un format constant dans la durée pour que le lecteur s’y retrouve, est-il vraiment raisonnable d’utiliser le temps de brillants analystes à cela, quand nous aurions pu nous contenter d’une simple page d’analyse, d’un bref mémo et d’une discussion ?\n\nL’exemple suivant illustre un cas extrême.\n\n→ La Heat Map de Meena\n\nMeena est une jeune analyste au parcours académique brillant dans une des sociétés de pointe de la Silicon Valley au sein du service d’analyse financière. Sa renommée en interne, qui a fait le tour des services, s’est bâtie sur sa fameuse Heat Map, un rapport d’analyse d’écart qui affiche en matrice, par filiale et centre de coût, l’importance des écarts réel/budget par voie d’intensité de couleur. Les fonctionnalités de la dernière version d’Excel, si longuement attendues, lui permettent d’afficher des couleurs dégradées et chatoyantes qui font l’admiration des collègues.\n\nMeena passe deux à trois jours par mois à faire ces graphiques, soit entre 10 et 14 % de son temps mensuel hors week-end. Ses automatisations requièrent plus de maintenance que prévu et il y a toujours un nouveau détail graphique qu’il est bon d’ajouter. À chaque fin de période comptable, elle présente à son directeur financier cette impressionnante liasse bigarrée d’une vingtaine de pages, imprimée en cinq exemplaires. Sa contribution à la déforestation et à l’achat de cartouches d’encre couleur est palpable.\n\nL’échange est irréel : son patron se penche sur le document, acquiesce devant la qualité graphique et l’impeccable maîtrise du tableur et, après quelques minutes de revue, pointe du doigt les zones rouge orangé en demandant : « Nous sommes clairement en décalage ici. Meena, tu as plus d’information ? » Meena approuve cette analyse fine du chef. Avec un regard entendu, elle se lève et répond : « Je vais m’en occuper, boss, pas de problème. Je reviens avec l’info demain. »\n\n» À votre avis, quels sont les éléments qui pourraient être améliorés pour gagner du temps sur vos rapports actuels ? Quelles optimisations permettraient de vous concentrer sur ce qui compte et de finalement consacrer vos compétences à ce qui est important ?\n\nJe vous propose les observations suivantes, avec une formulation volontairement provocatrice, qui ne tiennent pas compte du fait que Meena et son patron avaient forcément les meilleures raisons de procéder de la sorte (et c’est là ma première observation provocatrice) :\n\n– pourquoi passer plus de 10 % de son temps à faire de la cosmétique sur tableur ? Quand on est diplômé d’une grande université ou riche d’une grande expérience, est-ce la meilleure utilisation de son temps ?\n\n– pourquoi produire à grands frais un rapport qui n’est qu’une étape d’une démarche d’analyse d’écart ?\n\n– pourquoi ne pas simplement présenter les écarts en rouge et, avec le temps gagné par la simplification du rapport, investir dans les analyses de ces écarts ?\n\n– pourquoi passer par une représentation graphique chatoyante quand l’audience est une personne qui veut juste aller droit au but ?\n\n– pourquoi le directeur financier, pourtant garant de la rentabilité de l’entreprise, laisse-t-il autant de valeur salariale et humaine se perdre dans des activités aussi futiles (le coût complet de ce rapport jetable mensuel, fondé sur le taux horaire de la personne, était à l’époque de 3 000 dollars) ?\n\n\\> Les grandes familles de reporting\n\nÀ la question « quel type de reporting choisir ? », la réponse est à la fois :\n\n– simple : le reporting qui aura un impact sur son lecteur ;\n\n– et compliquée : le reporting qui aura un impact sur son lecteur.\n\nCe qui nous laisse à la fois la liberté dans les options, mais également la difficulté de choisir quelles options. Aussi, je vous propose de les revisiter non pas en décrivant leur infinie variété, mais en fonction de leur performance au regard de la rapidité avec laquelle ils peuvent amener à faire prendre une décision. Nous verrons les grands types de visualisations plus loin dans le chapitre.\n\nOn peut considérer deux grandes familles de reporting :\n\n– les revues récurrentes telles que les reportings mensuels, quotidiens, etc., qui s’assurent du maintien d’une performance ou d’une orientation stratégique ;\n\n– les revues ad hoc pour répondre à une question particulière.\n\nLes revues récurrentes\n\nLes reportings récurrents sont ceux qui donnent le pouls d’une activité : ils rassurent, ils visualisent les grandes tendances, les indicateurs clés de l’entreprise. Parmi les classiques, on retrouve les rapports comptables, d’écarts budgétaires, de vente, de production, etc. Ils ne sont pas à proprement parler des vecteurs de décision, mais plutôt des supports d’information pour donner à tout le monde le choix de se faire une opinion sur le cours de l’activité ou d’un projet.\n\nSont-ils de ce fait en opposition avec les objectifs plus haut ? Non, s’ils sont le support d’une discussion qui à son tour mène à des décisions et à des actions. Dans ce cas, nous pourrions arguer que cela devrait être à l’analyste d’arriver déjà préparé avec ses propres analyses complémentaires et des premières idées à travailler ou à trancher. Si ces rapports sont le support d’une vraie réunion de travail, dans laquelle des experts partagent ouvertement leur réflexion sur ces données communes dans le but de définir des plans d’actions ou bien sont l’occasion de connecter des éléments que l’analyste ne pourrait pas saisir à son niveau, ils justifient alors pleinement leur fonction.\n\nLes rapports tels que les bilans, les comptes de résultat, les liasses fiscales, les rapports annuels, qui visent plutôt l’information générale, parfois réglementaire, de lecteurs qui pourront poser des questions, mais n’auront pas directement ou pas du tout de capacité de décisions opérationnelles (actionnaires, fisc, communautés, etc.), ne rentrent pas dans ces critères. Il s’agit plus de communication et nous verrons qu’ils peuvent être produits différemment (voir infra, Les infographies et autres publications sur mesure). On pourra arguer que l’analyste se doit de préparer de nombreuses analyses pour valider et expliquer les chiffres dans le cas où un texte d’accompagnement est nécessaire, ou simplement pour se préparer aux questions des parties tierces.\n\nCompte tenu de leur valeur décisionnelle, ces reportings récurrents de situation ne devraient pas encombrer les journées des équipes opérationnelles. Parce que ces rapports sont souvent standardisés et stables dans le temps, ils devraient être complètement automatisés, une fois leurs sources de données validées et expliquées… par les analyses ad hoc.\n\nLe reporting ad hoc\n\nCes rapports peuvent avoir un caractère récurrent comme exceptionnel : leur point commun est que ce ne sont pas des rapports formels et fixes . Ils visent l’analyse d’un point ou d’un domaine particulier dans le but d’expliquer une problématique et de déclencher une action ou une réflexion de plan d’action. Ils reflètent la séparation saine entre la phase de recherche et d’analyse et celle de la communication de résultats dans laquelle l’analytique n’est pas freinée par des considérations esthétiques.\n\n\\> Les grands formats de reporting\n\nL’objet n’est pas ici d’établir un inventaire de modèles à suivre, mais plutôt de décrire les options disponibles, celles auxquelles vous n’auriez pas pensé, que vous auriez dédaigné ou qui pourraient vous inspirer. Nous allons également lister leurs points forts (quoique le réel point fort d’un reporting soit, encore une fois, sa capacité à générer des actions et des réflexions) et surtout leurs points faibles ou leurs inconvénients, car c’est souvent là que beaucoup de valeur se perd.\n\nLe tableau de bord\n\nIl vise à rassembler tous les indicateurs nécessaires pour le suivi d’une activité. Sous format électronique ou papier, il donne une vue d’ensemble d’une situation à travers plusieurs angles et permet de l’analyser dans sa globalité.\n\nAttrayant, coloré, dynamique dans sa version électronique, il a été pendant longtemps et est certainement encore la Rolls-Royce du reporting (on entend encore souvent résonner dans les salles de conseil d’administration : « Je veux mon tableau de bord ! »). Sa création et sa gestion posent toutefois quelques questions :\n\n– il est coûteux en temps et en complexité de mise en œuvre. Il nécessite un important travail de collecte, d’agrégation et d’alignement de données variées. Il requiert une maîtrise de la mise en page et/ou de la technologie qui le supportera s’il est électronique ;\n\n– il est également peu agile. Reconstruire ou adapter un tableau de bord n’est pas toujours trivial.\n\nDans le contexte d’un tableau de bord couvrant les fondamentaux d’une activité, on peut trouver un bon retour sur investissement. Ces fondamentaux n’évoluant que très peu en principe, le tableau de bord peut valoir la peine d’être développé consciencieusement.\n\nLe rapport",
          "Created At": "2025-09-06T13:22:34.063+02:00",
          "Updated At": "2025-09-06T13:22:34.063+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE049_1757157753082",
          "Status": "Ready For Refine",
          "Text": "Ce dernier prend souvent la forme papier, ou papier électronique (PDF). Son format reprend souvent les codes et les styles des documents de l’entreprise. Ils font l’objet d’une attention minutieuse dans leur création, avec des alignements, des fontes et des couleurs. Leurs créateurs rivalisent souvent d’inventivité pour présenter des nouvelles techniques de mise en page.\n\nLe rapport peut trouver sa justification s’il trouve sa cible et crée l’action comme tout autre reporting. Il souffre néanmoins de quel­ques faiblesses qu’il est intéressant de considérer avec pragmatisme :\n\n– comme les tableaux de bord, les rapports sont complexes à bâtir et à maintenir ;\n\n– ils restent souvent et jalousement « la propriété » et sous le contrôle exclusif de ceux qui les ont créés. Leur existence se limite souvent à la durée de poste de leurs créateurs. Ces œuvres d’art seront souvent remplacées à l’arrivée de nouveaux responsables qui sans nul doute pourront faire mieux, plus vite et surtout plus beau, en profitant des nouvelles fonctions gadgets des versions logicielles ultérieures ;\n\n– ils n’offrent pas la possibilité d’interactions avec la donnée et doivent donc faire un compromis habile entre concision et couverture d’un sujet et de ses variations ;\n\n– leur automatisation peut être obtenue comme celle du tableau de bord, mais leur complexité intrinsèque conduit souvent à des ajustements, à des corrections, à des « bidouillages » et à des crises de nerfs sur des macros et des liens interfichiers qui consomment toujours beaucoup plus de temps que prévu ;\n\n– l’habitude créée par un format particulier force parfois les analystes à tordre leurs outils BI pour leur faire reproduire les formats livrés par les solutions précédentes au pixel près.\n\nIl en résulte souvent que ces rapports sont rarement accompagnés d’analyses détaillées, faute de temps pour les réaliser : alors qu’ils devraient être le point de départ de l’analyse, ils en sont souvent la fin.\n\nLe rapport reste néanmoins une valeur sûre dans les chaînes de communication, même s’il n’est peut-être plus en parfaite adéquation avec les exigences d’un monde qui évolue chaque jour.\n\nLe tableau de chiffres\n\nLes tableaux de chiffres restent très populaires. Ils présentent moins de complexité à bâtir, même si la quête du style parfait continue à animer des analystes en besoin de créativité artistique.\n\nIls sont particulièrement efficaces pour les personnes qui aiment s’immerger dans les chiffres pour alimenter leur courant de pensée. Ils sont aussi redoutables pour mettre à plat une situation de manière détaillée avec des matrices de données ou des tableaux croisés.\n\nNéanmoins, ils demandent à leur lecteur un travail d’analyse. Dans certains contextes, il s’agit d’une phase intéressante de supervision d’analyse comme pour le rapport, voire le tableau de bord. Ces revues à plusieurs niveaux permettent de repérer davantage d’erreurs ou de problèmes. Mais ils s’adressent souvent à des lecteurs cadres supérieurs, voire dirigeants, qui devraient se concentrer sur des missions plus stratégiques. Éplucher les chiffres produits par ses équipes traduit soit un manque de confiance en elles, soit une nostalgie d’un temps où ces responsables étaient eux-mêmes analystes, soit un artifice pour repousser au niveau hiérarchique suivant la responsabilité de l’action ou de la décision.\n\nLes visualisations classiques\n\nSouvent inclus dans les tableaux de bord ou les rapports, ce média peut aussi être un reporting à lui seul. L’image qui vaut mille mots peut aussi valoir des millions de chiffres tant est grande sa capacité à synthétiser une situation complexe.\n\nQuel est le meilleur graphique pour une situation donnée ? Cela dépend de son objectif : toujours et encore déclencher une action, une décision ou une réflexion. Si certaines visualisations sont plus adaptées à certains cas, leur choix devrait dépendre de leur capacité à faire bouger leur audience en attirant l’attention par exemple sur une tendance, une différence, une concentration ou une exception atypique, en jouant sur les formes, les tailles et les couleurs et leur arrangement.\n\nParmi les visuels classiques, nous allons retrouver :\n\n– les graphiques en barres ;\n\n– les graphiques en ligne ;\n\n– les « camemberts ».\n\nL’application de quelques règles fondamentales permet de leur conserver utilité et pertinence : le simple a parfois du bon.\n\nLe graphique en barres va permettre de comparer des tendances et/ou des répartitions. Il peut être exprimé sous plusieurs variations telle que :\n\n– les barres horizontales ou verticales ;\n\n– les barres superposées ou côte à côte ;\n\n– les barres qui montrent les proportions ;\n\n– les barres dont la largeur varie ;\n\n– les barres qui s’enchaînent en partant du niveau de la précédente (graphique en cascade) ;\n\n– les barres boîtes à moustaches, aussi appelée box-and-whisker plot.\n\nIllustration 5. Graphiques en barres\n\nIllustration 6. Exemple de graphique en cascade : suivi de l’évolution d’un solde bancaire\n\nLes « boîtes à moustaches »\n\n  \nIllustration 7. Exemple de graphique en boîte à moustaches : revue de l’espérance de vie d’une sélection de pays à la fin des six dernières décennies  \nLa dernière boîte indique un « haut de moustache », à savoir l’espérance de vie la plus haute à 80 ans, un premier quartile à 77 ans et une médiane à 71 ans. On observe facilement l’amélioration de l’espérance de vie avec une amplitude entre les pays qui reste assez stable sauf en 2000.  \nLes boîtes à moustaches sont un moyen simple de visualiser les éléments clés d’une série statistique. Elles synthétisent la médiane, les quartiles/déciles, le minimum et le maximum des populations mesurées. Le rectangle central va du premier quartile au troisième quartile. Il est coupé par la médiane. Les segments aux extrémités donnent les valeurs extrêmes, ou les premier et neuvième déciles. Ce sont les moustaches du graphique.\n\nLeur application idéale est pour comparer des valeurs entre grou­pes, tels que des pays, des mois ou des clients par exemple. Le choix d’un format horizontal, vertical ou d’une variante dépend du message à passer.\n\nAttention, les graphiques en barres utilisés pour des nombres de catégories élevées perdent de leur lisibilité. Des catégories telles que des jours pourront mieux être représentées par une ligne.\n\nLes graphiques en ligne(s) montrent des tendances, des saisonnalités sur des catégories plus continues.\n\nOn retrouve :\n\n– les lignes qui relient les points de chaque catégorie ;\n\n– les lignes lissées (spline) ;\n\n– les lignes à paliers, qui représentent les plafonds d’un graphique en barre invisible ;\n\n– les lignes seulement désignées par leurs points.\n\nIllustration 8. Graphiques en ligne(s)\n\nIllustration 9. Exemple de graphique en ligne : production de bière d’un pays en milliers d’hectolitres par mois\n\nOn observe clairement sur l’illustration 9 les grandes tendances ainsi que les variations saisonnières.\n\nIllustration 10. Variations du graphique de l’illustration 9 avec différents types de lignes\n\nIl est à noter que la dernière itération, avec les mois colorisés, permet de mieux suivre l’évolution d’un même mois d’une année sur l’autre.\n\nEn dernier type de visualisation classique, nous trouvons les camemberts. En variation des camemberts, nous retrouvons :\n\n– les camemberts en secteurs\n\n– les « donuts » avec un trou au milieu ;\n\n– ou plus récemment les pyramides ou les entonnoirs.\n\nIllustration 11. Graphiques en camemberts\n\nL’utilisation des camemberts est moins évidente, car elle introduit des biais d’optique et, lorsque le nombre de catégories augmente, ces représentations sont moins lisibles. Beaucoup de praticiens lui préfèrent le graphique en barres. Je vous laisse juger de la pertinence de l’un ou de l’autre.\n\nIllustration 12a. Graphique en barres : ventes de véhicules hybrides et électriques aux États-Unis en 202010\n\nIllustration 12b. Graphique en camembert : ventes de véhicules hybrides et électriques aux États-Unis en 202010\n\n» Quel graphique trouvez-vous le plus lisible ?\n\nLes visualisations avancées\n\nLes visualisations avancées sont l’évolution naturelle des graphi­ques tels que nous les avons connus sous tableur et outils de Business Intelligence. Leur développement s’est fait en parallèle avec l’avènement des technologies Big Data afin de représenter de façon concise des situations complexes et de gros volumes de data. Les capacités des cartes graphiques et des moniteurs, le développement de bibliothèques de graphiques et de langages tels que le Python ont permis leur démocratisation.\n\n• Une infinité de possibilités\n\nOn peut accéder à de nombreux graphiques avancés en standard sur les logiciels d’analyse.\n\nIllustration 13. Options de graphiques classiques ou avancés dans Tableau, PowerBI et Pyramid Analytics\n\nCertains peuvent être ajoutés via des places de marché ou via l’intégration de bibliothèques comme sur Microsoft PowerBI ou sous langage Python.\n\nIllustration 14. Fenêtre d’accès de PowerBI à des visualisations complémentaires\n\nEn sachant appliquer un peu de code simple, et sur la base de données bien préparées (nous verrons qu’un simple fichier CSV peut suffire), il existe de nombreuses bibliothèques accessibles en ligne.\n\nIllustration 15. Galerie de la bibliothèque D311 (accessible avec du Javascript)\n\nIllustration 16. Extraits de la galerie de la bibliothèque Matplotlib (accessible avec du Python)\n\nLe choix pléthorique de formats ne doit pas faire oublier les objectifs de ces graphiques, sous peine de tomber dans les mêmes pièges qu’avec les reportings gadgets qui deviennent plus des passe-temps. Voici quatre exemples d’applications durables possibles pour des travaux d’analyse.\n\n• Le graphique en bulles\n\nPour une analyse multicritère visant à se concentrer sur la recher­che de situations atypi­ques (outliers) ou de corrélation, le graphique en bulles est très performant. Il permet, sur les deux dimensions d’un papier ou d’un écran, de visualiser simultanément cinq aspects d’une data, voire six ou sept s’il est possible de faire bouger le graphique sur l’écran.\n\nCe type de graphique fait apparaître quatre métriques ou attributs de la donnée :\n\n– sur l’axe X, l’abscisse ;\n\n– sur l’axe Y, l’ordonnée ;\n\n– selon la taille de la bulle ;\n\n– selon la couleur de la bulle.\n\nUne cinquième facette de la donnée peut apparaître dans la forme de la bulle : carré, triangle, croix, losange, etc. Une sixième peut être ajoutée sous la forme d’un curseur faisant varier par exemple la date. Une septième pourra même être un troisième axe dans le graphe.\n\nIllustration 17. Exemple de graphique en bulles : espérance de vie et PNB par pays et par continent\n\nLe diagramme de l’illustration 17 reprend un travail de Hans Rosling12. La magie de ces graphiques est que, même si les détails sont petits, on visualise immédiatement :\n\n– une corrélation intéressante entre PNB et espérance de vie ;\n\n– les grands pays (Chine, Inde, États-Unis, Brésil) et le peloton des grands pays industrialisés d’Europe ;\n\n– l’existence d’exceptions avec des bulles hors de la tendance qui pourraient être des champs immédiats d’investigation. Quelques bulles en bas du groupe posent question.\n\n• Le diagramme en cordes (chord diagram)\n\nCette représentation permet de visualiser des relations entre zones, entités ou personnes. La taille de la corde représente la grandeur de la mesure, sa couleur le sens ou l’origine du flux et les arcs de cercle en périphérie leur source et leur origine.\n\nFigure 6. Exemple de diagramme en cordes : trajets de courses de taxi dans une ville\n\nDans la figure 6, les quartiers de départ et d’arrivée forment le cercle. Les cordes représentent les trajets tandis que leur épaisseur représente leur nombre. Combien de secondes nous faut-il pour identifier les trois voire quatre courses les plus fréquentes ? Voilà un exemple de visualisation qui est rapidement claire et compréhensible, même si elle peut être déconcertante de prime abord.\n\n• Le diagramme Sankey (Sankey diagram)",
          "Created At": "2025-09-06T13:22:34.064+02:00",
          "Updated At": "2025-09-06T13:22:34.064+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE050_1757157753082",
          "Status": "Ready For Refine",
          "Text": "Ce diagramme représente à merveille des flux dans un processus à plusieurs étapes. En partant de la gauche, il montre comment des flux entrants se répartissent au cours de leur traitement jusqu’à leur destination finale.\n\nFigure 7. Exemple de diagramme Sankey : emplois/ressources d’une ville\n\nLes applications en finance, en logistique et même en RH ouvrent la voie à de nouvelles façons de représenter des situations complexes de flux. Elles peuvent rendre palpables des constats et les exposer au plus grand nombre, pour des prises de conscience et des actions !\n\nLes infographies et autres publications sur mesure\n\nPourquoi dépenser de l’argent et du temps pour livrer des représentations raffinées de nos analytiques, alors que nous venons de dire qu’il était primordial de nous concentrer sur leur impact ? Les plaquettes et autres illustrations sont ici destinées à une population très large ou d’une grande diversité de sensibilité, de perception ou d’objectifs, qui ne dispose pas de beaucoup de temps pour des recherches et a besoin d’un média commun afin de se faire une opinion ou de suivre une recommandation. Souvent, ces publications sont rendues obligatoires par une loi, une réglementation ou une habitude. On trouvera par exemple des plaquettes annuelles, des rapports pour des conseils d’administration ou de larges campagnes d’information.\n\nLa production de ces documents est souvent plus coûteuse en design (et en papier). Le retour sur investissement ne se justifie plus par l’impact des actions qui vont être prises, mais par le nombre de personnes touchées ou par le format qualitatif rassurant qu’ils vont livrer à des groupes. Nous passons ici du côté reporting au côté promotion et communication.\n\nLe mémo\n\nÉlément paradoxal d’un bon reporting : il devient inutile dès qu’il est publié. En effet, si les points présentés sont clairs et étayés, son lecteur doit immédiatement passer à la phase suivante de réflexion et d’action. Selon moi, un bon reporting s’efface du paysage dès qu’il est lu.\n\nLes rapports qui nécessitent de longues revues et des manipulations de filtre sans fin montrent que :\n\n– soit l’analyse n’est pas finie et qu’il faut continuer à creuser, auquel cas ce n’est pas un bon rapport ;\n\n– soit son lecteur est ou se prend toujours pour un analyste, ou il n’a pas confiance en ses analystes.\n\nDans les deux cas de figure, le « mauvais rapport » illustre souvent un problème plus profond de procédure, de compétence ou de politique.\n\nPar conséquent, pourquoi ne pas se contenter d’un bref mémo listant les conclusions de l’analyse de la semaine, illustré en annexe par quelques points de suivi généraux et surtout des zooms sur les points analysés en détail ?\n\nLe mémo de Jeff\n\nLe fondateur et P.-D.G. Jeff Bezos révèle que les dirigeants de l’entreprise « ne font pas de PowerPoint » ou toute autre présentation sous forme de diapositives. Au lieu de cela, ils « créent des notes narratives de six pages qui sont lues au début de chaque réunion, un peu comme une session de “salle d’étude” ».  \nSelon Bezos, « la raison pour laquelle écrire un “bon” mémo de quatre pages est plus difficile que “rédiger” un PowerPoint de vingt pages est que la structure narrative d’un bon mémo oblige à mieux réfléchir et à mieux comprendre ce qui est plus important que quoi. \\[…\\] Souvent, lorsqu’un mémo n’est pas génial, ce n’est pas l’incapacité de l’écrivain à reconnaître le niveau élevé, mais plutôt une mauvaise attente sur la portée13 ».\n\nChoisissez votre légende ! La distribution du mémo de reporting : les chiffres ne sont pas bons. Il n’y a plus de papier dans l’imprimante pour sortir le rapport. Excel a buggé et ils ne vont pas finir dans le temps.",
          "Created At": "2025-09-06T13:22:34.065+02:00",
          "Updated At": "2025-09-06T13:22:34.065+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE051_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## Pour conclure\n\nContrairement aux apparences, aux croyances et au marketing, les solutions décisionnelles ne se limitent pas à un outil d’analyse. Un processus d’analyse doit couvrir une chaîne qui va de la collecte de l’information brute jusqu’à la livraison d’une conclusion. Le data pionnier devra toujours avoir à l’esprit que c’est l’alignement de solutions sur l’ensemble de cette chaîne qui fera la performance et la qualité de ses analyses (voir chapitre 7, L’optimisation des neuf étapes d’un processus analytique).",
          "Created At": "2025-09-06T13:22:34.066+02:00",
          "Updated At": "2025-09-06T13:22:34.066+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE052_1757157753082",
          "Status": "Ready For Refine",
          "Text": "# Comment passer des solutions transactionnelles aux solutions décisionnelles ?\n\nLa compréhension des solutions décisionnelles, comme nous venons de le voir, est aisée. Pourquoi les analystes rechignent-ils à quitter leurs processus manuels pour embrasser leur puissance et leur confort ? Plusieurs facteurs organisationnels et humains freinent ces évolutions.",
          "Created At": "2025-09-06T13:22:34.066+02:00",
          "Updated At": "2025-09-06T13:22:34.066+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE053_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## L’accès à l’équipement\n\nSi la technologie n’est pas une condition sine qua non pour le traitement de données, elle devient rapidement cruciale pour des traitements rapides et fiables, FAIR et fluides (voir chapitre 3, Collecter une donnée pour une utilisation efficace, précise et durable). Nos premiers pas vers une analytique maîtrisée et durable peuvent se faire avec des outils bureautiques classiques, mais nos progrès seront rapidement conditionnés par l’accès à des solutions dédiées d’analytique. Cette étape est souvent douloureuse car, paradoxalement, l’accès à des outils qui pourraient rendre le travail des métiers plus performant n’est pas toujours possible, et ce, pour différentes raisons :\n\n– le budget de licence est insuffisant et tout le monde ne peut pas être équipé. Aujourd’hui, bon nombre de solutions reviennent moins cher qu’un abonnement Netflix et le seuil de retour sur investissement est extrêmement bas. L’argument financier est moins évident qu’il y a une dizaine d’années, surtout ramené aux heures économisées par chacun chaque semaine. Néanmoins, les objectifs de développer une culture data chez les analystes ne sont pas toujours accompagnés de leur équipement ;\n\n– les services techniques choisissent un outil trop technique et peu attrayant pour les utilisateurs finaux. Dans un souci d’homogénéisation des solutions, de la formation et du support, les groupes établissent parfois des standards qui, involontairement, conduisent à ces situations (et à la précédente également). La logique de cette approche est parfaitement défendable, mais, dans notre monde digital émergent, de nouveaux éléments prô­nent des outils plus accessibles et pas forcément uniques pour des populations variées ;\n\n– la bataille de l’analytique se gagnant dans la question et la préparation des données, bien avant l’analyse elle-même, l’outil analytique n’est que la partie émergée de l’iceberg. Il ne compte pas beaucoup dans la pertinence et la qualité de l’analyse. Ce qui est clé, c’est la question métier et la data collectée pour y répondre : la manière de l’agréger, de l’analyser et de la visualiser est secondaire, tant qu’elle demeure efficace ;\n\n– les utilisateurs finaux, habitués à des interfaces propres, légères, rapides, vont moins adhérer à des environnements plus techniques. En fonction de leur sensibilité, ils vont d’ailleurs être attirés par des solutions analytiques différentes ;\n\n– alors que les équipes diverses montent en compétence, leurs préférences et leur besoin évoluent, et sont moins susceptibles de se conformer à un seul moule de logiciel analytique ;\n\n– l’autonomie des utilisateurs est souhaitée mais pas voulue. La perte de contrôle et de pouvoir (et de magie) liée à la démystification et l’adoption d’outils entraînent de nouveaux rapports de force ou de synergie auxquels toutes les équipes ne sont pas prêtes.\n\nÀ charge aux métiers de montrer leur appétit à apprendre, leur volonté de progresser, afin de ne pas prêter le flanc à la critique et de se voir refuser ces outils pour les raisons ci-dessus.",
          "Created At": "2025-09-06T13:22:34.067+02:00",
          "Updated At": "2025-09-06T13:22:34.067+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE054_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## Le frein organisationnel\n\nLa puissance des outils actuels et leur capacité à croiser et à visualiser rapidement de gros volumes de données modifient les rapports de force et conduisent à repenser les priorités et les objectifs de chacun. Les métiers, nouvellement équipés, se retrouvent pris en tenaille entre des équipes data qui « perdent » une partie de leur rôle d’appui tactique (qu’elles n’arrivaient pas à tenir de toute façon compte tenu souvent du nombre des demandes) et les équipes informatiques qui voient diminuer les activités de production d’analyse et de rapport (qu’elles n’arrivaient pas à tenir dans tous les cas aussi).\n\nLe statu quo fonctionnait car, jusqu’à présent, il y avait un vrai déficit de compétence data dans les métiers et ces équipes data et techniques ne faisaient que pallier ce manque. Aujourd’hui, les équipes opérationnelles, grâce aux data pionniers, peuvent prendre en charge les rôles d’analytique terrain pour laquelle elles sont les mieux placées.\n\nMoins qu’une perte de pouvoir pour les équipes data et technique, considérons cette tendance comme une réelle opportunité pour ces dernières de se concentrer sur les vrais défis de demain, tels que la cybersécurité, la mise en place d’infrastructures ultraperformantes, la résilience d’un réseau de plus en plus partagé entre le bureau et le reste du monde (domicile, restaurant, transports), la gouvernance pour bâtir une donnée homogène, connectable et qui respecte l’éthique. Les équipes métier vont passer par une courbe de changement forte pour monter en compétence data : le même effort devra être entrepris par les groupes support si l’on veut casser le statu quo.",
          "Created At": "2025-09-06T13:22:34.068+02:00",
          "Updated At": "2025-09-06T13:22:34.068+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE055_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## Le frein humain\n\nNous allons toutefois retrouver des obstacles à cette migration technologique dans la hiérarchie, dans nos équipes et en nous.\n\nL’attitude du management est souvent directement liée à la faible adoption de nouvelles solutions et de processus appuyés sur les technologies décisionnelles. Plusieurs facteurs sont en cause :\n\n– la méconnaissance, et de fait une certaine méfiance, du domaine. Trop souvent « brûlé » par des projets data ratés ou des investissements sans débouchés, le management ne voit pas toujours d’un bon œil l’adoption de ces approches analytiques qu’il maîtrise mal ;\n\n– la crainte de perte d’autorité. Le paradigme du manager « sachant » est encore bien présent : « Je suis le dirigeant, donc je sais. » Donner plus de pouvoir d’analyse à une équipe change cet équilibre (voir supra, Le reporting) ;\n\n– la persistance des demandes de rapports manuels, ultracosmétisés et souvent sur Excel ;\n\nOui mais dans l’avion…\n\nUn responsable préférerait un rapport tableur prenant une heure de préparation à un document automatisé en ligne en donnant l’argument suivant : « Oui, mais si je dois prendre l’avion et que je n’ai pas de connexion, je fais comment ? » Pendant des années, cette posture a bloqué toute possibilité d’évolution du processus et même de réflexion sur les options possibles pour son optimisation.\n\n– la reconnaissance et la récompense de pratiques obsolètes n’incitent pas au risque d’essayer de nouveaux outils en analytique. Qui récolte les lauriers et les accolades le plus facilement ? L’au­teur d’un rapport magnifique, raffiné, obtenu de haute lutte par la maîtrise des dernières suites bureautiques, ou l’humble analyste qui se concentre sur l’impact de ces analyses dans des processus itératifs (test, échec, apprentissage, progrès) ? L’équipe qui ostensiblement travaille nuit et jour pour finir ses rapports, parce qu’entravée dans des processus manuels et fragiles, ou bien les groupes se reposant sur des processus lean qui peuvent se permettre de prendre du temps pour réfléchir et pour progresser ?\n\nNous devrions être rémunérés en partie sur la durabilité de nos processus analytiques. Bâtir des usines à gaz qui ne vivent que le temps de notre poste n’est pas le meilleur investissement de notre salaire pour une organisation, si tout s’évanouit à notre départ.\n\nLe bonus durable\n\nN’oublions jamais de regarder notre courrier, peut-être que, des années après, un employeur nous remerciera avec un chèque envoyé par la poste pour tous les processus durables que nous aurons mis en place. À l’heure où la transformation est le maître mot, l’évolution des méthodes de management autour de l’analytique et du reporting ne devrait pas être oubliée.\n\nNos coéquipiers ne seront pas toujours sources de support pour plusieurs raisons :\n\n– nous bouleversons une situation qui, sans qu’elle soit satisfaisante, avait au moins le mérite d’être enracinée dans une routine ;\n\n– nous risquons d’exposer au grand jour des pratiques particulièrement inefficaces qui jusqu’à présent étaient considérées comme l’état de l’art, faute de point de référence ;\n\n– nous rasons certains bidouillages analytiques qui avaient fait la réputation de leur concepteur et nous proposons à leur place des solutions simples, transparentes, accessibles et partagées.\n\nEnfin nous sommes parfois notre propre ennemi. Les raisons de ne pas faire pourraient être listées dans un ouvrage à part entière. Voici quelques réflexes ou habitudes que nous allons devoir traiter pour progresser :\n\n– la paresse ou le manque de courage : nous sommes tous un peu paresseux. Se rajouter du travail et une courbe de d’apprentissage dans des journées déjà bien remplies n’est pas un engagement anodin ;\n\n– le choix de carrière, qui va avoir un impact à deux niveaux : choisir de faire comme tout le monde et se conforter finalement dans une analytique manuelle, ennuyeuse, mais reconnue ; parfois choisir des solutions sur la base de la renommée de leur éditeur est l’assurance de se voir reconnaître une compétence dans les positions futures.\n\nSe former, se rapprocher de collègues embarqués dans la même aventure, se faire appuyer par un mentor et identifier un manager qui apportera le soutien nécessaire pendant les phases de tests et d’échecs, tout cela va permettre de passer cette phase.\n\nYes we can !\n\nJ’enseigne la data depuis 2008 sur des programmes de formation continue comme ceux de Stanford, Berkeley, sur des programmes exécutifs, sur des bachelors et masters, ainsi que dans des programmes d’inclusion et de retour à l’emploi par l’apprentissage de la data et de l’analytique. J’ai formé des milliers d’élèves de tous âges, expériences et horizons socio-économiques : s’ils l’ont fait, vous le pouvez !",
          "Created At": "2025-09-06T13:22:34.068+02:00",
          "Updated At": "2025-09-06T13:22:34.069+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE056_1757157753082",
          "Status": "Ready For Refine",
          "Text": "# Pour conclure\n\nLe schéma de pensée décrit dans ce chapitre va nous aider à mieux comprendre, reconnaître et ensuite positionner mentalement les solutions autour de nous dans leur contexte. Cette approche, simple mais non simpliste, permet de décoder les nouvelles solutions et les gadgets qui font constamment irruption dans notre monde. En utilisant des termes plus précis, sans tomber dans le jargon, nous allons pouvoir mieux décrire nos besoins et savoir où et comment bénéficier de la technologie.\n\nHumainement, nous serons plus respectés dans nos échanges avec l’informatique en n’étant plus des innocents aux mains pleines, mais des partenaires de réflexion et de coconstruction.\n\n------------------------------------------------------------------------\n\n5. Charpin D., Hammu-Rabi de Babylone, PUF, 2003.\n\n6. Voir McCue I., Qu’est-ce qu’un ERP (Enterprise Resource Planing), [www.netsuite.com](http://www.netsuite.com), 2016.\n\n7. BM Journal of Research and Development, II (4), octobre 1958.\n\n8. Clark J., Google retourne vers le futur avec la base de données SQL F1, The register, 30 août 2013.\n\n9. Source des données : [https://open-numbers.github.io/datasets.html](https://open-numbers.github.io/datasets.html)\n\n10. Source des données : kaggle.com\n\n11. [https://d3js.org/](https://d3js.org/)\n\n12. Hans Rosling est un statisticien suédois, décédé en 2017. Son livre Factfulness, publié à titre posthume et coécrit avec Anna Rosling Rönnlund et Ola Rosling, est devenu un best-seller international.\n\n13. Umoh R., Pourquoi Jeff Bezos oblige les dirigeants d’Amazon à lire des mémos de 6 pages au début de chaque réunion, cnbc.com, 23 avril 2018.\n\nCHAPITRE 5  \nMaximiser le potentiel de la donnée\n\nLes 152 000 lignes ne tiennent pas dans le tableur !\n\nMaintenant que nous avons acquis les principes de base de la donnée et que nous avons pris connaissance des boîtes à outils mises à notre disposition, nous sommes parés pour commencer à travailler avec cette matière première qu’est la data.\n\nNous procéderons en plusieurs étapes :\n\n– démystifier les différents types de base de données. Ce sont les containers de nos données, nous devons connaître les options à notre disposition ;\n\n– structurer et organiser sa data. 80 % du travail de l’analyste est la préparation de la donnée ;\n\n– considérer les approches plus avancées pour tirer le maximum de nos données le plus efficacement possible ;\n\n– appréhender le Big Data avec confiance.",
          "Created At": "2025-09-06T13:22:34.069+02:00",
          "Updated At": "2025-09-06T13:22:34.069+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE057_1757157753082",
          "Status": "Ready For Refine",
          "Text": "# Les types de bases de données\n\nLa confusion ambiante, savamment entretenue par certains éditeurs ou informaticiens, ne nous aide pas à cerner ces technologies pourtant centrales. Une fois leur rôle fondamental et trivial de  \nstockage de données compris, nous pouvons nous concentrer sur les raisons pour lesquelles il existe différents choix et options technologiques.\n\nLes différentes bases de données ont toute leur raison d’être, bien qu’aucune ne soit la panacée pour toutes les applications du décisionnel. À l’image d’une voiture, si une petite hybride, une grosse berline, un SUV, une Formule 1 ou un van peuvent tous transporter passager(s) et bagage(s), leur format n’est pas adapté à tous les contextes, toutes les routes, tous les garages et tous les conducteurs. Pour mieux comprendre ces technologies clés pour nos analyses, nous pouvons considérer les bases de données en six groupes, de la plus simple à la plus complexe.",
          "Created At": "2025-09-06T13:22:34.070+02:00",
          "Updated At": "2025-09-06T13:22:34.070+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE058_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## Les fichiers plats, texte ou CSV\n\nÉvoqués dans le chapitre précédent, ils représentent la forme la plus simple de stockage d’information. Ils sont le plus souvent sous forme de lignes et de colonnes délimitées par des éléments de texte tels que la virgule, le point-virgule, la tabulation, etc. Il existe d’autres formats, tels que le JSON (JavaScript Object Notation), qui permettent de stocker les données de manière légèrement plus lisible pour l’humain, mais qui ne sont pas toujours lus avec facilité par les outils du marché.\n\nIllustration 18. Extrait d’un fichier JSON\n\nNous avons vu que les fichiers texte sont des médias de choix en raison de leur simplicité et de leur universalité. Néanmoins, ils présentent rapidement des limitations quand il s’agit de :\n\n– définir des relations entre différents fichiers. Ces relations, appelées jointures (voir infra, Connecter la data pour une analytique plus riche et pertinente) sont toujours possibles, mais elles ne peu­vent pas être vraiment gouvernées et encadrées par des règles communes à tous les analystes. Chaque utilisateur doit bâtir ses jointures à chaque fois ;\n\n– contrôler les formats des colonnes. La qualité de la donnée et l’homogénéité des formats au sein d’une même colonne ne peuvent pas être garanties à l’avance ni forcées. Le fichier texte stockera ce qu’on lui injecte sans aucune forme de contrôle. On pourra ainsi voir des dates, des nombres ou des textes cohabiter dans une colonne, rendant délicate voire impossible l’analyse ;\n\n– gouverner des fichiers plats localisés sur de nombreuses machines, dans des dossiers différents, n’est pas simple. Il revient à chaque analyste de gérer ses fichiers sans vraiment de coordination globale possible, en évitant les effacements par erreur.\n\nLes bases de données relationnelles vont pallier ces manques. Avec l’encadrement plus strict des règles et des normes de stockage, elles vont perdre un peu de l’agilité des fichiers plats. Elles ont aussi un coût, si ce n’est en licence, au moins en temps et en effort de maintenance.\n\nFigure 8. Représentation des bases de données texte dans les diagrammes de flux des données",
          "Created At": "2025-09-06T13:22:34.070+02:00",
          "Updated At": "2025-09-06T13:22:34.071+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE059_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## Les bases de données relationnelles\n\nLes bases de données relationnelles peuvent être imaginées comme un ensemble de tables, similaires à des fichiers plats mis dans un environnement dédié. Cet environnement apporte de nombreux avantages tels que :\n\n– une formalisation des formats requis. Nous allons pouvoir définir et forcer des formats spécifiques comme des dates, des types de nombre ou des longueurs de texte ;\n\n– une formalisation des liens entre les tables et des modèles qu’elles forment. Cela nous permettra de ne pas avoir à reconstruire des liens entre les tables à chaque nouvelle analyse (voir infra, Connecter la data pour une analytique plus riche et pertinente) ;\n\n– des procédures de contrôle d’accès et de sauvegarde ;\n\n– les contraintes ACID qui permettent d’assurer l’intégrité de toutes les transactions saisies, modifiées ou effacées dans le système :\n\n• l’atomicité : dans une transaction impliquant deux ou plusieurs éléments d’information distincts, soit tous les éléments sont engagés, soit aucun ne l’est ;\n\n• la cohérence : une transaction crée un nouvel état de données valide ou, en cas d’échec, ramène toutes les données à l’état où elles se trouvaient avant le début de la transaction ;\n\n• l’isolement : une transaction en cours et non encore terminée doit rester isolée de toute autre transaction ;\n\n• la durabilité : les données engagées sont sauvegardées par le système de telle sorte que, même en cas de panne et de redémarrage du système, les données sont disponibles dans leur état correct.\n\n– un langage de requête et de calcul dédié, pour effectuer des traitements sur un serveur puissant et non plus seulement sur notre machine.\n\nCes bases relationnelles sont aussi parfois appelées base SQL14 (Structured Query Language), parce que leur langage de gestion est dans la grande majorité des cas le SQL. Leur intérêt en décisionnel est que leur structure en tables est naturellement compréhensible. La logique de connecter des tables entre elles est facilement visualisable. Le langage SQL est aussi accessible car lisible simplement en anglais.\n\nCes bases de données ont toutefois quelques limites :\n\n– leur environnement très structuré peut ralentir les développements itératifs ;\n\n– leur structure tabulaire n’est pas efficace pour travailler sur des calculs en ligne. Autant, en colonne, il est possible d’effectuer des traitements et de stocker les résultats dans une nouvelle colonne, autant réaliser des opérations sur des lignes est délicat : où stocke-t-on les résultats ? Une nouvelle ligne serait considérée comme un nouvel enregistrement et non pas comme une donnée spécifique calculée, et la table perdrait sa consistance ;\n\n– leur moteur de calcul n’est pas toujours approprié pour appliquer des formules au travers de tout un modèle ;\n\n– elles deviennent moins performantes et compliquées à gérer lorsque le nombre de colonnes augmente ;\n\n– enfin, elles ne sont pas adaptées au stockage de données non structurées.\n\nC’est pour cela que d’autres formats de base de données ont été développés.\n\nFigure 9. Représentation des bases de données relationnelles dans les diagrammes de processus analytiques\n\nLe siège d’Oracle à Redwood Shores en Silicon Valley\n\nUne claire référence à la base de données relationnelle qui a fait la fortune de la société.",
          "Created At": "2025-09-06T13:22:34.071+02:00",
          "Updated At": "2025-09-06T13:22:34.071+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE060_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## Les cubes de données\n\nAussi appelées bases OLAP (On Line Analytical Processing), hypercubes ou bases « multidimensionnelles », ces technologies permettent de résoudre au moins deux limitations des bases de données relationnelles :\n\n– la possibilité de faire pivoter les lignes en colonnes et vice versa, de manière à pouvoir appliquer des calculs sur toutes les dimensions ;\n\n– l’application de calculs à tous les niveaux d’une hiérarchie ou à certains membres sélectionnés.\n\nCes options en font des outils remarquablement adaptés à l’analytique financière ou au contrôle de gestion. Néanmoins, elles ne sont pas l’arme absolue. Elles trouvent leur limite en capacité de stockage comparée aux bases relationnelles et en termes de nombre d’attributs. Plus on augmente leur nombre, plus le nombre de cellules à créer pour chaque autre dimension du cube grandit exponentiellement. Elles sont aussi plus coûteuses à l’achat et à la maintenance. Elles requièrent des qualifications techniques plus rares et donc plus onéreuses que celles requises pour les bases de données relationnelles.\n\n→ Tu prendras du OLAP, mon fils\n\nJ’ai été confronté à une demande, ou plutôt à une injonction de passer notre analytique sur une base OLAP il y a quelques années. Je connaissais très bien la technologie demandée et j’avais vu avec quelle habileté l’équipe commerciale du fournisseur avait convaincu l’équipe dirigeante de la pertinence de ce choix (les repas, le golf et les conférences faisaient partie des arguments !). La solution était bonne, ultraperformante et semblait être une évidence, même si nos besoins analytiques se satisfaisaient très bien des solutions en place qui étaient loin d’être à leur limite.\n\nLe point vendeur de mon patron était que la solution n’était pas si chère. Effectivement, le prix du logiciel était raisonnable et nous pouvions imaginer un retour sur investissement rapide. Néanmoins, en ajoutant le coût des serveurs et de l’implémentation, le budget faisait plus que doubler. En additionnant les frais de formation de l’équipe, la nécessité de recruter un responsable pour le contrôle de la solution et également les charges du consultant côté informatique pour une maintenance à mi-temps, nous avions triplé la dépense et créé une dépense récurrence conséquente.\n\nLes bases OLAP sont par conséquent un excellent complément des bases relationnelles. Ces dernières stockent massivement les informations avec de grands niveaux de granularité et de détail, et peuvent exporter une sélection de données pertinentes sous forme de cubes des sélections de données pour des analyses spécifiques.\n\nComment choisir entre base de données relationnelle et base de données multidimensionnelle ?\n\nOLAP et « multidimensionnel » sont devenus des termes standards pour qualifier ces technologies en cube. Ce ne serait pas un problème si ces mots ne biaisaient pas les choix de base de données en amenant à ne penser qu’à ces options lorsque nous voulons faire des analyses suivant différents angles ou agrégats. Les bases relationnelles peuvent faire des analyses multidimensionnelles par produits, clients, pays etc. Elles peuvent gérer des hiérarchies. Et avec les outils d’analyse d’aujourd’hui, elles peuvent être représentées par des tableaux croisés dynamiques et approcher l’agilité de cubes. Par ailleurs, leur interrogation peut être faite… en ligne, online. Une base de données relationnelle peut donc être aussi OLAP et multidimensionnelle. Attention à choisir les cubes de données pour les bonnes raisons.",
          "Created At": "2025-09-06T13:22:34.072+02:00",
          "Updated At": "2025-09-06T13:22:34.072+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE061_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## Les bases de données en colonnes\n\nAvec l’émergence des réseaux sociaux, la création massive d’informations individuelles lors d’une navigation internet ou d’expérience en ligne, les volumes de données ont non seulement explosé, mais leur nombre d’attributs (de colonnes) aussi. De plus, ces attributs ont commencé à devenir de plus en plus variés pour capter les facettes des individus ou de leur comportement, sous la pression des analystes toujours plus gourmands en données pour connaître leurs clients ou leur marché. Les bases de données relationnelles (tout comme les cubes) n’offrent pas cette flexibilité d’ajout massif de colonnes et perdent rapidement en performance lors des phases d’analyse sur des tables larges.\n\nLes bases de données en colonnes (columnar database) ont permis de répondre à ce défi en stockant chaque colonne indépendamment des autres. Chaque valeur de colonne est bien sûr attachée à une clé qui permet de les relier à leur membre. L’intérêt de cette approche est que, pour l’analyse d’une caractéristique parmi des centaines, il suffit de prendre la colonne qui la contient et, indépendamment des autres, d’effectuer les opérations voulues. La machine n’a pas à lire toute une ligne d’informations non pertinentes pour récupérer cette donnée et, de ce fait, le temps d’accès est largement réduit.\n\nLà encore, ces technologies ne sont pas le Graal de l’analytique : elles sont plus des techniques à gérer, elles n’offrent pas la performance des bases de données relationnelles en écriture/effacement de nouveaux enregistrements (car il faut écrire dans autant de fichiers qu’il y a de colonnes). Elles sont également moins facilement prises en main pour des utilisateurs métier et généralement plus onéreuses en licence et en maintenance.",
          "Created At": "2025-09-06T13:22:34.072+02:00",
          "Updated At": "2025-09-06T13:22:34.073+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE062_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## Les bases de données NoSQL (Not Only SQL)\n\nNous entrons maintenant dans le Far West des bases de données. Ces bases de données NoSQL ont été créées pour plusieurs raisons à l’origine :\n\n– stocker toutes les nouvelles données que le monde (organisations, humains et machines) a commencé à générer grâce à la révolution numérique (texte, images, vidéo, voix, etc.) ;\n\n– stocker sans limite grâce à leur capacité de distribuer leur stockage sur plusieurs machines et disques ;\n\n– calculer de manière massive en distribuant la charge sur plusieurs serveurs et processeurs ;\n\n– s’affranchir des grands éditeurs avec des technologies Open Source15 ;\n\n– se libérer de contraintes de gestion pour aller au plus vite dans la capture de nouvelles informations.\n\nLeur avantage est certain dans un contexte où la boulimie des entreprises pour la donnée explose. Ces technologies ne sont néanmoins pas l’arme absolue. Leur technicité et leur complexité sont un frein pour beaucoup d’équipes, surtout dans les métiers. L’accès direct par des analystes n’est pas réaliste. Ce sont potentiellement d’excellents réservoirs de données brutes, mais leur aptitude à livrer une donnée préparée et adaptée à l’analyse est limitée.",
          "Created At": "2025-09-06T13:22:34.073+02:00",
          "Updated At": "2025-09-06T13:22:34.073+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE063_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## Les bases de données décisionnelles virtualisées\n\nCe mode de présentation de données pour l’analytique s’est progressivement démocratisé au début des années 2000.\n\nCette approche permet de ne pas forcément stocker toute la donnée et de ne traiter que la donnée qui est demandée pour l’analyse. Elle crée une vue utilisateur similaire à un modèle de données d’analyse (voir infra, Les options pour travailler la donnée : à la volée ou stockée) et la solution se charge de gérer les liens, les requêtes et les traitements de données en tâche de fond.\n\nL’avantage de ces solutions est qu’avec la virtualisation de la base les coûts de stockage diminuent. La modification d’une logique de présentation ou de préparation de données est également plus agile, car il suffit de changer le code sous-jacent et non pas la structure entière d’un modèle.\n\nL’agilité peut avoir une conséquence sur la performance des analyses, car chaque requête d’un analyste nécessite de traiter toute la chaîne virtuelle. On pourra donc arbitrer les éléments à virtualiser ou à implémenter physiquement pour choisir où l’agilité ou la performance sont le plus critiques. Même si ces choix sont techniques, le data pionnier ne devrait pas hésiter à faire part de ses contraintes opérationnelles pour optimiser au mieux la chaîne analytique.",
          "Created At": "2025-09-06T13:22:34.074+02:00",
          "Updated At": "2025-09-06T13:22:34.074+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE064_1757157753082",
          "Status": "Ready For Refine",
          "Text": "## Pour conclure\n\nNous voyons que, même dans un domaine relativement technique, le pragmatisme métier reste fondamental. Beaucoup de choix s’offrent à nous et souvent nous sont imposés. Comprendre les technologies qui sous-tendent nos sources de données est important puisqu’elles conditionnent l’agilité de nos analyses. Sans entrer dans les détails, cette hiérarchisation des fonctionnalités va permettre de mieux utiliser le potentiel de nos bases de données, voire d’en influencer le choix.\n\n→ Tu prendras Hadoop, mon fils\n\nTout au long de ma carrière, mes équipes data se sont souvent vu imposer des choix de bases de données. Était-ce notre rôle de gérer la sélection et de faire un choix final ? Probablement pas. Néanmoins, ces choix ont considérablement impacté notre performance et notre agilité d’analyse. Rétrospectivement, je pense que ce sont les besoins métier qui devraient primer. Imposer des choix techniquement bons mais sous-optimaux pour les analystes, ne serait-ce pas voir le problème à l’envers ?\n\nLors de mon passage dans un grand groupe de l’Internet, en tant que responsable de la donnée financière, mon équipe d’analystes s’était vu imposer une base de données Open Source NoSQL : Hadoop. J’étais moi-même assez intrigué et enthousiaste à l’idée de me former à ces nouvelles solutions. J’ai malheureusement rapidement déchanté et je suis allé de déconvenue en déconvenue. Non pas que la technologie ne fût pas bonne ou que les équipes techniques ne la maîtrisent pas, mais parce qu’une série d’inquiétudes et de déceptions se sont imposées à mes équipes :\n\n– nous n’avions aucune maîtrise des flux entrants de données. Nous ne pouvions pas ajuster nous-mêmes la sélection des informations et encore moins leur préparation. Nous n’avions pas de visibilité sur la bonne fin des processus de chargement des données. Cette absence de transparence est rapidement devenue pré­judiciable dans le cadre du support des processus d’analyse financière récurrent : quand le chronomètre tourne et que, pour chaque besoin de mise à jour de données, vous devez vous référer à un ingénieur pour déclencher la remontée des données et leur contrôle, vous augmentez votre vulnérabilité ;\n\n– nous étions également fortement dépendants des équipes techniques pour les extractions d’informations. Les outils attachés à la base de données n’avaient rien de simple pour des utilisateurs métier. Nous étions à la merci de la disponibilité d’un technicien pour chaque nouvelle extraction de données ;\n\n– nous verrons plus bas que profilage, contrôle qualité et préparation de données sont le pilier de l’analytique. Or, ces étapes nous étaient désormais inaccessibles au niveau de la base de données et nous étions livrés au mieux à des outils de Business Analytics et au pire à des tableurs, au prix d’une grande perte de temps et potentiellement d’intégrité de données ;\n\n– notre dépendance à l’informatique s’exprima de manière encore plus nette en période de vacances, de longs week-ends ou de vendredi off. Les clôtures financières ne suivent pas les rythmes des loisirs de chacun et nous nous sommes retrouvés sur des processus urgents avec personne vers qui nous tourner pour des mises à jour de données. Notre urgence n’était pas de facto une priorité pour des équipes qui avaient d’autres engagements ;\n\n– pour couronner le tout, les temps de traitement étaient inférieurs à nos systèmes existants. Il s’est avéré que les temps de distribution des processus sur plusieurs machines puis leur récupération prenaient plus de temps que le traitement lui-même, et que le codage des ingénieurs était loin d’être aussi performant que celui de nos solutions, développées par des éditeurs dont c’était le métier depuis des décennies.\n\nNous étions littéralement otages d’une solution que nous ne pouvions pas maîtriser. Nous avions la responsabilité des délais et de la qualité de nos données, sous contrôle d’équipes dont nous n’étions pas la priorité.\n\nEn épilogue : j’avais pris soin de conserver tous les processus intacts et de ne rien basculer avant les tests complets, malgré la pression d’un groupe technique qui ne rêvait que de sortir un éditeur du parc informatique. Mais le non invented here (non inventé ici), qui fait référence au dénigrement de technologie ou de pratiques extérieures, n’a pas tenu. Nous avons conservé nos outils, non sans une certaine amertume : j’espérais vraiment pouvoir trouver de nouvelles opportunités avec ce projet. Mais la performance opérationnelle devait primer (voir note 8).",
          "Created At": "2025-09-06T13:22:34.075+02:00",
          "Updated At": "2025-09-06T13:22:34.075+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE065_1757157753082",
          "Status": "Ready For Refine",
          "Text": "# Organiser la donnée pour une meilleure analytique\n\nUne fois la donnée extraite des sources et stockée dans la base de données décisionnelle, une deuxième phase commence par trois grandes étapes :\n\n– la prise de connaissance du jeu de données récupéré et de sa structure ;\n\n– l’audit de sa qualité et de sa pertinence ;\n\n– l’organisation des données et leur mise en relation avec les données existantes en vue des analyses ultérieures.\n\nEnsuite, nous nous tournerons sur les techniques de préparation et d’enrichissement pour faciliter nos analyses.",
          "Created At": "2025-09-06T13:22:34.075+02:00",
          "Updated At": "2025-09-06T13:22:34.077+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE066_1757157753084",
          "Status": "Ready For Refine",
          "Text": "## Les fondamentaux pour comprendre la donnée\n\n\\> Identifier les formats de la data\n\nLa data qui nous entoure peut revêtir des formats aussi variés que des sons, des images, des textes, des odeurs, des sentiments. Quand il s’agit de faire des analyses fluides et de conserver une donnée FAIR (voir chapitre 3, Collecter une donnée pour une utili­sation efficace, précise et durable), toutes ces données devront être numérisées. Elles seront alors codifiées dans quatre grandes familles :\n\n– les chaînes de caractères alphanumériques (String) : ces données peuvent contenir tout type de caractère et des textes entiers, tels que des revues clients ;\n\n– les nombres : ces champs vont pouvoir varier en fonction du type de nombre et pourront distinguer entre autres :\n\n• les entiers (integer), qui pourront être longs ou courts (int16, 32 ou 64) selon la taille des nombres que nous devons stocker ;\n\n• les décimaux (decimal) qui stockent jusqu’à 28 décimales après la virgule et les décimaux doubles qui vont stocker de plus grandes valeurs, mais avec 16 décimales. Les premiers seront à employer pour des calculs plus précis ;\n\n– les Booléens (boolean) vont, eux, indiquer « juste » ou « faux », « oui » ou « non », 1 ou 0 de manière binaire ;\n\n– les dates/heures, qui pourront prendre différents formats selon :\n\n• la région (le mois figure avant les jours dans une data amé­ricaine : le 12 avril 2020 s’exprimera 04/12/2020 au lieu de 12/04/2020 en Europe) ;\n\n• la convention retenue dans la base de données : 12042020, 12/04/2020, 2020-04-12. Malgré leurs expressions différentes, ces valeurs seront considérées comme une même date si leur logique est reconnue par le système qui les gère ;\n\n• la précision souhaitée : l’heure pourra être attachée à la date avec différents niveaux de précision. On ne pourra d’ailleurs retenir que la partie heure d’une date.\n\nConnaître ces formats va s’avérer important pour le data pionnier. En effet, il arrive parfois que la donnée ne soit pas reconnue comme elle devrait l’être ou comme nous le voulons dans nos solutions d’analyse. Nous aurons la possibilité de demander un formatage précis au niveau de la base de données, ou bien, si nous travaillons à partir de fichiers texte, soit de forcer le format dans notre logiciel, soit de modifier la donnée à la source pour qu’elle devienne compatible avec le format souhaité.\n\nVoici quelques exemples classiques que nous rencontrerons souvent, surtout dans le cas de travaux sur des fichiers plats qui n’ont pas été normalisés en étant intégrés dans une base de données :\n\n– le nombre qui est reconnu comme une chaîne de caractères :\n\n• il y a probablement des champs avec des valeurs nulles sous forme d’espace dans la colonne ;\n\n• le séparateur de décimale est une virgule là où le logiciel attend un point ou vice versa (par exemple 183,33) ;\n\n• le séparateur de milliers est un espace, ou une virgule là où on n’en attend pas, ou vice versa (par exemple 1 182 329 ou 976,678,768.67) ;\n\n– la date qui n’est pas reconnue comme une date ;\n\n• le format international avec la position des mois avant les jours, ou vice versa, n’est pas reconnu par la solution d’analyse ;\n\n• les séparateurs jours-mois-année, qui peuvent être des tirets, des points, des barres de fraction, des espaces, ou simplement ne pas exister, sont mal interprétés ;\n\n• l’existence de l’heure à la suite de la date empêche la reconnaissance en tant que date ;\n\n• le nombre de positions pour coder les différents éléments de la date peut être différent de ce qu’attend le logiciel. Par exem­ple, le codage d’un jour ou d’un mois peut être sur un ou deux chiffres, ou bien forcé sur deux chiffres. Le codage d’une année peut être sur deux chiffres ou quatre chiffres. Par exemple, les chaînes suivantes peuvent exprimer les mêmes dates, mais leur format peut conduire à une interprétation comme une chaîne de caractères : 12/04/2021, 04-12/2021, 20210412, 12 Avril 2021, 12/04/21 12h31m34.23s ;\n\n– la chaîne de caractères qui est reconnue comme un nombre :\n\n• souvent des codes d’identification, des clés (voir infra, Jauger la qualité de données initiale : profiler la donnée) peuvent être numériques et vont donc se retrouver à être traités comme des nombres avec l’ajout de décimales ou l’agrégation en somme, ce qui n’a aucun sens et aucune utilité pour les analyses ;\n\n• des dates sans délimiteur telles que 20200412 ou des noms faits de chiffres peuvent également se retrouver qualifiés de nombres et perdre tout leur sens ;\n\n• la valeur booléenne peut être une chaîne (Oui/Non), un nombre (1/0) ou prendre une forme binaire spécifique (Da/Niet, Ok/Not OK, Approuvé/Rejeté, etc.) et elle peut ne pas être interprétée comme telle.\n\nPlusieurs options vont nous permettre de résoudre ce problème :\n\n– forcer les formats dans la base de données ou dans le logiciel d’analyse si les contenus de colonne le permettent ;\n\n– insérer un caractère alphanumérique à la saisie ou dans les traitements pour éviter la qualification de nombre.\n\nNous retrouvons dans l’illustration 19 les options de (re)classement de type de données de Pyramid Analytics, Tableau et PowerBI. Avec des variations sémantiques ou de langue, nous retrouvons sans difficulté les différents types.\n\nIllustration 19. Exemples d’options de choix de format de donnée dans un logiciel de Business Intelligence\n\nLe casse-tête des dates\n\nLes formats de dates sont un véritable problème à gérer lors de l’acquisition de données brutes. Lorsque la donnée est extraite d’une base de données, la normalisation des formats et le classement en type de données date évitent cet écueil, mais bien souvent, lors d’analyses exploratoires, nous n’avons pas le luxe d’avoir des sources de données propres et bien gouvernées.  \nIl va par conséquent falloir procéder à des nettoyages des chaînes que nous voulons interpréter en dates pour que toutes les données aient le même format interprétable. Les techniques classiques sont entre autres :  \n– permuter les mois/jours pour les dates américaines/reste du monde ;  \n– forcer ou non les formats à deux chiffres pour les jours et les mois, et à quatre chiffres pour les années ;  \n– supprimer ou normaliser la partie « heure » du champ « date » ;  \n– remplacer les délimiteurs qui ne sont pas dans les normes.  \nSi cela nous paraît fastidieux, c’est normal. C’est toujours un point de contention dans la préparation de données. Il vaut mieux engager des actions de gouvernance et de sensibilisation proactive de nos fournisseurs de données pour éviter d’avoir à dépenser notre énergie dans ce travail de nettoyage.\n\n\\> Identifier les rôles de la data\n\nSe retrouver face à de nouvelles données est souvent déroutant au départ. Face à une table, à des lignes et à des colonnes, nous ne savons pas où regarder ou par où commencer.\n\nReconnaître les rôles de la data va nous guider pour mieux la comprendre et la jauger.\n\nIl existe de nombreuses variations dans les noms utilisés pour définir ces rôles. Entre les différents jargons et le choix internes d’entreprise, ce qui est important est de reconnaître leurs finalités et les méthodes de contrôle et d’utilisation pour en faire le meilleur usage.\n\nUne table de données est la plupart du temps constituée de quatre grands types de colonnes :\n\n– les colonnes de métriques ou mesures ;\n\n– les colonnes d’attributs ou dimensions ;\n\n– les colonnes de clés, identifiants ou index ;\n\n– les données calculées ajoutées à la donnée d’origine.\n\n» Prenez un de vos fichiers tableurs ou un extrait de données favorites et essayez d’identifier ces différents éléments (voir illustration 20). Avec l’habitude, cela deviendra un réflexe.\n\nIllustration 20. Exemple d’un fichier client d’une société de télécommunication\n\nLes métriques ou mesures\n\nIls sont l’élément quantitatif ou qualitatif au cœur de la question métier. Nous pouvons les additionner, en faire la moyenne, rechercher les minima/maxima. Nous pouvons également compter les occurrences ou compter les éléments distincts de données qualitatives pour les transformer en métriques. Ce sont tout type de nombre : entier, décimal, etc. Leur qualité sera dépendante de leur saisie ou de leur capture et il sera difficile de savoir si une métrique est bonne ou mauvaise intrinsèquement. Un nombre est nombre : nous saurons s’il est bon grâce au profilage et à l’analyse (voir infra, Jauger la qualité de données initiale : profiler la donnée).\n\nLes attributs\n\nIls apportent des informations sur les mesures. Parce que les attributs ajoutent des moyens de regrouper, de trier et d’organiser les données, nous les appelons parfois des dimensions. Même s’il ne s’agit généralement pas de mesures, nous pouvons également les compter ou dénombrer les éléments distincts. Ils peuvent être dans n’importe quel format : numérique, alphanumérique, date, booléen, etc. Leur qualité, à savoir cohérence et homogénéité, sera dépendante de leur format. Ces données appartiennent à une catégorie spéciale : les données maîtres, les Master Data (voir infra, Gérer les données maîtres : le Master Data Management (MDM)).\n\nLes clés\n\nElles permettent de localiser ou d’identifier un enregistrement spécifique. Elles peuvent être utilisées pour connecter des ensembles de données et apporter plus de données à l’analyse en faisant correspondre des lignes entre elles quand elles ont la même clé. Elles peuvent être dans n’importe quel format : numérique, alphanumérique, date. Habituellement, le format le plus court et constant est préférable pour limiter le risque d’erreur. Des clés peuvent aussi être constituées en combinant des colonnes : nous pourrons les obtenir grâce à un champ calculé (voir infra, Créer des clés pertinentes pour de nouvelles jointures). Leur qualité dépendra de la constance de leur format et parfois de leur unicité.\n\nLes données calculées\n\nCe sont de nouvelles données générées à partir de nos données d’origine. Elles peuvent être obtenues par des formules mathématiques, des transformations de texte ou toute autre action que nous souhaitons appliquer à notre donnée initialement capturée. Elles peuvent être exprimées dans n’importe quel format, numérique, alphanumérique, date, booléen, et peuvent ensuite avoir le rôle que nous souhaitons. Leur qualité est dépendante des mesures, des formules, ou des ordres de priorité dans leur exécution.\n\n\\> Connaître ou définir les règles d’agrégation de la data\n\nNous avons vu dans le chapitre précédent que certains rôles de data permettaient certains calculs mais pas d’autres. Revenons en détail sur un dernier point clé : comment nos données vont s’agréger lorsque nous allons les synthétiser ?\n\nTableau 5. Types d’agrégation pertinents pour le jeu de données à partir d’un extrait de la table de l’illustration 20\n\nIllustration 21. Exemples d’options d’agrégation dans un logiciel de Business Intelligence\n\n\\> Pour conclure\n\nTableau 6. Récapitulatif des rôles que peuvent avoir les données\n\nÀ la lumière ces éléments, la table en figure 2 du chapitre 2 va prendre des couleurs. Ces lignes et ces colonnes vont enfin apparaître comme une vraie matière première à affiner et à analyser avec toute une liste d’options de format et d’agrégation que nous devrons valider ou changer.\n\nFigure 10. Organisation d’une table de données\n\nNotre œil va devoir s’habituer à identifier au premier regard ces éléments qui vont tous jouer leur rôle dans les étapes d’analyse suivantes.\n\nL’œil (du tigre) de l’analyste",
          "Created At": "2025-09-06T13:22:34.077+02:00",
          "Updated At": "2025-09-06T13:22:34.078+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE067_1757157753084",
          "Status": "Ready For Refine",
          "Text": "Lorsque nous travaillons aux côtés d’analystes aguerris, regardons les mouvements de leurs yeux à la première lecture d’une table. Avant « d’attaquer la découverte » avec des techniques de profilage (voir infra, Jauger la qualité de données initiale : profiler la donnée), leur regard balaie souvent les colonnes en long et en large, et ils identifient au premier abord les clés, les métriques et les attributs en présence pour avoir une première idée de la pertinence de la donnée qu’ils regardent. Ils vont repérer en quelques secondes des éléments souvent rédhibitoires ou problématiques comme :  \n– l’absence de clé ou leur homogénéité en prévision de comptage ou de lien avec d’autres tables ;  \n– la présence d’attributs avec une très forte hétérogénéité qui risquent de rendre les regroupements et les agrégations délicats ;  \n– la présence de colonnes totalement ou partiellement vides qui vont créer des trous dans les analyses et biaiser les résultats ;  \n– le format des dates et les traitements (pénibles) à venir qui vont nécessiter du temps de préparation ;  \n– l’absence de richesse des attributs et la diversité des métriques qui vont compter pour des analyses pertinentes ;  \n– la présence d’attributs sous forme de code qu’il faudra interpréter et traduire pour les analyses ;  \n– les colonnes booléennes qui vont départager les datas de manière nette (binaire).",
          "Created At": "2025-09-06T13:22:34.078+02:00",
          "Updated At": "2025-09-06T13:22:34.078+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE068_1757157753084",
          "Status": "Ready For Refine",
          "Text": "## Jauger la qualité de données initiale : profiler la donnée\n\nLa compréhension de la structure intrinsèque de la donnée est une première étape. Cette phase d’audit indispensable, surtout lors de la découverte de nouvelles données, qui consiste à passer en revue le jeu de données avant son analyse, est appelée profilage (profiling). Elle va permettre de prendre connaissance de la donnée, de comprendre sa structure et d’évaluer sa qualité. Avec un peu d’habitude, c’est aussi durant cette étape que nous prenons conscience de la charge de travail de préparation et du potentiel analytique du jeu de données.\n\nUne première étape est de comprendre la géométrie de la table : nombre de lignes et de colonnes, ainsi le taux de remplissage des colonnes. Cela va nous être utile pour éventuellement ajuster les outils de traitement pour faire face au volume ou choisir de travailler sur un échantillon.\n\nTypiquement, nous récupérons deux nombres à ce stade, compte de lignes et compte de colonnes), comme (2488225, 14) : 2 488 225 lignes et 14 colonnes.\n\nLa deuxième étape du profilage est d’identifier les métriques, les attributs et les clés, voire les éléments calculés existants et potentiellement pertinents à ajouter. Nous allons également revoir les types de données et la cohérence des formats utilisés. Cette phase peut se faire avec un simple éditeur de texte si nous travaillons sous fichiers plats ou par visualisation de la table correspondante. Reconnaître les métriques donne une idée de ce que le jeu de données va nous permettre de mesurer. Les attributs indiqueront le contexte de ces données et les différents axes d’analyse. Les clés permettront d’évaluer le niveau de granularité des informations et leur propension à être jointes à d’autres données. Les éléments calculés enfin, qu’ils existent ou qu’ils soient prévus par des calculs ultérieurs, permettront de juger du potentiel analytique de la table au-delà des colonnes existantes. Cela pourra aussi nous donner une idée du volume de préparation nécessaire, voire de la validité du set dès le départ. Il est inutile de commencer une analyse si aucune cohérence n’apparaît dans les données de la table.\n\nEnsuite, nous allons plonger un peu plus en détail et analyser la qualité des différents éléments constitutifs du jeu de données. Comme ils ont des types et des rôles différents, notre attention va se porter sur des aspects spécifiques de chacun d’entre eux.\n\nProfiler est toujours bon, mais tout n’est pas bon à profiler\n\nCette phase préliminaire de profilage est primordiale lors des premières prises en main, elle pourra être limitée à un audit plus élémentaire sur un processus mature et récurrent. Elle peut s’avérer fastidieuse sur des jeux de données larges, aussi doit-elle se concentrer sur les colonnes pertinentes pour l’analyse. Inutile de chercher à auditer des pans de données que nous n’utiliserons pas. D’ailleurs, que font-ils toujours dans nos tables s’ils ne sont pas utiles ?\n\n\\> La revue des clés existantes\n\nL’identification des clés ou des identifiants de lignes est fondamentale. Une table sans identifiants pour ces éléments pourra difficilement être rattachée à d’autres jeux de données, mais, surtout, nous ne pourrons pas identifier un élément plutôt qu’un autre dans nos requêtes. Faute de colonne « clé », nous devrons regarder si des combinaisons de colonnes peuvent permettre de générer cette clé. Nous pourrions ainsi combiner une date avec un type d’événement, une taille avec une couleur pour tenter de trouver cet identifiant unique. Notre attention devra aussi se porter sur la concision et la régularité du format de ces clés. Leur hétérogénéité ou leur format complexe peut être un signe de la difficulté de leur utilisation future.\n\nLa revue de la qualité des clés pourra se faire par des comptages, une mesure de leur structure ou longueur, une revue visuelle ou des graphiques.\n\nUn compte et un compte distinct des clés permettent de valider leur unicité de manière simple et rapide.\n\nUne revue visuelle peut permettre de percevoir les formats de clé standards.\n\nUn compte de caractères peut permettre d’évaluer la stabilité d’une clé.\n\nDes graphiques représentant la distribution des valeurs ou des longueurs de clé permettent une première validation rapide.\n\nÊtre le maître des clés\n\nLorsqu’une table ne possède pas de clé unique permettant d’identifier un enregistrement particulier, il peut être pertinent de créer une clé à partir d’attributs existants pour générer un identifiant unique.  \nPrenons un jeu de 52 cartes. Un tableau comptant le nombre d’occurrences d’une couleur montre qu’elle n’est pas une clé unique. De même pour la valeur d’une carte. Notre seul moyen de pouvoir extraire une carte en particulier est de combiner la clé couleur et la clé valeur. Si nous demandons un pique, nous aurons 13 cartes en retour. Si nous demandons une dame, nous aurons 4 cartes en retour. Cependant, si nous appelons la dame de pique, nous n’aurons qu’une seule carte correspondante.  \n  \nIllustration 22. Compte des clés couleur, valeur et valeur-couleur dans un jeu de cartes.  \nSeule la combinaison valeur-couleur est une clé unique.\n\n\\> L’audit des attributs\n\nLa revue des attributs permet d’abord de voir leur répartition au travers du jeu de données. Par exemple, une colonne {âge, sexe, prix} devrait se répartir selon une certaine courbe, comme une courbe de Gauss par exemple. Elle va permettre de voir combien d’attributs ne sont pas renseignés : ils représenteront des trous dans notre analyse.\n\nDans l’illustration 23, la courbe en cloche est un indicateur d’une probable bonne qualité des attributs classe de cabine et classe d’âge.\n\nIllustration 23. Ventilation du compte des survivants et des victimes du Titanic par classes d’âge et par classes de cabine16\n\nEnfin, cette revue permet d’examiner la variabilité de ces attributs : comme pour le profilage des clés, avons-nous plusieurs expressions pour un même attribut, avec ou sans espaces, avec ou sans majuscules, avec des erreurs de frappe, etc. ?\n\nLa qualité des attributs conditionne celle de nos analyses, de nos représentations graphiques et du résultat de nos algorithmes.\n\n\\> Le contrôle des métriques\n\nLa validation des métriques est plus délicate. La donnée doit tout d’abord être un nombre correctement constitué de chiffres. Il arrive que des grands nombres comportent l’exposant « E » ou que des fichiers étrangers n’aient pas le même signe pour marquer les décimales (un point ou une virgule), voire comporte des séparateurs de milliers, ce qui conduit à leur prise en compte comme des chaînes de texte alphanumériques. Ces formats vont empêcher les agrégations et les calculs ultérieurs. Ils vont bien souvent forcer les solutions d’analyse à considérer la colonne comme une colonne d’attribut.\n\nEn dehors de ce risque de format, il est souvent délicat de savoir si un chiffre est correct dans l’absolu. Ce constat nous conduit à procéder par revue analytique pour auditer sa variabilité dans le temps, ou à faire une série de calculs élémentaires tels que somme, moyenne, minimum et maximum, voire déviation standard, pour identifier si les nombres ont une certaine cohérence.\n\nCes phases peuvent être faites par des calculs ad hoc, sous forme de tableau. Elles peuvent également être réalisées sous forme de graphique, comme pour une analyse classique.\n\nLes deux graphiques du haut de l’illustration 24 indiquent des valeurs a priori cohérentes sauf peut-être à partir de la seconde moitié de l’abscisse. Dans le deuxième, le nombre de pièces moyen atteint les 100, ce qui semble anormal. Celui du bas, qui donne l’âge moyen des maisons, semble présenter beaucoup de variabilité mais dans des valeurs d’âge cohérentes (entre zéro et 100 ans).\n\nIl est parfois plus facile à l’œil de repérer un problème de cohérence entre plusieurs métriques dans un nuage de points ou sur une courbe dans le temps que dans un grand tableau.\n\nIllustration 24. Trois exemples de visualisation de la variabilité d’une métrique\n\nIllustration 25. Exemple de relative cohérence entre deux variables en x et y\n\nNous voyons un groupe de points en bas à droite de la figure 25 qui ne semble pas être en ligne. S’agit-il d’une erreur de saisie ou d’une opération particulière ? Ce sera au data pionnier d’investiguer.\n\nMalheureusement, l’absence de variabilité ou d’anomalies n’est pas synonyme de validité : elle ne remplace pas l’expérience et la communication avec le terrain que nos gains en efficacité nous permettront d’avoir régulièrement.\n\n\\> La validation des calculs\n\nPour la validation des calculs, la relecture des formules, leur test sur des valeurs connues ou extrêmes telles que 0, 1, – 1 ou leur projection dans des graphiques ou des matrices de résultats évitent les erreurs classiques.\n\nLe second risque est celui de leur ordre d’application. Nos analyses agrègent souvent de gros volumes de données avec des sommes, des moyennes ou des comptes (voir infra, Optimiser les analyses par les données calculées/préparées). Si des éléments calculés sont déjà des ratios calculés à l’origine entre colonnes, alors leur regroupement en sommes peut poser un problème d’ordre de calcul. Ce sont les fameux problèmes d’associativité ou de commutativité des opérations.\n\nNous pensions que ces notions de collège ne nous serviraient plus ? Elles reviennent hanter nos rapports en nous forçant à positionner nos calculs aux bons endroits dans la chaîne de traitement, et parfois à la toute fin d’un processus, pour éviter que la moyenne des sommes ne devienne la somme des moyennes.\n\n\\> Les fonctions automatisées de profilage (profiling)\n\nGrâce à des fonctions de logiciel d’analyse ou de langage de programmation, il est possible d’automatiser la production de rapports de profiling assez détaillés. Regardons cela à partir de cette simple table répertoriant les performances motrices de voitures dont voici les premières lignes. Nous distinguons les attributs, les clés et quelques métriques.\n\nNous pouvons appliquer une première analyse de nos métriques.\n\nCertains rapports de profilage, tels que pandas-profiling, accessibles à partir de commandes Python, vont aller encore plus loin dans la revue avec par exemple le nombre de cellules manquantes et celles dupliquées, le nombre de catégories ou de valeurs numériques.\n\nIls vont nous permettre d’aller en profondeur pour chaque colonne.\n\nCes rapports vont même nous proposer des analyses plus qualitatives telles que des corrélations ou des avertissements sur des problèmes potentiels.\n\n\\> Pour conclure\n\nCes tâches de profilage qui nous donnent la carte d’identité et le bulletin de santé de toute nouvelle data sont la condition nécessaire pour pouvoir travailler dessus avec sérénité. Elles doivent devenir un réflexe.\n\nTableau 7. Synthèse des points d’attention en profilage",
          "Created At": "2025-09-06T13:22:34.079+02:00",
          "Updated At": "2025-09-06T13:22:34.079+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE069_1757157753084",
          "Status": "Ready For Refine",
          "Text": "## Connecter la data pour une analytique plus riche et pertinente\n\nUne seconde phase de la chaîne analytique va être la manière dont nous organisons nos données. Cette phase est peut-être la plus technique pour les utilisateurs métier, car elle contient des concepts et des termes auxquels nous ne sommes pas habitués. Les prochaines pages vont être un peu délicates à digérer du fait de l’emploi de nouveaux termes, mais, une fois les concepts compris, ils seront acquis pour toujours et sembleront simples et logiques.\n\n\\> La notion de jointure\n\nUne jointure consiste à relier deux tables entre elles par l’intermédiaire d’une clé commune.\n\nPrenons l’exemple d’une liste de factures qui contient les codes de client. Si nous souhaitons détail­ler notre chiffre d’affaires en utilisant le pays du client ou son type d’activité, nous allons devoir chercher cette information dans la table client. Afin de join­dre chaque facture à ces attributs, nous allons les joindre à leur client correspondant de l’autre table en utilisant comme moyen de liaison le code du client, présent dans les deux tables. Ce numéro est appelé clé de jointure.\n\nCes jointures sont la clé d’une analyse avancée, car c’est grâce à elles que nous allons regrouper toutes les données, les facettes d’une situation, et les soumettre à des analyses holistiques.\n\nTraditionnellement, nous situons les tables jointes en fonction de leur position à droite ou à gauche sur le papier ou l’écran. Nous verrons dans les étapes suivantes que cette notion a son importance.\n\nNous voyons ici que les clés, qui, paradoxalement, n’ont qu’un petit rôle d’identifiant unique utile pour la sélection ou le comptage, ont un rôle fondamental dans la création de modèles de données, eux-mêmes cruciaux pour des analyses futures riches et pertinentes.\n\n\\> La cardinalité d’un modèle\n\nSous un nom complexe se trouve un concept simple : combien de clés à gauche de la jointure correspondent à combien de clés à droite. Logiquement nous avons trois cas de figure.\n\nLe One to One ou « un vers un »\n\nChaque clé à gauche n’aura qu’un équivalent à droite et réciproquement. Cela se produit quand nous joignons des données de listes finies telles que des pays, des catalogues produits, des groupes d’individus.\n\n→ Exemple de one to one\n\nUne table du PNB par pays jointe à une table de population par pays pour ensuite calculer un PNB par habitant constitue une relation one to one, car il n’y aura (en principe) que des pays uniques à gauche et à droite de la jointure.\n\nLe One to Many ou le Many to One, le « un vers plusieurs ou plusieurs vers un »\n\nÀ chaque clé unique d’un côté de la jointure va correspondre plusieurs clés de l’autre côté. Ce sera le cas le plus répandu. Souvent nous avons une longue liste d’enregistrements que nous souhaitons enrichir d’attributs stockés dans des tables de référence. Chaque enregistrement va être connecté à cette table et il sera tout à fait possible que certains d’entre eux pointent vers la même référence.\n\n→ Exemple de One to Many\n\nUne liste de factures sera certes composée de factures uniques, mais les clients sur ces factures pourront apparaître à de multiples reprises (un client pouvant faire plusieurs achats). Relier cette table de factures à la liste des clients pour obtenir un grou­pement de celles-ci par types ou villes du client donnera une jointure Many to One, ou bien, si vous permutez visuellement les tables, One to Many. Ainsi, si vous avez n factures pour un client, votre jointure à la table clients, où chaque client est unique, vous obtiendrez n lignes, enrichies chacune des détails du client que vous aurez sélectionné.\n\nLe Many to Many ou « plusieurs vers plusieurs »\n\nPlusieurs clés à gauche pointent vers plusieurs clés à droite. Cette option est la seule qui, même si elle est techniquement réalisable, ne donnera aucun résultat sensé. En effet, les n clés à gauche vont se connecter aux m clés à droite et vont générer n × m lignes de résultat. L’analyste n’aura plus un compte de lignes correct, qu’elles proviennent de la table de gauche ou de droite.\n\n→ Exemple de Many to Many\n\nNous croisons ici une table Commandes et une table Factures avec des clés « Code Client » apparaissant plusieurs fois dans chacune. Pour la jointure sur le Code Client, nous allons avoir 3 x 2 = 6 lignes de résultats, ce qui n’aura aucune pertinence analytique. Toutes les métriques de ces tables apparaîtront plusieurs fois (trois fois pour les montants de commandes et deux fois pour les montants de factures).\n\nCe type de jointure crée ce qu’on appelle un produit cartésien. Outre le fait que leurs résultats sont inutilisables, ces jointures peuvent gravement nuire à la performance d’un système. Lancées sans contrôle sur des milliers de lignes avec des clés répétées, elles peuvent rapidement générer des grands nombres de lignes en résultat (juste croiser 10 000 clés identiques avec 5 000 aboutit à 50 000 000 lignes), ce qui conduit invariablement à ralentir, voire à bloquer les bases de données qui croulent sous les volumes de lignes générées.\n\nC’est pour cela que nous devons nous garder de travailler en direct sur les modèles complexes des bases transactionnelles. Il est très facile de créer de produits cartésiens sans le savoir et d’entraîner des crashes système, avec les conséquences opérationnelles (voir chapitre 4, L’analytique transactionnelle n’est pas l’analytique décisionnelle).\n\n\\> Les types de jointures\n\nEn plus de leur cardinalité que nous venons de voir, les jointures vont également avoir un sens et une dynamique, d’où l’importance du positionnement des tables évoqué plus haut.\n\nIl existe quatre dynamiques de jointures (on passera les moins courantes).\n\nLa jointure simple ou inner join\n\nElle ne retourne que les éléments correspondants dans les deux tables jointes. Cette jointure est la plus restrictive. Elle peut sembler la plus courante car la plus simple, mais elle a l’inconvénient d’éliminer les enregistrements sans correspondance de part et d’autre.\n\n→ En reprenant l’exemple précédent, les pays qui n’apparaissent pas dans une des tables de statistiques que nous joignons (de nombreux pays n’ont souvent pas la capacité statistique de tout mesurer tous les ans) seront tout simplement complètement éliminés des résultats. Ils n’apparaîtront ni pour leur PNB ni pour leur population. Un analyste négligent les fera donc disparaître de son rapport.\n\nLa jointure gauche ou left join\n\nElle conserve toujours tous les enregistrements de la partie gauche et ajoute, s’ils existent, les éléments correspondants de la partie droite.\n\n→ Dans le même exemple, elle permettrait de conserver tous les pays de la table PNB avec ou sans statistique de population dans les résultats.\n\nLa jointure droite ou right join\n\nC’est l’inverse de la gauche.\n\n→ Toujours dans l’exemple, elle permettrait de conserver tous les pays de la table Population avec ou sans statistique de PNB dans les résultats.\n\nLa jointure full outer\n\nElle apporte dans les résultats l’intégralité des deux tables, même s’il n’y a pas de correspondance de part et d’autre.\n\n→ Nous aurons donc cette fois-ci l’ensemble les pays des deux tables, dont certains auront PNB et Population, d’autres n’auront que PNB ou Population.\n\nLes deux jointures droite et gauche sont les plus courantes car elles préservent l’intégralité de l’une des tables dont nous voulons conserver l’intégrité. Elles évitent par conséquent de se retrouver avec des extractions qui omettent des éléments quand une jointure n’a pas pu être trouvée.\n\n→ Dans l’exemple de la table des factures jointe à la table du détail des caractéristiques clients, il est classique de faire une jointure gauche (car la table facture est à gauche du diagramme) de manière à assurer la conservation de l’intégralité de chiffres d’affaires dans l’analyse.\n\nLe choix d’un type de jointure n’est donc pas anodin. Il dépend de la question posée. Une jointure peut avoir un effet :\n\n– filtrant, lorsqu’elle est stricte et ne retient que des éléments en commun ;\n\n– enrichissant, pour une base existante lorsqu’elle est gauche ou droite ;\n\n– combinant, lorsque l’on souhaite marier deux bases, même s’il n’y a pas forcément 100 % de correspondance.\n\n\\> Les résultats de jointures\n\nQu’obtenons-nous en résultat de ces jointures ? La requête nous restitue une table combinant les informations croisées. Nous pouvons accessoirement choisir les champs de chacune des tables à conserver dans les résultats.\n\nLes relations One to One\n\nRegardons les différents résultats de jointures de ces deux tables sur la clé Code Client. La table Factures a huit enregistrements et la table Créances Clients en a neuf. Cette jointure va nous permettre d’estimer le risque que nous prenons à émettre de nouvelles factures sur des clients qui ont déjà de gros encours de dettes sur des durées importantes.\n\nAppliquons les quatre grands modèles de jointure.\n\n• Jointure simple : seules les lignes avec des clés existant dans les deux tables vont être retournées par la requête. Au total, six lignes ont des clés communes.\n\n• Jointure gauche : toutes les lignes de la table de gauche, les factures, vont être retournées par la requête et celles qui ont une correspondance dans la table de droite, les créances clients, seront enrichies avec des données d’encours de dette.\n\nAu total, les huit lignes de la table Factures sont conservées et deux d’entre elles n’auront pas de données de créances correspondantes.\n\n• Jointure droite : toutes les lignes de la table de droite, la créance client, vont être retournées par la requête et celles qui ont une correspondance dans la table de gauche, les factures, seront enrichies avec des données de facturation. L’intégralité des neuf lignes de la table dette va être restituée, que des données de factures correspondent ou pas.\n\n• Jointure outer : les lignes des deux tables vont être restituées. Il y aura dix lignes dans les résultats. Trois lignes n’auront pas de détail de facture et deux lignes n’auront pas de détail de dette.\n\nLes relations « un vers plusieurs » ou « plusieurs vers un » (One to Many ou Many to One)\n\nNous ne détaillerons qu’un seul cas, la jointure gauche, les autres pouvant être déduits.\n\nCet exemple joint les dépenses par département d’une entreprise à leur détail par catégorie.\n\nLe résultat de la requête sera le suivant :\n\nLes lignes du côté « un » sont multipliées par le nombre de lignes correspondant coté « plusieurs ». Par conséquent, une partie de l’information générée se répète sur certaines colonnes. Les calculs de somme ne seront plus possibles sur celles-ci, car leur métrique ou attribut, si nous les comptons, est multipliée par le nombre de lignes du côté many.\n\nAfin d’éviter ce problème et qu’un utilisateur lambda ne tombe pas dans ce piège de l’agrégation, nous pouvons :\n\n– forcer l’agrégation par défaut à la moyenne, au minimum ou au maximum pour ne pas être impacté par la répétition ;\n\n– prédiviser les nombres par le nombre attendu de répétitions de manière que l’agrégation par la somme, qui est souvent celui par défaut, retourne à un résultat juste.\n\n\\> Pour conclure\n\nCes jointures sont finalement un exercice assez naturel. Dans la vie courante, nous appliquons des jointures régulièrement, lorsque nous regardons un sommaire, un index, un catalogue de références, ou que nous regardons les personnes communes de notre réseau professionnel ou personnel avec celui d’un ami.\n\nNous voilà prêts avec les trois concepts clés de la modélisation. Avec ces bases, nous allons pouvoir joindre tout type de données.\n\nSouvenir lointain d’Excel\n\nCela nous rappelle sans doute le recherchev (vlookup) d’Excel : effectivement, les tableurs suivent la même logique pour joindre deux tables. La logique de la donnée en table qui peut être jointe à d’au­tres s’applique tout simplement à toute technologie, et les tableurs ne font pas exception.",
          "Created At": "2025-09-06T13:22:34.080+02:00",
          "Updated At": "2025-09-06T13:22:34.080+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE070_1757157753084",
          "Status": "Ready For Refine",
          "Text": "Nous allons donc pouvoir progressivement joindre de la donnée et enrichir notre vision d’un problème de nouveaux points de vue. Petit à petit, notre table principale, qui contient les métriques que nous voulons étudier, va s’enrichir de tables de part et d’autre. Cette table centrale devient notre « table de fait » et les jointures tout autour forment les rayons d’un modèle qui ressemble à… une étoile : d’où le nom de ces représentations, le modèle en étoile.\n\nLa table de fait\n\nUne table de fait est une table qui contient les données que nous voulons observer/mesurer (les faits) sur le sujet que nous voulons analyser. Ici, il s’agit de la table des ventes.  \nIllustration 26. Modèle en étoile\n\nLes jointures de l’illustration 26 vont nous permettre d’établir une vision très riche de nos ventes.\n\nNous allons pouvoir répondre à des questions beaucoup plus fines qu’avec la seule table de Ventes initiale. Par exemple, nous pourrons mesurer :\n\n– les ventes par superviseur pour chaque pays ;\n\n– la famille de produits qui se vend le mieux pour chaque client ;\n\n– comment la météo influence les ventes, etc.\n\nNous avons vu au chapitre 4 que des modèles très enrichis pourront nécessiter des visualisations avancées pour présenter tous leurs éléments sur une seule vue. Nous verrons que l’intelligence artificielle peut être nécessaire pour détecter des corrélations et faire des inférences ou des prédictions à partir de toutes ces données reliées (voir infra, Le Big Data : ses mythes, sa réalité).\n\nLes modèles en flocon\n\nL’apparence de ces modèles a aussi inspiré d’autres noms tels que le modèle en flocon lorsque celui-ci présente plusieurs niveaux de jointure dans chaque branche de l’étoile.\n\nLes éléments clés d’un beau modèle de données décisionnel  \n\nPhilippe Champleboux, diplômé de l’École supérieure d’informatique, a démarré sa carrière en 1988. Après une première partie de carrière orientée informatique de gestion où il participa à des projets de toutes tailles, depuis des projets mainframe engageant le budget de la nation jusqu’aux développements d’application spécifiques pour micro-ordinateurs exploitant la « nouveauté » des réseaux locaux. En 1998, il s’oriente résolument vers la BI, prenant plaisir à contenter les utilisateurs avec des applications spécifiques, conçues pour eux, répondant à leurs besoins propres et ciblés d’indicateurs et de pilotage. À partir de 2008, il élargit son domaine de compétence en travaillant sur des projets d’élaboration budgétaire qu’il juge complémentaires à la BI (on exploite les informations du passé, on se donne des objectifs et on se projette dans le futur).  \nEn 2014, il rejoint un cabinet d’experts de l’informatique décisionnelle, Next Decision, et travaille indifféremment sur des projets BI et d’élaboration budgétaire, tout en collaborant avec l’IGR-IEA de Rennes dans la structuration d’un nouveau cours, « élaboration budgétaire » adossée à l’un des logiciels phares du marché actuel.  \nPourquoi un beau modèle (physique ou virtualisé) est clé pour une analytique facile, performance et juste.  \nLa modélisation d’un modèle de données BI doit répondre à plusieurs objectifs, en apparence contradictoires, notamment la lisi­bilité, afin que des utilisateurs puissent intuitivement retrouver l’information qu’ils désirent retrouver, la simplicité d’utilisation, l’unicité (univo­que) des termes, vocabulaire et concepts employés, la performance en termes de restitution et d’exploitation, la maintenabilité et l’évolutivité.  \nCes objectifs sont autant de défis à relever, dont la réalisation permettra de voir fleurir un sourire de contentement et de satisfaction de la part des utilisateurs (alors que je n’ai jamais vu un utilisateur bondir de joie quand il recevait un nouveau progiciel de gestion !).  \nMais… qu’est-ce qu’un « beau » modèle ? Hormis les questions d’esthétique, somme toute assez subjectives, un beau modèle est un modèle de données qui répondra à tous les objectifs cités précédemment.  \nChaque utilisateur devra pouvoir retrouver une vision qui lui est propre et uniquement celle dont il a besoin. Un responsable commercial sera intéressé par la performance et le travail de ses commerciaux ; un gestionnaire RH sera intéressé par l’importance et l’évolution des masses salariales ; le directeur financier par ses encours et sa trésorerie ; le trésorier par l’évolution des taux du marché, des cours de devise, des échelles d’intérêt, etc.  \nChacun des métiers de l’entreprise peut avoir besoin (ou pas !) d’informations statistiques lui permettant d’analyser la situation vivante, en termes de quantité, de qualité, de valorisation, d’évolution, de comparaison, de pourcentage d’atteinte, et de décliner ces analyses selon ses propres axes métier (produits, clients, géographie, circuit de distribution, devise, etc.).  \nOn y parlera de la manière la plus précise possible, distinguant par exemple le CA brut sur facture du CA net après remises, du CA net après déduction des frais de transport, du CA net après déduction des coûts variables, etc.  \nMes recettes métier pour un modèle élégant et performant  \nA) Soyez extrêmement précis dans les termes que vous employez et pensez à autant d’indicateurs en cas de risque de confusion.  \nN’oubliez jamais que, lorsque vous sortirez un reporting, une statistique, un tableau de bord, il faudra que tous parlent le même langage et qu’il soit impossible (bon, d’accord extrêmement difficile) de produire la même statistique avec des montants différents. Pensez que, d’un service à l’autre, la même notion peut cacher des significations différentes. Qui n’a jamais entendu dire qu’il faudrait réconcilier le chiffre d’affaires commercial et le chiffre d’affaires comptable ?  \nAfin de satisfaire l’ensemble des interlocuteurs concernés par cette notion de chiffre d’affaires, il faudra dresser :  \n1. un certain nombre d’indicateurs successifs permettant cette réconciliation : CA brut facturé, CA net après remises, montant comptable des remises arrière, montant comptable des avoirs, clients douteux, créances irrécouvrables, etc. ;  \n2. plusieurs tables de faits, chacune ayant la granularité souhaitée par les utilisateurs désirant exploiter individuellement chacun de ces indicateurs ;  \n3. une table de fait unique, avec le niveau de granularité et d’axes d’analyse permettant de stocker l’ensemble des indicateurs et d’avoir ainsi une vue comparative unique de cette réconciliation.  \nB) Effectuez et stockez tous les calculs possibles lors de l’alimentation de votre modèle.  \nRépondez simplement à la question suivante : est-il plus rapide d’effectuer un calcul, une fois, lors du chargement de votre modèle, ou plusieurs centaines de milliers de fois, lorsque chaque utilisateur voudra analyser le résultat d’un calcul ?  \nD’accord, la formulation de la question entraîne d’elle-même une seule réponse (la bonne réponse !).  \nLorsqu’un calcul, tel qu’une conversion, une addition, une multiplication par un taux peut se faire lors de l’alimentation et que le résultat de ce calcul n’évolue plus par la suite, il est conseillé de le calculer, de le stocker, afin que tous les utilisateurs puissent directement utiliser ces résultats (pas de risque d’erreur, pas de risque de formules dépendant de l’utilisateur, moins de risques de dégradation des performances). Il m’est arrivé, dans le cadre de la mise en œuvre d’un BI dans une entreprise de vins, liqueurs et spiritueux de stocker les indicateurs de volumes (commandés, achetés, fabriqués, stockés, vendus, expédiés, facturés, cassés, etc.) en différentes unités : nombre d‘articles (en unité), nombre d’articles (en contenance totale d’alcool), nombre d’articles (en équivalent « caisse standard »).  \nEt n’oublions pas tout ce qui va permettre d’effectuer des statistiques en « devise de transaction », « devise de reporting », « devise de comptabilité », pour lesquels on s’attachera à stocker les montants concernés dans chacune des devises concernées, mais également leur contre-valorisation en devise « statistique » permettant de tout sommer et additionner. (Bien sûr, utilisez le bon cours de conversion… pas forcément le même cours pour contre-valoriser du bilanciel et du résultat ?)  \nC) Unifiez vos référentiels et vos données. Appelez un chat un chat ! (Et nommez chacun de vos chats !)  \nUtilisez un identifiant unique pour chacun de vos axes d’analyses métier, et ce, quelle que soit l’origine (parfois multiple) de vos données de référence. Sans aller jusqu’à initier le concept MDM du « Golden record », vous devez impérativement croiser, mapper, identifier, faire correspondre vos référentiels article, clients, sites, société, usine, établissement, etc.  \nSi vous avez des données qui viennent à la fois de votre logiciel de gestion commerciale et de votre logiciel de fabrication ou de gestion de stock, vous devrez impérativement arriver à faire correspondre la codification « commerciale » des articles (commandés, vendus, facturés, cassés, etc.) avec la codification des articles (stockés, consommés, fabriqués, produits). Ce n’est qu’à ce prix que vous pourrez procéder (par exemple) à des calculs de marge sur coûts variables. Dans la même veine, unifiez vos unités internes pour ramener vos calculs à un système de mesure cohérent.  \nExemple des prix : prix à l’article, prix au kilo, prix à la palette, prix à la tonne… Quelle est la marge unitaire d’un article vendu 1 euro le kilo dont le prix de revient unitaire est de 10 centimes et le coût de transport de 50 euros la tonne ?  \nIl vous faudra parfois établir vos propres tables de correspondance (obligatoirement univoque) entre une codification article commerciale et une codification produit logistique ; cette table de correspondance pourra faire intervenir des coefficients de mappage.  \nAssurez-vous de la complétude de vos référentiels de données, qui seront autant d’axes d’analyse pour étudier vos indicateurs et en déduire vos axes d’amélioration.  \nIl y a quelques années de cela, alors que nous démarrions le chargement de notre Data Warehouse, après avoir (re)codifié nos référentiels articles et clients, nous avons constaté près de 20 % de rejets lors de l’alimentation des données « commandes ». Il nous manquait des clients ! « Impossible, nous dit le responsable informatique, je vous ai communiqué l’intégralité des clients autorisés à passer des commandes, les autres sont blacklistés par notre politique relative aux mauvais payeurs ! »… Les commandes indûment enregistrées par le progiciel ont pu, grâce au Data Warehouse, être bloquées. À l’heure où l’on parle de retour sur investissement, je pense que jamais projet informatique ne fut aussi rapidement amorti !  \nQuelques écueils à éviter  \nComparer des données ayant des niveaux de granularité différents  \nImaginez que les objectifs des commerciaux sont assignés par commercial, par région et par trimestre, et que, par ailleurs, vous ayez le détail du chiffre d’affaires par commercial, ville, mois et produit.  \nVous ne pouvez avoir qu’une seule table, car les axes ne sont pas les mêmes et certains axes communs n’ont pas le même niveau de granularité.  \nModélisez donc au minimum deux tables de faits, l’une contenant le détail du CA réalisé, l’autre contenant (par commercial, région et trimestre) le CA réel et le CA objectif. Cette seule table de fait vous permettra, quel que soit le logiciel d’analyse utilisé, de sortir tous vos indicateurs de performance, pourcentage atteint, évolutions, etc., selon une granularité ou maille qui convient à cette vision « Objec­tivation des commerciaux ».  \nFaire fi du futur",
          "Created At": "2025-09-06T13:22:34.080+02:00",
          "Updated At": "2025-09-06T13:22:34.080+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE071_1757157753084",
          "Status": "Ready For Refine",
          "Text": "Votre société évoluera (on vous le souhaite), vous êtes déjà au courant de futures évolutions (acquisition, innovations, nouvelle activité, etc.). Vous devez, lors de votre modélisation, lors de la définition de votre modèle de données, de la conception de vos dimensions et de vos tables de faits, penser au futur, vous poser la question d’axes d’analyse futurs (devise, langue, pays) qui sont déjà dans l’air du temps, mais peut-être pas encore vivants dans les faits.  \nPosez-vous alors la question : qu’est-ce que je sais (ou subodore) des évolutions, des projets, des changements dans la société et saurai-je y faire face ou ne puis-je pas, dès maintenant, prévoir un modèle « multisociété », « multidevise », « multi-activité », etc.  \nNe pas respecter les contraintes RGPD  \nVous ne pouvez plus ignorer les contraintes réglementaires de la protection des données personnelles, les conséquences en sont pécuniairement trop graves pour votre société.  \nGarantissez le cas échéant le droit à l’oubli, à l’anonymisation, à la communication.  \nFaites-vous aider par la personne (ou le service) en charge, dans votre société, du respect de ces contraintes réglementaires.  \nMes conseils aux data pionniers  \n« Ratissez large pour démarrer petit », ne soyez pas trop ambitieux pour votre (vos) premier(s) projet(s). Vous pouvez identifier et analyser un grand panel de besoins, pour différents services de votre société, mais ne commencez la réalisation que d’un domaine fonctionnel qui vous permettra de faire vos premières armes : tester la solidité des solutions logicielles implémentées et choisies vous apprendra les bons réflexes.  \n« Décomposer chaque sujet en plusieurs sujets aisément maîtrisables » : si un sujet, une vision, un besoin vous paraît trop vaste, trop complexe pour être traité dans son ensemble, décomposez ce sujet en plusieurs sujets plus simples et, le cas échéant, décomposez encore tout ou partie de ces sujets en sujets plus simples, jusqu’à arriver à un ensemble de sujets maîtrisables et réalisables (sujets que vous allez réaliser).  \nUne fois tous les sujets élémentaires traités, résolus, validés, leur intégration en un sujet plus vaste sera aisément facilitée.",
          "Created At": "2025-09-06T13:22:34.081+02:00",
          "Updated At": "2025-09-06T13:22:34.081+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE072_1757157753084",
          "Status": "Ready For Refine",
          "Text": "# Tirer le meilleur de la donnée avec une bonne préparation\n\nOublier d’ajouter les clés dans un jeu de données.\n\nLes phases de modélisation associées aux phases de préparation sont essentielles pour la qualité et la performance des analyses. Nous avons vu que les bases de la modélisation tiennent en quelques éléments. Nous allons voir maintenant que les techniques de préparation de données peuvent également être résumées en une liste d’actions simples.",
          "Created At": "2025-09-06T13:22:34.082+02:00",
          "Updated At": "2025-09-06T13:22:34.082+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE073_1757157753084",
          "Status": "Ready For Refine",
          "Text": "## Ajouter des tables de groupements ou des tables de hiérarchies\n\nL’objectif est de permettre une analyse à différents niveaux de granularité, comme si on regardait la data à différents niveaux de détails, à travers un télescope, un microscope, une loupe ou tout simplement à l’œil nu.\n\nUn chiffre d’affaires, par exemple, peut être considéré au niveau mondial ou par continent, pays, ville, commercial, type de produit, produit, jour, etc., en fonction des besoins d’analyse. Des entités peu­vent être analysées géographiquement, mais aussi par leur forme légale ou leur activité.\n\nCes regroupements ou hiérarchisations s’accompagnent de la possibilité d’agréger les métriques à chaque niveau par le biais de sommes, minima, maxima, moyennes, comptages etc. (voir supra, Connaître ou définir les règles d’agrégation de la data).\n\nAlors comment appliquer un groupement ou une hiérarchie à nos tables de faits, pour qu’elles ne soient pas seulement une liste interminablement détaillée dans laquelle il est difficile de se retrouver ?\n\nIl nous suffit de faire une jointure ! Une table qui hiérarchise des pays, des produits ou des entités, ou leur applique des attributs de regroupement peut instantanément s’appliquer à une table de faits à partir d’une simple jointure. Là où l’opération peut devenir périlleuse sur des tableurs dès que les volumes augmentent ou que le nombre de jointures se multiplie, elle conserve sa simplicité avec des tables de données. Une jointure est une construction logique qui s’applique indifféremment à 10 lignes ou à 1 million (voir supra, Connecter la data pour une analytique plus riche et pertinente).\n\nParmi les hiérarchies fréquemment retrouvées dans les décisionnels, nous trouvons :\n\n– les hiérarchies géographiques : du continent à l’appartement ;\n\n– les hiérarchies des secteurs d’activité : de l’industrie au détail de l’activité d’une entreprise ;\n\n– les hiérarchies de personnes : du patron au stagiaire par exemple.\n\nDans l’illustration 27, nous pouvons rattacher l’activité de la Barbade (Barbados) à celle de la région « Caribbean », qui est dans « Latin America and the Caribbean », qui fait partie des « Americas » et est supervisée par Carole Rylance.\n\nIllustration 27. Table de hiérarchie pays/sous-région/région/continent  \net d’attribution à des managers\n\nAutre exemple, voici une belle hiérarchie du règne animal, con­centrée sur une infime partie incluant les dinosaures.\n\nIllustration 28. Extrait de la hiérarchie des animaux : règne, phylum, classe, famille et nom, soit six niveaux de détails\n\nCertaines hiérarchies peuvent être développées spécifiquement pour un contexte. Rien n’empêche de créer des niveaux de regroupement pour tout élément analysé. Ces derniers n’ont pas forcément besoin d’être une hiérarchie, ils peuvent juste allouer des codes pour indexer des contenus suivant plusieurs axes d’analyse potentiels : par exemple, une liste de produits peut se voir attribuer des codes tels que « dangereux », « lourd », « fragile », « beau ».\n\nLa magie des reportings « multidimensionnels » est en fait liée à l’ajout de ces hiérarchies ou groupements. Toute table de faits peut devenir « diaboliquement » multifacette, dès qu’on lui joint des tables regroupant et enrichissant ses attributs. Il suffit d’une simple jointure. N’hésitez pas à en créer plusieurs pour supporter tous vos axes d’analyse.",
          "Created At": "2025-09-06T13:22:34.082+02:00",
          "Updated At": "2025-09-06T13:22:34.082+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE074_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Créer des clés pertinentes pour de nouvelles jointures\n\nLe potentiel analytique d’une table de faits est fortement lié aux possibilités de la joindre à d’autres tables. Les clés jouent un rôle critique. Leur gestion ou leur création ne peut être laissée au hasard. Voici quelques points tactiques à garder en tête au niveau métier pour éviter des frustrations ou des catastrophes dans les analyses futures.\n\n\\> Se conformer aux clés du transactionnel\n\nC’est une évidence, beaucoup de clés sont définies dès le transactionnel : les codes clients, les codes produits. Parfois leur nomen­clature est imposée par le système, d’autres fois nous aurons la possibilité de la définir. Si les options de format sont théoriquement illimitées, certaines pratiques sont moins « bonnes » que d’autres.\n\n\\> Ne pas générer des clés très longues\n\nCertaines clés doivent être d’une taille suffisante pour capturer la diversité de la population qu’elle couvre. Par exemple, le VIN (Vehicule Identification Number ou Numéro d’Identification du Véhicule) sert d’empreinte digitale à la voiture, car deux véhicules en fonctionnement ne peuvent pas avoir le même. Un VIN est composé de 17 caractères (chiffres et lettres majuscules) qui agissent comme un identifiant unique. Un VIN affiche les caractéristiques uniques, les spécifications et le fabricant de la voiture.\n\n→ Avez-vous déjà vu le VIN de votre voiture ?\n\n\\> Ne pas choisir des clés numériques qui commencent par 0\n\nEn théorie, ce n’est pas un problème au sein d’une base de données où le format, comme nous l’avons vu, peut être contrôlé et normalisé. En pratique, cela peut rapidement poser un souci si la donnée doit être traitée hors système pour des analyses ponctuelles ou pour des processus dans des équipes qui n’ont pas accès à ces systèmes. En effet, lorsqu’elle est sortie de son environnement normatif, cette clé pourra être considérée comme un nombre et perdre son zéro. Excel ne fera aucun cas de ce zéro et le supprimera la plupart du temps pour faire de cette clé un simple nombre.\n\n\\> Éviter les clés sans format standard\n\nCe n’est pas un choix rédhibitoire. Cela va juste rendre moins délicat les actions de profilage qui ne pourront pas s’appuyer sur une règle précise pour contrôler la forme de la clé.\n\n\\> Choisir des clés universelles ou ISO quand c’est possible\n\nCela n’a pas d’impact quand nous travaillons en vase clos, mais cela peut rendre la jointure avec des tables externes plus difficile. Pourquoi continuons-nous à coder nos pays avec des codes internes alors que chaque nation a son code ISO, indépendant de la langue, qui permet immédiatement des jointures avec toute donnée économique, démographique ou politique de la planète ?\n\nTableau 8. Exemple de codes ISO rattachés à un pays\n\n→ Jointure impossible après le choix d’une clé locale\n\nUn analyste avait choisi de coder la devise américaine DOL, parce que USD (US Dollar), la dénomination standard, n’était pas intuitive. Dans un rapport, ce choix pouvait avoir un certain rationnel, et encore, mais dans des bases de données ? Quand il s’est rendu compte qu’aucune table contenant des devises ne pouvait être jointe, il était trop tard. Il a fallu créer une table de correspondance intermédiaire pour relier DOL des tables de faits à USD, puis USD aux autres tables du modèle.",
          "Created At": "2025-09-06T13:22:34.083+02:00",
          "Updated At": "2025-09-06T13:22:34.083+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE075_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Optimiser les analyses par les données calculées/préparées\n\nDans notre recherche de nouvelles métriques et de gains de temps, ces données calculées vont être précieuses. Ajouter des calculs aux données de base peut sembler une évidence, mais nous ne poussons pas toujours la logique jusqu’au bout. Si nous revenons aux motivations premières de mieux maîtriser la data – gagner en visibilité et gagner en temps –, la création et le bon positionnement des données calculées peuvent avoir un impact immédiat sur les deux.\n\nOr, souvent, dans des équipes d’analystes, chacun récupère sa donnée (sous Excel très souvent) et fait ses préparations, ses agrégations et ses calculs de son côté. Pourquoi ne pas effectuer ces opérations en avance de phase une fois pour toutes, pour que plus personne n’y consacre la moindre énergie ? Certes, chacun aura besoin d’une donnée propre à ses questions métier, mais on peut penser qu’il existe tout un pan commun de data préparée à l’avance, que chacun pourrait récupérer. Plus nous raffinons notre matière première qu’est la data, moins il reste de travail à faire par la suite, et plus nous pouvons nous concentrer sur l’analyse et l’action.\n\nNous allons aborder ces éléments calculés en quatre groupes et nous verrons ensuite quelles méthodes et technologies peuvent nous aider à réaliser ces gains de productivité promis.\n\n\\> La préparation de calculs complexes\n\nCombien sommes-nous dans toutes les équipes d’analystes à travers le monde à faire les mêmes calculs à chaque nouvelle échéance de reporting ? N’y a-t-il pas des calculs que nous pourrions mutualiser ? Non seulement nous y gagnerions en temps, mais nous réduirions notre risque d’erreur à quasiment à zéro.\n\n→ La conversion de devises\n\nL’exemple de la conversion des devises pour des filiales reportant à leur maison mère est classique. Si chaque filiale fait un reporting dans la devise du groupe, nous nous exposons à :\n\n– des risques de prendre de mauvaises hypothèses telles que prendre les cours de change à la mauvaise date ;\n\n– appliquer le taux dans le mauvais sens (dollar vers euro au lieu de l’inverse) ;\n\n– appliquer les mauvaises quotités : certaines devises se cotent par 100, d’autres par 1 000, voire 10 000 ;\n\n– ou tout simplement faire une erreur de calcul en référençant de mauvaises cellules ou colonnes dans nos tableurs.\n\nUne solution simple est de ne pas demander ces conversions au niveau local, mais de récupérer les données dans leur devise d’origine, d’appliquer les conversions dans un environnement central, contrôlé et fiable, et ensuite de redistribuer les informations converties à chacune des équipes. Nous verrons que cette option est simple à mettre en œuvre, si nous travaillons avec des tables, de manière fluide… en dehors des tableurs.\n\nFaisons un calcul simple : si, dans 20 filiales, 2 personnes passent une demi-heure par mois pour faire ce travail de conversion (en tenant compte de la mise à jour des taux et des calculs, de leur vérification, etc.), cela fait 20 heures par mois qui pourraient être consacrées à des travaux à valeur ajoutée si cette conversion était faite en central de manière instantanée grâce aux ETL, outils analytiques et procédures stockées (voir infra, Optimiser le choix du moteur de calcul).\n\n\\> Les agrégations et les réductions du nombre de colonnes\n\nLes agrégations permettent d’ajuster le volume de données entrantes au besoin d’analyse. La tentation est grande de tout vouloir, car « on ne sait jamais ». On s’encombre alors d’une data qui ralentit nos processus et alourdit les transferts des reportings.\n\nSi notre seul but est de présenter un rapport (on ne parle pas de l’analyse sous-jacente), alors le volume de données peut être considérablement réduit.\n\nCombien faut-il de lignes et de colonnes pour faire le graphique de l’illustration 29 a minima, quand bien même chacun de ces vendeurs aurait généré des milliers de transactions chacun ? Il suffit de 3 colonnes et 15 agrégats au vendeur-produit.\n\nIllustration 29. Graphique présentant la répartition des ventes par commerciaux et par produits\n\n→ Trop c’est trop\n\nGulnar, contrôleuse de gestion pour la partie production/supply chain, se plaignait sans cesse de la performance de sa machine, du réseau, des serveurs. Il lui fallait jusqu’à quinze minutes d’attente pour rafraîchir un rapport de production hebdomadaire. Après des semaines de tergiversation, d’échange de machine, de changement de câblage et d’ajout de mémoire sur les serveurs, les services informatiques décidèrent un dernier audit avant d’abdiquer.\n\nPour chaque analyse de synthèse qu’elle lançait, Gulnar réextrayait deux années de données au niveau le plus fin avant de procéder sur tableur à une agrégation massive des informations à la semaine et à la gamme de produits, puis de réduire de 90 % le nombre de colonnes pour ne conserver que celles réellement utiles à son travail. La suggestion faite par l’analyste fut de ne récupérer que les données nécessaires et de les agréger à l’avance. « Oui, mais si j’en ai besoin, je fais quoi ? » répondit Gulnar.\n\nL’analyste lui proposa de faire une extraction plus précise quand elle aurait besoin de zoomer sur une zone particulière ou de préparer des extractions de données un peu plus riches que ses besoins pour éviter de recourir à de nouvelles requêtes trop souvent. Épilogue : le volume de données fut réduit d’un facteur 30, sans dégrader aucune des analyses, Gulnar gagna 90 minutes pour chaque processus et ce n’est que très rarement qu’elle alla chercher plus de détails.\n\n\\> Les corrections de format\n\nSelon leur système d’origine ou les personnes qui ont saisi les données, les informations peuvent arriver avec une grande diversité de formes. Certaines peuvent nuire à la lecture ou à l’esthétique d’un rapport, telles que l’usage de capitales, l’apparition de caractères spéciaux ou d’espaces avant les champs, sans mentionner les variations de dates (voir supra, Les fondamentaux pour comprendre la donnée). Plutôt que de procéder à une normalisation de ces chaînes de caractères en toute fin de processus, lors des phases d’analyse, nous pouvons bâtir des traitements en avance de phase qui effectuent ces tâches une fois pour toutes et pour tout le monde.\n\nLes capitales, ce n’est pas capital\n\nUne application immédiate et simple de ces corrections de format est celle du reformatage des intitulés de colonnes ou de noms apparaissant dans les rapports. Les noms de clients, de fournisseurs ou de personnes sont souvent en majuscules dans les systèmes transactionnels. Ce qui peut être avantageux pour des en-têtes de facture ou autre document commercial peut devenir un problème pour la lisibilité du rapport d’analyse. Plutôt que chaque analyste développe sa propre macro ou y consacre une quelconque énergie pour remettre les lettres en forme (par exemple, première lettre en majuscules et les suivantes en minuscules), ce travail peut être fait systématiquement pour tous les champs entrant dans le décisionnel.  \nUne étude réalisée en 1955 par Miles Tinker a montré que « le texte tout en majuscules retardait la vitesse de lecture de 9,5 à 19 % pour les limites de temps de 5 et 10 minutes, et de 13,9 % pour l’ensemble de la période de 20 minutes17 ». Tinker a conclu que, « de toute évidence, l’impression tout en majuscules ralentit la lecture à un degré marqué par rapport aux minuscules romaines18 ».  \nTinker l’explique de la manière suivante : le texte tout en majuscules couvre environ 35 % de surface d’impression de plus que le même texte mis en minuscules. Cela tend à augmenter le temps de lecture. Si l’on ajoute à cela la difficulté de lire les mots tout en majuscules comme des unités, l’entrave à la lecture rapide devient marquée. Dans l’étude sur les mouvements oculaires réalisée par Tinker et Patterson, la principale différence dans les schémas oculomoteurs entre les minuscules et les majuscules était la très forte augmentation du nombre de pauses de fixation pour la lecture des caractères tout en majuscules19.  \nTinker préconise que tout texte en capitales soit éliminé des rapports à chaque fois que la rapidité de lecture et l’opinion du lecteur sont importantes : en plus de ralentir la vitesse, elle est ressentie comme parfois offensive par les lecteurs. Cela est d’autant plus important lorsqu’il s’agit par exemple de matériel de lecture continue, d’affiches, de panneaux d’affichage, de textes publicitaires, de magazines, de titres de livres, de formulaires et de rapports commerciaux, de titres d’articles, de livres, de chapitres de livres et de titres de journaux20.\n\n\\> Les snapshots, les sauvegardes d’instantanés\n\nCette technique consiste à sauvegarder tout un jeu d’enregistrements à un instant donné afin d’en conserver la situation exacte, sans qu’elle ne puisse être altérée. Nous pourrions penser qu’il suffit de filtrer a posteriori les informations par date pour retrouver l’état en question. Ce n’est pas toujours possible. Il arrive que ces données soient modifiées au fil de l’eau sans que les étapes de leur évolution soient conservées dans les systèmes.\n\nLes portefeuilles de prospection des vendeurs sont un exemple classique. À un instant donné, les opportunités de ventes se trouvent à un certain stade. Quelques jours plus tard, certaines auront avancé vers une vente de contrat et d’autres se seront éteintes : les statuts antérieurs de ces opportunités auront disparu. Pour connaître la dynamique d’évolution de ces affaires jour après jour, le seul moyen consiste à stocker les bilans au fur et à mesure. L’exercice consiste donc à prendre une photo des éléments pertinents pour l’analyse et à la stocker dans une table où elle sera indexée avec, par exemple, la date de sa capture.\n\n→ Évolution d’un portefeuille de vente\n\nCe graphique donne un exemple de situation d’un portefeuille de vente d’un commercial sur dix semaines. Beaucoup de nouvelles opportunités apparaissent, mais peu d’affaires arrivent à être clôturées.\n\nFigure 11. Évolution d’un portefeuille d’affaires, semaine après semaine\n\n→ Photo de classe\n\nLa seule façon de conserver une réalité d’une classe d’élèves à un instant est de prendre une photo. Dès le lendemain, cet effectif pourrait changer à cause d’élèves absents, partis, etc. En reconstituant soixante ans plus tard la classe de l’époque avec ces mêmes élèves, la photo sera différente.",
          "Created At": "2025-09-06T13:22:34.084+02:00",
          "Updated At": "2025-09-06T13:22:34.084+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE076_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Les options pour travailler la donnée : à la volée ou stockée\n\nIl existe deux manières différentes de traiter les données calculées. Aucune n’est meilleure que l’autre. À vous de choisir celle qui s’adapte le mieux à vos besoins analytiques et à vos contraintes opérationnelles.\n\nNous pouvons effectuer ces calculs une fois pour toutes et en stocker les résultats « en dur » dans les sources de données. L’intérêt est qu’il n’y aura plus de temps de préparation et d’attente : tout sera prêt à l’emploi. L’inconvénient est que, si les données sont trop volatiles, le calcul stocké peut ne plus être à jour. Calculer à l’avance des agrégats, des conversions ou des ratios sur des données d’un exercice fiscal clos est particulièrement pertinent : pourquoi recalculer à chaque fois quelque chose dont les hypothèses ne varieront plus ?\n\nNous pouvons également les faire calculer chaque fois qu’elles sont demandées. L’analyste n’a pas à effectuer les traitements comme précédemment : ces derniers sont justes calculés pour chaque donnée demandée. C’est l’assurance d’une donnée fraîche, mais aussi le risque d’attendre le temps que tout soit traité. Même cinq à dix secondes peuvent parfois sembler une éternité au cours d’un processus itératif d’analyse.\n\nCes deux alternatives de traitement présentent les résultats in fine de la même manière pour l’utilisateur, sous forme de table :\n\n– dans le premier cas, la table est physiquement écrite et prête à la lecture : c’est une « vraie » table ;\n\n– dans le second cas, les calculs sont faits à la volée, leurs résultats apparaissent dans une table virtuelle qui a l’apparence d’une table physique : c’est une « vue » qui, dès qu’elle est interrogée par l’analyste, cherche les données nécessaires et effectue les opérations prévues. Il y a donc un temps de latence pour l’analyste.\n\nComment choisir entre stocker les calculs et les faire à la volée\n\nLes calculs à la volée sont-ils la panacée ? Si nous pouvons attendre de longues secondes, voire des minutes, à chaque fois que nous demandons un calcul ou si notre puissance de calcul est illimitée, pourquoi pas ? Cependant, même dans ces cas-là, nous pourrions nous interroger sur ce besoin de tout recalculer à la dernière minute. Par ailleurs, certaines informations peuvent être largement suffisantes, même si elles ont été calculées la veille ou il y a quelques heures.  \nCe choix, c’est le vôtre. De plus, aujourd’hui, la capacité des disques n’est plus un facteur limitant pour stocker cette donnée préparée, donc pourquoi s’en priver ?",
          "Created At": "2025-09-06T13:22:34.084+02:00",
          "Updated At": "2025-09-06T13:22:34.084+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE077_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Optimiser le choix du moteur de calcul\n\nLes données calculées peuvent être créées à plusieurs niveaux :\n\n– dans l’outil analytique : cette option présente le mérite d’apporter l’agilité de dernière minute pour des calculs auxquels nous n’aurions pas pensé ou des besoins trop éphémères pour nécessiter un développement amont ;\n\n– dans la base de données : c’est aussi une puissante base de calcul, soit par le calcul de champs avec des formules que nous pouvons toujours programmer, soit avec quelques techniques particulièrement performantes (voir encadré infra Les procé­dures stockées dans les bases de données) ;\n\n– dans l’ETL : ce dernier peut effectuer de nombreux calculs de préparation. C’est le fameux T de l’ETL. Il peut également appeler des procédures stockées et se reposer sur les données du Master Data Management pour lancer des calculs encore plus complexes tout en conservant l’alignement des résultats avec les standards de l’entreprise. L’ETL peut devenir la bête de somme de l’analytique en effectuant des calculs massifs de préparation en avance de phase. La possibilité de programmer ces calculs à des périodes de faible activité des serveurs permet de limiter l’impact sur les sources ;\n\n– dans des petits programmes ad hoc : les Pandas sous Python. Le langage Python s’est progressivement imposé comme le langage agile et accessible dans les communautés de développeurs. Grâce à la bibliothèque Pandas, il permet de développer très rapidement des modules de préparation, de profilage et d’analyse de données, même sur des volumes de plusieurs millions de lignes. Ces morceaux de codes peuvent même parfois être appelés par les processus d’ETL. Leur aspect pratique et agile est séduisant, mais ne doit toutefois pas masquer le besoin d’un minimum de contrôle et de gouvernance de leur développement. Le risque est de voir proliférer de manière anarchique des bouts de programme partout dans l’entreprise, comme l’ont fait depuis des décennies les fichiers et les macros des tableurs.\n\nFigure 12. Forces et faiblesses des processus de calcul et de préparation de données en fonction de leur positionnement dans la chaîne analytique\n\nLes procédures stockées dans les bases de données\n\nDans les bases relationnelles, nous pouvons créer des petits programmes appelés procédures stockées qui accomplissent des suites d’opérations de manière systématique. L’intérêt de ces modules est qu’ils peuvent effectuer des traitements très complexes au niveau du serveur et donc bénéficier d’une puissance de calcul que nous n’aurions jamais sur notre poste utilisateur. En traitant la donnée granulaire dans la base et en n’envoyant que les résultats sur le réseau, ces procédures stockées permettent aussi de limiter la bande passante nécessaire pour les requêtes de données et de garantir plus de confidentialité en ne transportant jamais les détails des calculs.  \nCes procédures peuvent aussi contenir des boucles de programmation ainsi que toutes sortes de commandes récursives, ce qui leur apporte des possibilités que les outils analytiques utilisateurs ont rarement.  \nElles peuvent être enregistrées sous forme de fonctions avec des paramètres (comme sur tableur où des fonctions peuvent appeler des lignes ou des colonnes), qui peuvent être appelées par la plupart des outils de la chaîne analytique, tels que l’ETL ou les outils analytiques. Pour ceux qui douteraient de leur capacité à traiter des problèmes complexes, ces procédures stockées peuvent même réaliser des opérations très complexes de valorisation d’instruments dérivés, sur des matrices d’hypothèses de taux de change et de volatilité.  \nLes bases de données OLAP, multidimensionnelles et en cube peu­vent elles aussi effectuer des calculs qui s’agrègent en respectant les ordres des opérations, afin de conserver des moyennes, des taux, des variations en pourcentage, quel que soit l’axe d’analyse.",
          "Created At": "2025-09-06T13:22:34.085+02:00",
          "Updated At": "2025-09-06T13:22:34.085+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE078_1757157753085",
          "Status": "Ready For Refine",
          "Text": "# Augmenter la valeur des données",
          "Created At": "2025-09-06T13:22:34.086+02:00",
          "Updated At": "2025-09-06T13:22:34.086+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE079_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Gérer les données maîtres : le Master Data Management (MDM)\n\nLes données maîtres (Master Data) sont un élément essentiel de la donnée. Nous les avons déjà évoquées dans les chapitres sur les systèmes qui aident à les gérer ou encore en abordant certaines de leurs formes telles que les hiérarchies ou les clés d’identification. Un peu à la manière d’un Monsieur Jourdain pour la prose, nous utilisons tous le Master Data sans forcément le savoir.\n\n\\> Définition\n\nLes données maîtres sont un ensemble d’éléments de données de base telles que des hiérarchies, des attributs, des propriétés, etc., que nous utilisons pour décrire un client, un produit, une entité juridique, un compte comptable, un employé, un fournisseur, un canal de marché, un emplacement géographique, etc. Ces données ne sont pas des données de transaction.\n\nFigure 13. Exemple d’une transaction et de ses Master Data\n\nSur cette liste d’achats, nous distinguons clairement les données relatives à la transaction :\n\n– la quantité choisie ;\n\n– la date ;\n\n– le montant total de la transaction.\n\nEn plus de ces données transactionnelles, nous retrouvons :\n\n– le nom de l’acheteur ;\n\n– le nom du produit.\n\nCe sont des données maîtres.\n\nAutant les données de transaction changent à chaque transaction, autant les données maîtres demeurent les mêmes. On peut imaginer une adresse de livraison comme une autre donnée maître : cette adresse pourra certes évoluer dans le temps, mais à des rythmes sans comparaison avec ceux des quantités et du prix total sur les factures émises quotidiennement. C’est pour cela que l’on parle aussi de slow moving data : données qui évoluent lentement, quand on parle de Master Data.\n\n\\> Quelques exemples de problèmes avec des données maîtres mal maîtrisées\n\nLes Master Data sont souvent les héros inconnus de l’analytique, d’où l’importance de leur maîtrise. Voici quelques écueils de leur méconnaissance.\n\nLa non-standardisation des hiérarchies et des groupements de données\n\nImaginons que les tables de groupement continent/région/pays/code postal/ville ne soient pas homogènes au sein d’une équipe de vente. Lors d’une réunion de suivi d’objectifs, nous allons rapidement voir que les mêmes régions sont reprises par plusieurs responsables de zones, que des budgets ont été réalisés suivant une répartition géographique différente d’une équipe à une autre. Dans des entreprises qui évoluent rapidement, ce type d’écart est fréquent. Il peut se produire aussi sur des analyses d’employés et leur rattachement à des équipes ou sur des filiales et leur dépendance à un hub.\n\nLa création de doublons pour un même client, fournisseur, produit, etc.\n\nC’est un classique dans des entreprises décentralisées ou lorsque plusieurs équipes ont accès à la création de nouveaux clients/fournisseurs/produits/entités. Pour des raisons de temps ou en raison de la difficulté de vérifier s’ils existent déjà sous une autre variation, nous allons créer une nouvelle donnée maître, sous un nouvel identifiant, alors que celle-ci a déjà été enregistrée. Ce doublon va par la suite empêcher de faire l’agrégation des analyses au niveau de cette donnée maître, puisqu’une partie des transactions seront rattachées à l’une des données maîtres et les autres ventilées sur un ou plusieurs doublons.\n\nLes raisons sociales de sociétés sont souvent cause de confusion. Nous voyons souvent dans les bases de données clients IBM, IBM WW, International Business Machines, International Business Machine (sans le « s »), IBM Inc., comme des variations de la célèbre marque. Comment peut-on avoir une vue agrégée des transactions si elles sont réparties sur autant de noms ? Nous pouvons imaginer la complexité supplémentaire si cette variabilité dans les dénominations se cumule avec celle d’un pays : par exemple, IBM France, IBM FR, IBM SA France, International Business Machine(s) FR, etc.\n\nSans Master Data unifiant ces noms de clients, nous n’aurions jamais la vision totale de l’activité avec le groupe IBM. Certes, nous pouvons faire ce regroupement à la main sur notre tableur ou dans notre solution analytique, mais lorsque ces duplications apparaissent sur des milliers de clients, cela devient ingérable.\n\nL’enregistrement d’un même attribut sous des noms différents dans des systèmes différents\n\nC’est une des plaies pour la vente multicanale. Lorsque notre paire de chaussures, notre boîte de rangement ou notre tee-shirt sont décrits avec une couleur, une taille ou encore des unités différentes quand nous passons du catalogue au magasin et au site en ligne, comment pouvons-nous être sûrs de ce que nous achetons ? Du côté vendeur, comment pouvons-nous faire des analyses sur ces critères variables d’un canal à l’autre ?\n\nSi nous voulons acheter un modèle de lunettes bleues et que celui dont la photo nous plaît est décrit « Bleu Pétrole » sur le site internet, « Oil Blue » sur le catalogue, que le vendeur du magasin nous confirme au téléphone que c’est le modèle « Deep Blue » qui n’est plus en stock, mais que le « Bleu cobalt » est disponible et qu’il semble être d’après lui de la même couleur que l’image en ligne… comment savoir quoi choisir ? Si nous ajoutons à cela les variantes de langues et les variations de photos, nous avons toutes les chances de choisir la mauvaise teinte.\n\nImaginons la complexité de ce problème pour des rouges à lèvres, pour la plupart… rouges.\n\nLe traitement de ce problème de Master Data est malheureusement complexe. C’est peut-être un des points les plus délicats à gérer en data, car il va nécessiter des solutions technologiques parfaitement paramétrées, des processus définis et clairs, et une discipline des équipes, le tout dans un environnement où chaque jour de nouveaux clients, fournisseurs, produits, etc. doivent être créés et ajustés dans de multiples systèmes et entités en plusieurs langues.\n\nL’importance du Master Data Management et cas illustrés  \n\nSalah Kamel est membre du conseil d’administration de Semarchy, la société éditrice du logiciel xDM, leader sur le marché du Master Data Management, qu’il a fondée et dirigée entre 2011 et 2021. Auparavant, S. Kamel était Platform Architect for Enterprise Infor­ma­tion Management and Integration, responsable de la rationali­sation du portefeuille d’intégration de données d’Oracle incluant Oracle Data Integrator (ODI). Il a été aussi Chief Technology Officer et l’un des premiers fondateurs de Sunopsis, un des principaux fournisseurs de logiciels d’intégration acquis par Oracle en 2006. Avant de rejoindre Sunopsis, il gérait les déploiements mondiaux d’entrepôts de données pour les grandes entreprises et était personnellement impliqué dans l’implémentation de très grandes bases de données.  \nPourquoi j’ai choisi de m’attaquer au défi du Master Data Management avec Semarchy : les problèmes observés dans l’entreprise  \n« Master Data Management » est un terme qui regroupe l’ensemble des initiatives menées par de grandes entreprises pour rationaliser leurs données de référence. Ces données souvent dispersées dans plusieurs applications/systèmes représentent le socle fondateur qui permet à l’entreprise de construire son système d’information transactionnel et analytique. Elles regroupent les données servant à décrire les Tiers (personnes physiques, personnes morales, employés, clients, partenaires, etc.), les Choses (articles, produits, services, actif, etc.), et les Lieux (sites, localisations, magasins, restaurants, usines, exploitations, points géographiques, etc.).  \nCes données de référence sont dispersées et répliquées dans plusieurs systèmes opérationnels (ERP, Finance, Achats, CRM, Marketing, Supply Chain, RH, etc.) et elles sont souvent mal gouvernées malgré leur importance vitale pour toute entreprise. En effet, l’ensemble des décisions prises par les grandes entreprises sont basées sur des indicateurs de performance provenant de divers systèmes analytiques tels que les Data Warehouse et les Data Lakes. Or, ces indicateurs sont agrégés à plusieurs niveaux et peuvent produire des résultats erronés si les données auxquelles ils se réfèrent ne sont pas bien définies et gouvernées. Cette gouvernance, appelée Master Data Management, inclut une variété de disciplines, telles que la modélisation et le stockage centralisé de ces données, le développement d’applications d’édition et de modification de ces données, la définition de processus de gestion du cycle de vie de ces données, l’intégration, le nettoyage, le dédoublonnage et la réconciliation de ces données, l’enrichissement avec des données en provenance d’applications externes, et enfin la distribution de ces données aux autres systèmes consommateurs.  \nLorsque j’ai fondé la société Semarchy en 2011, les grandes entreprises déployaient une énergie phénoménale et un budget colossal pour essayer de mettre en place leurs projets de gestion des Master Data. Prises de court par la course à la digitalisation, ces entreprises ont mis en place des solutions logicielles lourdes, inadaptées, onéreuses et souvent focalisées sur un seul vertical. Elles ont sous-estimé l’importance de l’intégration des utilisateurs métier dans la définition de la gouvernance de ces données, et elles se sont retrouvées coincées par les technologies rigides qu’elles ont choisies, inadaptées à l’agilité requise sur un marché en perpétuel mouvement.  \nSemarchy a alors vu le jour en proposant une offre logicielle complète, multidomaine, agile, incluant des algorithmes avancés d’enrichissement, de dédoublonnage, de mise en qualité, le tout facilement accessible au travers d’interfaces utilisateur intuitives.  \nCas réel  \nLorsqu’une caisse de retraite et d’assurance gérant plus de 10 millions de sociétaires (l’Entreprise) recherche de nouveaux leviers de croissance, elle se doit d’intégrer de nouveaux métiers, tels que la banque privée ou la gestion de patrimoine. Ces leviers sont souvent atteints par le biais de croissance externe via des fusions ou des acquisitions.  \nL’Entreprise qui n’avait que trois ou quatre applications métier se retrouve, après la fusion, à devoir intégrer, gérer et gouverner plus d’une dizaine d’applications, chacune contenant ses propres données Tiers (personnes physiques et personnes morales). En les comptant tous, ces Tiers représentaient 55 millions d’enregistrements ! Bien entendu, et bien que non identifiés, ces enregistrements faisaient souvent référence au même Tiers, avec des informations potentiellement divergentes (doublons). Il n’était alors pas rare, suite à la fusion, que des clients appellent cinq services différents de l’Entreprise, pour effectuer un simple changement d’adresse… Inutile de préciser également que les services métier avaient une connaissance parcellaire et disséminée des clients avec lesquels ils étaient censés interagir.  \nL’Entreprise s’est alors lancée dans un grand programme nommé « Connaissance Client » dont les objectifs principaux étaient : 1) d’améliorer l’expérience client pour toutes ses interactions avec l’Entreprise grâce à un identifiant unique quel que soit le point de contact ; 2) de mettre les données clients en conformité réglementaire (RGPD) ; 3) de réduire les risques de fraude en détectant les doublons de données créés volontairement par certains clients ; 4) de fournir aux services métier une vision à 360° de toutes les interactions et des événements générés par les clients pour détecter de nouvelles opportunités d’upsell (vente incitative) et de cross-sell (vente croisée).",
          "Created At": "2025-09-06T13:22:34.087+02:00",
          "Updated At": "2025-09-06T13:22:34.087+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE080_1757157753085",
          "Status": "Ready For Refine",
          "Text": "Avec une très forte implication des services métier et le choix des bonnes technologies agiles de gestion de données de référence, l’Entreprise a pu mettre en place un référentiel unique centralisé de tous ses clients en moins de six mois. Ce référentiel (ou Data Hub), basé sur la solution logicielle de Semarchy, se charge : 1) de collecter en temps réel l’ensemble des données clients situées dans toutes les applications de l’entreprise ; 2) de les enrichir, de les nettoyer et de les valider via des règles métier complexes ; 3) de les rapprocher avec toutes les autres données clients en vue de détecter de potentiels doublons ; 4) de fusionner intelligemment les doublons en gardant les données les plus pertinentes ; 5) d’attribuer des identifiants uniques à chaque client « consolidé » ; 6) de stocker et de distribuer ces données à toutes les applications Front Office ou Back Office qui sont en interaction directe avec les clients.  \nGrâce à la mise en place de sa gouvernance autour du Data Hub, l’Entreprise a pu bénéficier d’une réduction des coûts opérationnels liés à la gestion des données clients de plus de 25 % et d’une croissance estimée des nouvelles opportunités (upsell et cross-sell) de l’ordre de 15 %.  \nL’ensemble des objectifs de l’initiative initiale ont été atteints. Désormais, l’Entreprise dispose d’une vision à 360° de ses clients, quel que soit le service métier qui la demande. Les risques de non-conformité ou encore de fraude ont été largement réduits grâce au processus de dédoublonnage et à la centralisation des données. Enfin, les clients de l’Entreprise ont un accès unique pour leurs interactions, ce qui facilite grandement leur suivi et contribue à leur satisfaction.  \nLes solutions apportées par le Master Data Management  \nLe Master Data Management est une discipline complexe et qui apporte des solutions incontournables pour toute entreprise qui souhaite tirer profit de ses données. Les critères clés du succès d’une telle initiative reposent en grande partie sur la capacité d’évolution des solutions technologiques pour pouvoir absorber, au sein d’un environnement centralisé unique (Data Hub), l’ensemble des données vitales de référence en vue de les gouverner. Voici quelques solutions métier apportées par le Master Data Management (MDM).  \nRéférentiel client unique (B2B ou B2C)  \nUne grande majorité du temps passé par les entreprises est consacrée aux processus commerciaux qui impliquent les clients : les servir, interagir avec eux, leur vendre et leur fournir les biens et les services qu’ils désirent. Le MDM permet d’augmenter les opportunités de ventes incitatives et croisées, d’augmenter la fidélisation des clients avec une meilleure satisfaction, d’accroître la notoriété de la marque, de diminuer l’exposition aux risques réglementaires, de mieux gérer la vie privée, de détecter les fraudes de manière proactive, de diminuer les risques financiers dus à la méconnaissance client et de réduire les coûts opérationnels, de marketing et d’impacts liés à la non-qualité de données.  \nRéférentiel produit unique  \nLes processus commerciaux des entreprises sont également axés sur les produits et les services : les identifier, gérer leurs informations, les acheter ou les vendre, en faire la publicité, et suivre leurs performances et leur cycle de vie. Le MDM va contribuer à une mise sur le marché plus rapide des nouveaux produits et services, augmenter la confiance et la satisfaction, améliorant ainsi la notoriété de la marque, réduire les risques de non-conformité réglementaire (export, consommation, etc.), réduire l’exposition aux atteintes à la propriété intel­lectuelle, assurer une meilleure traçabilité des produits, accroître la précision, l’exhaustivité et la cohérence des données, assurant ainsi une meilleure visibilité pour les consommateurs, optimiser les coûts de la chaîne d’approvisionnement et réduire les coûts d’organisation interne avec une source de vérité unique.  \nRéférentiel organisationnel  \nLes processus de ressources humaines sont fondamentaux pour les entreprises ayant atteint une taille critique : mettre à jour les informations des employés, définir les parcours d’intégration en début ou en  \nfin de contrat, disposer de la structure organisationnelle à jour (sites, adresses, services, unités fonctionnelles, centres de coûts, etc.). Le MDM permet de mieux gérer les talents, d’améliorer les processus de fusion et d’acquisition, de renforcer une meilleure collaboration entre les employés, de réduire les risques de non-conformité sociale (vie privée, assurance, santé, charte, etc.), de réduire les risques juridiques liés aux contrats des employés, de maîtriser les coûts RH et leur budgétisation, et de réduire les coûts et les risques d’embauche, d’intégration et de départ.  \nLe futur du Master Data Management  \nLe Master Data Management est une discipline indispensable pour les grandes entreprises. Il est le socle sur lequel repose une grande partie de la gouvernance des données. Cependant, de nombreuses entreprises n’ont pas réussi à déployer des projets de Master Data Management en raison de leur complexité, de leur coût et de leur risque lié à l’ambition d’avoir un ensemble unique et accepté de définitions et de données partagées dans toute l’entreprise. Ces échecs sont principalement dus à l’échelle gargantuesque des projets, au manque d’outils collaboratifs pour orchestrer la gouvernance, à l’absence de vision d’ensemble permettant de privilégier les « victoires rapides », et au manque de maturité et d’agilité des solutions logicielles permettant d’outiller l’initiative.  \nDepuis peu, les entreprises qui réussissent leur initiative de Master Data Management se focalisent sur une nouvelle approche appelée « Intelligent Data Hub », qui leur permet d’évoluer au fur et à mesure en intégrant de nouvelles briques de Master Data au fil de l’eau, tout en ayant la possibilité de remettre en cause, à tout moment, les choix d’implémentations passées. La différence entre les solutions d’Intelligent Data Hub et de Master Data Management repose sur la richesse des services proposés par ces nouvelles plateformes logicielles. Outre les services de stockage, d’intégration, d’enrichissement, de dédoublonnage, de mise en qualité et de distribution des données, ces nouvelles plateformes incluent des services de catalogage des données existantes, de référentiel de gouvernance pour définir les sémantiques métier, d’outils analytiques, d’algorithmes de parcours de graphes complexes, de connecteurs applicatifs d’intégration et d’algorithmes précâblés d’intelligence artificielle.  \nGrâce à ces nouvelles plateformes, les entreprises gagnent en agilité et sont ainsi capables de faire évoluer leurs besoins au cours du temps à moindre coût. Elles peuvent « commencer petit et voir grand ». En outre, ces nouvelles plateformes sont souvent disponibles dans le Cloud ou en tant que Service (as a Service), facilitant d’autant plus l’adoption par le plus grand nombre.  \nJe pense que l’avenir de la gestion des données de référence sera largement influencé par et reposera sur des architectures plus évolutives, agiles, impliquant directement les utilisateurs métier. C’est en tout cas la promesse de l’Intelligent Data Hub…  \nMes conseils aux data pionniers  \nLes données de référence, et en règle générale toutes les données de l’entreprise, sont comme des affaires que l’on met dans un placard. Si elles ne sont pas rangées correctement, au bon endroit, lavées régulièrement et entretenues, elles sont inutiles. Pire, elles nous coûtent de l’argent. Cette analogie, je la tiens de la directrice générale des systèmes d’information de Chipotle, une grande chaîne de restaurants américains. Durant un repas de négociation commerciale, elle m’a posé une seule question : « Monsieur Kamel, comment rangez-vous vos placards ? » Interloqué, j’ai répondu : « J’ai très peu d’habits, mais ils sont bien rangés. Les chemises d’un côté et les pulls de l’autre… » Ce n’est que plus tard qu’elle m’a expliqué l’analogie.  \nEn tant que data pionnier, vous allez être amené à créer, à modifier ou à utiliser une très grande quantité de données, et peut-être à définir de nouveaux concepts pour offrir un avantage compétitif à votre entreprise. Les règles que j’ai toujours appliquées sont les suivantes :  \n– d’autres savent mieux que moi : quel que soit le domaine fonctionnel dont vous allez manipuler les données, questionnez vos pairs, les référents métier, sur l’origine des données, leur signification, leur cycle de vie (quand sont-elles créées, modifiées, supprimées ?), leurs habilitations (qui y a accès), leur provenance (sont-elles créées manuellement, achetées, importées ?) ;  \n– avec l’analyse, je peux remettre en cause le statu quo : demandez d’avoir les données disponibles et accessibles au plus vite pour pouvoir effectuer des analyses poussées avec des outils de profilage, des tableaux croisés, des graphiques. Défiez les experts métier lorsque vous trouvez des données qui divergent de ce qu’ils vous ont dit. Proposez des solutions ;  \n– trouver LA définition peut être difficile : chaque personne au sein de l’entreprise a sa propre définition et sa propre sémantique des données qu’elle manipule. Prenez le temps de définir de manière non ambiguë les concepts que vous manipulez. Partagez-les avec les experts. Référez-vous au dictionnaire d’entreprise s’il existe. Immergez-vous dans les processus métier qui ont permis de créer ou de modifier les données que vous manipulez ;  \n– après moi, quelqu’un d’autre peut retrouver la donnée : si je la range bien, là où elle est censée être, n’importe qui la retrouvera et pourra donc l’utiliser à bon escient. Ne créez pas des silos de données supplémentaires, cherchez à centraliser pour mieux partager. Et communiquez, documentez, collaborez…\n\nIl faut vraiment contrôler nos Master Data, parce que c’est la seule manière de pouvoir joindre les données de l’entreprise et de bâtir une base de données cohérente. Se passer de Master Data Management, c’est se limiter à l’analyse de silos de données et se priver du potentiel d’analyse holistique que l’IA ou le Machine Learning peuvent réaliser (voir infra, Le Big Data : ses mythes, sa réalité).\n\n\\> Le processus de Master Data Management\n\nLes techniques de gestion varient d’une organisation à l’autre, mais nous pouvons retenir un ensemble d’étapes clés.\n\nIllustration 30. Étapes clés du Master Data Management\n\nIdentifier et extraire les données maîtres dans les différents systèmes transactionnels avec tous leurs attributs\n\n• Enrichir et normaliser les données\n\nLe but est d’avoir dès le départ une homogénéité des attributs et d’augmenter le nombre de points communs potentiels. Pour cette phase, nous pourrons :\n\n– faire appel à des services externes de validation et de normalisation de l’adresse. Par exemple, trouver l’adresse complète « 123 woodland Avenue » deviendrait « 123 Avenue de Woodland, Paris, 75016 » avec ses coordonnées géographiques ;\n\n– appliquer des formats standards à l’e-mail ou au numéro de téléphone. Par exemple, reformater le numéro français 07-08-76-12-34 en +33708761234 ;\n\n– ajuster les caractères spéciaux et les majuscules. Par exemple, changer « Laura BLAKE\\_ » en « Laura Blake » ;\n\n– utiliser certaines inférences pour compléter des champs. Par exem­ple, pouvoir mettre France ou FR pour une adresse clairement identifiée pour le pays.\n\n• Valider les données maîtres",
          "Created At": "2025-09-06T13:22:34.087+02:00",
          "Updated At": "2025-09-06T13:22:34.087+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE081_1757157753085",
          "Status": "Ready For Refine",
          "Text": "Après la phase de nettoyage et d’enrichissement de la donnée, il faut pouvoir écarter les enregistrements foncièrement mauvais. Ce contrôle de qualité est très subjectif. Nous pouvons le limiter à l’exclusion de données contenant des attributs clairement hors norme qui peuvent venir influencer les prochaines étapes du processus ou qui dénotent de la mauvaise qualité de la saisie initiale. Par exemple, renseigner une date de naissance vieille de 220 ans illustre peut-être le manque total d’attention de la personne qui a rentré cette donnée et peut laisser présager des soucis de qualité sur les autres attributs.\n\n• Relier les données maîtres\n\nUne fois que les données maîtres sont enrichies, nettoyées et filtrées, commence leur mise en correspondance. Nous avons vu que les jointures entre deux enregistrements sont simples à cons­truire… lorsque nous avons des clés communes. Or, ces dernières sont plus difficiles à trouver lorsque les données maîtres viennent de systèmes différents. L’exercice est encore plus délicat quand un même système contient des doublons d’une même donnée maître sous des identifiants différents. C’est pour cela que nous allons utiliser une alternative à la jointure (voir supra, La notion de jointure) : la jointure floue, ou fuzzy matching. Celle-ci s’appuie sur des similarités d’attributs pour associer des données maîtres.\n\nLa similarité entre deux champs peut se mesurer entre autres par :\n\n– le même nombre de caractères ;\n\n– le même nombre de changements de caractères qu’il faut faire pour transformer un champ en l’autre : c’est la distance sémantique ;\n\n– leur même sonorité à la lecture : ce sont les algorithmes de type Soundex.\n\nLa jointure se fait en fonction du pourcentage de similarité. Choisir un taux de 100 % de similarité fait retomber dans le cas de la jointure de clé. Le choix d’un taux entre 1 et 99 % dépend du niveau de confiance ou de tolérance vis-à-vis des données et de leur contexte.\n\nNous pourrons panacher l’application de ces taux de similarité entre deux champs en la combinant avec d’autres tests de similarité sur d’autres champs. Par exemple, nous pouvons développer la logique suivante : si le nom est similaire à 90 %, le prénom similaire à 70 % et l’adresse similaire à 80 %, alors nous décrétons que les deux enregistrements correspondent.\n\nC’est dans la définition de ces taux que notre jugement est lourdement sollicité. En fusionnant deux personnes à qui nous allons envoyer un catalogue de vêtements, le risque est d’oublier un client ou d’envoyer deux catalogues au même. En fusionnant deux patients d’hôpital, les conséquences peuvent être dramatiques humainement et/ou légalement. Le choix de ces méthodes d’agrégation n’est pas compliqué techniquement, il est parfois humainement très délicat.\n\n→ Les jeux du matching et du hasard\n\nLa fusion de bases de données de joueurs de casinos avec des données plus larges relatives à leur séjour peut être problématique. Lorsque des hôtels-restaurants-casinos cherchent à mieux connaître leur clientèle pour cibler des actions marketing, ils fusionnent plusieurs bases de données telles que les réservations d’hôtels, les achats boutiques, les consommations spa, etc., y compris certaines sources externes telles que celle des distributeurs de billets situés dans leurs bâtiments. Cette pratique marketing, a priori classique, peut devenir très risquée dans le contexte de la réglementation actuelle aux États-Unis. En effet, la loi assure à tout individu qui le désire le droit d’être tenu à l’écart des casinos et de leurs sollicitations commerciales à des fins de protection contre les addictions aux jeux. Tout casino doit connaître parfaitement cette liste noire des personnes à ne pas contacter. Mais que se passe-t-il si Paul Dupont, interdit de jeux, est fusionné avec Paul Dupond, un gros joueur, et qu’il commence à recevoir des sollicitations commerciales à son adresse physique ou électronique ? Pour cette simple erreur de « surjointure » (overmatching) le casino risque des millions de dollars d’amende, voire sa fermeture.\n\n• Sélectionner les champs à retenir\n\nUne pratique tenace pousse souvent à choisir un enregistrement parmi ceux retenus à l’issue de cette phase de matching. Nous pouvons être un peu plus subtils dans cette approche en sélectionnant non pas l’enregistrement, mais les attributs gagnants, un par un, en fonction de la fiabilité estimée de leur origine.\n\nSi, par exemple, dans les données maîtres connectées se trouvent plusieurs numéros de téléphone différents, nous allons retenir celui qui vient du call center. Si nous avons plusieurs adresses physiques, celle des systèmes logistique et livraisons a le plus de chance d’être la bonne. L’e-mail aura plus de chance d’être valide, s’il vient des services e-marketing.\n\n• Construire le Golden Record, l’enregistrement maître\n\nÀ ce stade, nous allons enfin pouvoir physiquement ou virtuellement assembler notre donnée maître en une seule référence, issue du meilleur de tous les enregistrements lui correspondant. Nous allons lui adjoindre LA clé unique qui sera reliée à toutes les clés des enregistrements fusionnés. Nous aurons, à ce stade, une super table de correspondance pour joindre ces données dans nos analyses !\n\nQue se passe-t-il si l’on corrige les bases transactionnelles ?\n\nLa tentation existe de réécrire ces données fusionnées dans les bases transactionnelles dont elles sont issues. En effet, si nous corrigeons le problème à la base, les problèmes de Master Data décrits plus haut devraient progressivement disparaître. Malheureusement, cette option n’est pas forcément aussi triviale qu’elle en a l’air. Écrire dans les systèmes transactionnels n’est tout d’abord pas une tâche légère, car ces systèmes doivent conserver la plus haute intégrité. Ensuite, que se passe-t-il pour des transactions en cours sur un client ou un fournisseur fusionné ? Le début commence sur une entité et finit sur… une autre ? Les contraintes varient largement d’une entreprise à l’autre : il faut simplement retenir que ces mises à jour ne sont pas évidentes.",
          "Created At": "2025-09-06T13:22:34.088+02:00",
          "Updated At": "2025-09-06T13:22:34.088+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE082_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Les subtilités et les risques du Master Data Management\n\nUne des raisons pour laquelle les projets MDM sont délicats est qu’outre leur complexité intrinsèque ils présentent des subtilités et des risques éthiques. Un ouvrage entier pourrait être consacré à ces sujets : voici une sélection de points de réflexion pour attiser votre curiosité.\n\n\\> Pareil ne veut pas dire le même\n\nFaire le matching entre deux individus peut devenir vraiment complexe, lorsque nous prenons en compte le contexte géographique.\n\n→ Questions de correspondance\n\nQue vaut la correspondance à 100 % d’un Jean Durand avec un autre Jean Durand à Paris ? Même si elle est parfaite d’un point de vue de la chaîne de caractères, elle risque d’aboutir à une « surjointure » (overmatching) tant le nom est courant en région parisienne. Même en ajoutant la rue, il pourrait y avoir un risque d’avoir une jointure qui marche entre deux individus différents.\n\nPrenons maintenant le nom Noémie De Ficci Lucas à Bennet, Vendée. Noé De Fichi dans la même ville a de fortes chances d’être cette même personne, tant le nom est particulier. Pourtant, la différence est grande entre les deux chaînes de caractères.\n\n\\> Peu d’attributs ne veut pas dire anonymat\n\nEu égard à la vie privée, nous avons parfois la conviction que ne travailler qu’avec un nombre limité de données est un gage de respect de l’anonymat. C’est sans compter sur le pouvoir des jointures et des inférences.\n\nUne étude du professeur Sweeney, de l’université de Carnegie Mellon (voir infra, L’anonymat à l’épreuve du MDM), a mis en évidence qu’il suffit de connaître le sexe, l’âge et l’ethnicité d’une personne pour que, dans certaines communautés, l’identification de la personne soit possible.\n\nImaginons que, grâce à nos capacités de Master Data Mana­gement, nous soyons maintenant capables de croiser suffisamment de données pour constater que, dans tel quartier de telle ville, une femme de tel âge, avec telle origine, ne peut être par élimination qu’une seule personne. Dans le même temps, si, avec un peu de recherche légale (ou non), nous accédons à des données médicales, religieuses ou politiques anonymisées détaillées par sexe, âge et adresse des individus, nous entrevoyons avec quelle facilité nous pouvons maintenant croiser ces données et commencer à bâtir des profils détaillés sans le consentement des personnes.\n\n→ L’anonymat à l’épreuve du MDM21\n\nConsidérons ce tableau. Si les trois enregistrements présentés faisaient partie d’une vaste base de données sur les résidents de l’Illinois, il pourrait sembler raisonnable de supposer que ces trois enregistrements soient anonymes. Cependant, le recensement fédéral de 1990 rapporte que le ZIP (code postal) 60602 consistait principalement en une communauté de retraités dans le Near West Side de Chicago et, par conséquent, très peu de personnes (moins de 12 individus de moins de 65 ans) y vivent. Le code postal 60140 est le code postal de Hampshire, en Illinois, dans le comté de Dekalb. Et deux femmes noires résidaient dans cette ville. De même, 62052 ne comptait que quatre familles asiatiques.\n\nDans chacun de ces cas, l’unicité des combinaisons de caractéristiques trouvées pourrait aider à réidentifier ces individus.\n\nLa loi protège en théorie de certaines de ces dérives, mais elle ne peut pas tout envisager. Conserver une pratique des Master Data éthique est de notre responsabilité.\n\n\\> Processus simple ne veut pas dire évident\n\nUn processus de MDM peut être détaillé simplement, comme nous venons de le faire plus haut. Il est néanmoins riche en subtilités non triviales. Que se passe-t-il par exemple quand nous le prenons à l’envers ? Comment pouvons-nous disjoindre des enregistrements précédemment « matchés » par erreur ? Comment recycler ces données redevenues orphelines pour qu’elles fassent à nouveau l’objet d’une tentative de matching avec les autres données ? Lorsque les conditions de matching sont inadaptées, comment mettre à jour une condition de jointure et réestimer tous les matchs réalisés jusqu’à présent ?\n\nLe traitement de ces points peut rapidement devenir complexe et venir à bout de nombreuses velléités de tentatives à la main ou sur tableur. Le MDM fait partie de ces disciplines en analytique où la technologie joue un rôle incontournable pour la traçabilité des opérations.",
          "Created At": "2025-09-06T13:22:34.089+02:00",
          "Updated At": "2025-09-06T13:22:34.089+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE083_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Établir une qualité holistique de la donnée\n\nNombre de techniques évoquées jusqu’à présent contribuent à la qualité de la donnée, si critique pour des analyses fiables et durables. Ces dernières peuvent être regroupées dans des outils de gestion dédiés qui centralisent les phases clés de profilage et de correction : les solutions de Data Quality. Réaliser le rêve d’une qualité de données parfaite requiert toutefois une approche vraiment holistique du problème. C’est ce qui rend la discipline particulièrement complexe mais également passionnante.\n\n\\> Jouer en mode Street Fighter Style\n\nNous avons tous joué ou vu jouer des gens à ces jeux vidéo de combat un contre un, inspirés des mangas japonais. L’ob­jectif est simple : infliger à l’adversaire des coups qui finiront par le terrasser, avec en plus la possibilité d’engager des frappes exceptionnellement déclenchées lors de certaines séquences de boutons frappées frénétiquement par les doigts. Ces jeux sont accessibles à tous et souvent jubilatoires, car, en tapant comme des fous sur les boutons de la manette, on arrive souvent à des coups magistraux que les graphistes et les animateurs ont l’art et la manière de scénariser.\n\nQuel est le rapport avec la gestion de la qualité de donnée ? À l’exception de la frénésie du jeu (quoique), la pratique holistique de la qualité de la donnée se gagne par l’activation régulière et récurrente de plusieurs leviers.\n\nLes leviers préventifs\n\n• La sensibilisation et la formation continue des personnels à l’importance de la qualité de donnée et sur le fait qu’une donnée de mauvaise qualité va gêner toute une chaîne de traitement, dont ils font partie, car eux aussi vont devoir s’appuyer sur de l’information propre.\n\n• Le bridage des saisies des données dans les systèmes en forçant l’application des formats standards ou par la recherche en temps réel de suggestions de saisie déjà existantes. Les grilles d’entrée de donnée peuvent forcer l’entrée de chiffres uniquement, ou exclure certains caractères. En tapant le début du nom d’un client, le système pourra aussi aller chercher les correspondances potentielles dans le Master Data, ou faire appel à des bases des données extérieures pour suggérer des options.\n\n• Le retour rapide sur les problèmes identifiés. Si nous n’avons pas connaissance de nos erreurs, nous ne pourrons pas les corriger et encore moins les éviter dans le futur. Des rapports de fin de journée dressant un bilan de ce que nous avons saisi, à la manière des rapports de profilage (voir supra, Jauger la qualité de données initiale : profiler la donnée) vont permettre de nous autocontrôler.\n\nLes leviers réactifs\n\n• Le profilage des données reçues dans le décisionnel. L’analyse régulière des données nouvellement saisies permet d’identifier rapidement les problèmes et de les corriger avant que les erreurs ne viennent polluer d’autres processus.\n\n• La correction à la source. Avec une chaîne de contrôle courte, il est facile de retrouver les personnes ayant saisi les données erronées et d’effectuer les correctifs nécessaires dans les systèmes d’origine avant que, par exemple, une transaction soit complètement validée. C’est la meilleure option.\n\n• La correction a posteriori. Il est parfois délicat, coûteux, voire impossible de changer des données déjà impliquées dans des transactions. Afin d’éviter qu’elles ne prolifèrent dans le décisionnel, nous pouvons choisir de les ajuster à l’entrée dans les bases de données. Cela créera un écart avec les bases transactionnelles qu’il faudra documenter, mais économisera le temps précieux des analystes qui n’auront plus ce retraitement à faire manuellement. Il suffira alors de conserver une table d’inventaire de ces corrections que les analystes et les auditeurs apprécieront.\n\nLes leviers de gouvernance\n\n• Une prise de position forte exprimée par le management. Sans cette pression, nous nous concentrerons sur des tâches à meilleur retour sur investissement politique ou opérationnel plutôt que sur la correction de données.\n\n• Un cadre de gouvernance qui définit les actions standards à suivre en termes de saisie et de correction ainsi que les normes à respecter en data.\n\n\\> Se concentrer sur ses besoins\n\nLa quête de la donnée 100 % parfaite est vaine. L’arrivée en continu de nouvelles données ainsi que les erreurs humaines inévitables rendent l’exercice futile. En nous concentrant sur les données qui comptent vraiment pour la gestion de l’entreprise, nous pouvons considérablement réduire le volume à contrôler et à traiter, et augmenter nos chances d’avoir une data de qualité sur les domaines pertinents de nos analyses.\n\nL’imposition de règles de qualité strictes sur des données clés est plus facile à comprendre par les opérateurs et sera mieux acceptée que si elle est présentée comme « faisons de la qualité de la donnée une priorité ». Elle est également plus facile à superviser et à corriger.\n\nPar conséquent, avant de remuer ciel et terre, considérons nos enjeux métier et concentrons nos efforts sur les données nécessaires à leur traitement.\n\n→ Le tonneau de Danaïdes des données clients\n\nDans un espoir louable d’obtenir la meilleure donnée client possible, une équipe Master Data s’était formée dans la direction commerciale d’un grand groupe automobile. Ce projet pharaonique qui devait engager l’achat de logiciels, la redéfinition de processus et un effort général sur plusieurs mois fut arrêté net par le constat du responsable de la division. Que s’est-il passé ?\n\nLa base clients n’avait que 63 % des adresses postales, 34 % des adresses e-mail et 20 % des numéros de téléphone. Avant de courir après la déduplication, l’enrichissement et la correction des autres champs, ne valait-il pas mieux s’attaquer à ces trois éléments en priorité ?\n\nL’essence du marketing et de la vente n’est-il pas avant tout de pouvoir contacter le client ?\n\n\\> Ne jamais s’arrêter\n\nUn autre défaut des grands projets de gouvernance et de qualité de la donnée est que leur taille et leur complexité les rendent très vulnérables aux changements de priorité de l’entreprise. Lorsque la conjoncture change, que les objectifs commerciaux sont plus délicats à atteindre, l’attention et les efforts se détournent vite des activités de gouvernance qui semblent bien éloignées des besoins concrets et immédiats. La mise en place d’une politique de qualité de donnée est nécessaire pour coordonner les efforts, mais, dans sa construction, nous devons tenir compte de sa durabilité dans le temps.\n\n\\> Se souvenir que c’est la bonne décision qui compte\n\nQuand la donnée est-elle suffisamment bonne pour prendre une décision ? Quand nous nous éloignons du transactionnel, quand la tolérance à l’erreur augmente. Le point n’est pas de laisser des erreurs ou des inexactitudes proliférer dans nos analyses, mais plutôt de savoir quel est le degré de précision nécessaire.\n\nPlus que l’erreur d’arrondi ou le manque de certaines données, ce sont les biais d’analyse ou l’exhaustivité de la collecte qui doivent nous inquiéter. Avons-nous toute la data nécessaire pour une analyse du problème et nous sommes-nous posé la bonne question ? Mis à part les cas de mauvaise qualité majeurs, il y a de fortes chances pour que notre donnée soit suffisante pour permettre des décisions rationnelles. Comme beaucoup d’éléments en analytique, ce sont notre expérience et notre pragmatisme qui seront déterminants.",
          "Created At": "2025-09-06T13:22:34.089+02:00",
          "Updated At": "2025-09-06T13:22:34.089+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE084_1757157753085",
          "Status": "Ready For Refine",
          "Text": "# Le Big Data : ses mythes, sa réalité\n\nLa data doit beaucoup au Big Data. Sans cette vague, elle serait certainement restée le domaine peu attractif de techniciens. Nous voulions tous être « génération data », mais sans mettre les mains dans une matière jugée complexe.\n\nL’arrivée de sociétés innovantes et fun telles que Yahoo, Google et le chapelet de start-up du début des années 2000, dont la raison d’être était la data, sa collecte, son analyse et sa revente, ont mis sur le devant de la scène une data tout à coup utile, sexy, pionnière, libérée.\n\nIl recommence un projet Big Data !\n\nLa gestion de la data et son application à la finance, à l’audit ou au marketing, vues alors comme une déviance froide et informatique, étaient en passe de devenir une discipline faisant partie intégrante de tout métier. Enfin, la data pouvait s’écrire avec un grand « D ». En parallèle, néanmoins, ce tournant salutaire pour la démocratisation de la data a eu pour effet pernicieux de faire oublier les fondamentaux nécessaires à de bonnes analyses : une data de qualité, bien préparée, répondant à des questions métier. Le Big Data a mis au second plan le pourquoi pour se concentrer sur le comment : nous devions tous avoir un projet Big Data. Mais cet engouement était la parfaite excuse pour se mettre à la data sans risque d’y perdre la noblesse de son métier (même les dirigeants, les grands stratèges n’avaient que ce mot à la bouche) et sans avoir à se poser les questions délicates autour de tout projet data. Faire du Big Data suffisait largement à convaincre ou à impressionner un comité de direction, ses employés, le grand public ou ses amis.",
          "Created At": "2025-09-06T13:22:34.090+02:00",
          "Updated At": "2025-09-06T13:22:34.090+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE085_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Comment définir le Big Data ?\n\nLa définition des analystes a rapidement gravité autour des trois « V », un concept simple à retenir, qui sonnait bien et qui, somme toute, résumait bien la situation. Les trois V représentaient les trois caractéristiques de la data :\n\n– volume ;\n\n– vélocité ;\n\n– variété.\n\nRapidement, le Big Data étant de toutes les conversations et de toutes les conférences, d’autres V ont été ajoutés, tels que véracité ou valeur. Ces deux notions n’avaient rien de neuf pour les praticiens de la donnée : elles n’avaient juste jamais fait l’objet d’une attention particulière jusqu’à présent.\n\n» Vous aussi, ajoutez votre V au Big Data !\n\nReprenons chacun de ces trois V pour mieux comprendre le sujet.\n\n\\> Pourquoi volume et vélocité ont-ils été si moteurs au départ ?\n\nLe Big Data a commencé par l’explosion du volume des données pouvant être traitées. Les données avaient toujours été là depuis la nuit des temps, mais, en l’espace de quelques années, leur émission, leur capture et leur stockage numériques sont devenus possibles. Une gigantesque quantité de données devenait soudain accessible à l’analyse à des vitesses sans précédent.\n\nLa numérisation massive des données s’est faite grâce au développement des capteurs, de l’Internet des objets (IoT, Internet of things), des ordinateurs personnels, des téléphones portables, des appareils photo numériques et des réseaux sociaux. Avant ce tournant, la majorité des données utilisables étaient celles de l’ERP (Enterprise Ressource Planning) de l’entreprise ou de quelques services d’État, de sondage ou de marketing.\n\nEn l’espace de quelques années, machines et humains ont commencé à générer en continu des données digitales. Des progrès technologiques ont aussi propulsé la croissance exponentielle de cette révolution :\n\n– la densité du nombre de transistors, doublant tous les dix-huit mois, permet des calculs toujours plus complexes et massifs. Cette loi, appelée loi de Moore, continue à être valide ;\n\n– l’effondrement du coût du Mo de stockage permet de tout conserver ;\n\n– la bande passante des réseaux autorise des échanges d’informations volumineuses.\n\nÉchange, stockage, traitement : la technologie créait la « tempête parfaite » pour l’avènement des volumes et de la vélocité du Big Data.\n\nCette révolution a déclenché la création de nouvelles organi­sations dont le métier est la revente d’analyses ou de données : Google, Yahoo, Facebook, Twitter, etc. ont perçu l’analyse comme une nouvelle ressource de profits.\n\nLes entreprises des secteurs plus traditionnels, tels que la banque, l’assurance, le commerce, l’automobile, les biens de consommations, etc., faisaient déjà de l’analyse massive de données dans certains de leurs départements comme le marketing quantitatif, l’analyse de risque, le calcul de primes, le relevé de capteurs, etc. Cette vague leur a donné accès à plus d’informations, à plus de capacités de stockage et à un traitement à bas coût, créant une incitation pressante à appliquer le Big Data à tous les étages.\n\n\\> A-t-on vraiment besoin de volume pour être pertinent ?\n\nCe volume de données était-il devenu nécessaire à la prise de décisions, même stratégiques, des entreprises en dehors du secteur du numérique ? Considérer qu’il fallait désormais du Big Data pour répondre correctement aux problèmes s’est souvent révélé une décision coûteuse et peu pertinente.\n\n→ Capteurs sur des voiliers de course\n\nDonald analyse les données fournies par des capteurs sur des voiliers de course ultraperformants. Il a à sa disposition un flux massif de données constamment enregistrées sur tous les points sensibles du bateau. Il considère qu’il ne fait pas du Big Data, juste de la data. Ses arguments sont les suivants :\n\n– j’ai effectivement à ma disposition un gros volume de données stockées. La taille de ces jeux de données est toutefois relative. On est loin des données d’astrophysique ou de télémétrie de fusée ;\n\n– lorsque j’analyse un problème, ce ne sont pas des heures de données de navigation qui vont être pertinentes, mais les dizaines de minutes pendant lesquelles le bateau est dans la situation critique qui m’intéressent ;\n\n– ensuite, de manière générale, je porte mon analyse sur un point précis. Je n’analyse pas « le bateau », mais une pièce ou une zone. Je vais donc juste me concentrer sur quelques capteurs ;\n\n– quand je commence une analyse, j’ai rarement du Big Data. Le Big Data, c’est quand les machines commencent à souffrir. Mon ordinateur portable peut traiter des dizaines de millions de lignes sans difficulté. Cela me laisse beaucoup de marge pour commencer à dire que je fais du Big Data.\n\nLe concept de projet Big Data devrait plus souvent faire place à la résolution de big problems nécessitant, ou non, de gros volumes de données. Commencer par faire du Big Data, c’est mettre la charrue avant les bœufs.\n\nÀ partir de quel volume commence le Big Data ?\n\nSi nous nous sentons mal à l’aise avec la notion de Big (Data) et que nous nous demandons si nous devons passer du côté des technologies avancées dédiées à leur traitement, ramenons nos volumes de données à certaines références telles que le volume de données prévu par le réseau d’antennes de radiotélescopes en Australie : 35 000 DVD par seconde, soit 157 térabits par seconde.  \n\nEnfin, pour les statisticiens, plus de volume ne veut pas dire plus de pertinence statistique. En étendant trop largement les périodes temporelles de jeux de données, nous introduisons plus de bruits, plus d’éléments anciens qui polluent l’analyse de la situation actuelle.\n\nFinalement, si nous nous appuyons sur une question métier précise, que nous préparons notre donnée pour la mettre à la bonne échelle et sur le bon périmètre, il y a de fortes chances que nos jeux de données ne dépassent pas des volumes raisonnables pour des équipements conventionnels. Un domaine de l’analytique recourt néanmoins à des volumes massifs d’informations, bien souvent non structurées comme des images ou de la voix : le Machine Learning, qui va s’appuyer sur le plus de cas d’apprentissage possible pour définir des règles de comportement décisionnelles (voir infra, Les domaines du Big Data).\n\n\\> L’obsession illusoire de la vitesse\n\nAvec les progrès de la technologie et la génération de données digitales en continu par les humains et les machines, l’analyse en temps réel de situation est devenue techniquement possible. Jusque-là concentrée à juste titre sur des domaines transactionnels tels que le suivi de production, la surveillance, la télémétrie de machines ou de véhicules, par exemple, elle s’étend désormais au monde du décisionnel.\n\nDes équipes décisionnelles se sont équipées à grands frais de technologies qui donnent en temps réel des indicateurs de gestion. Les motivations allaient de « plus je colle à mon métier, mieux je le gère » à « les concurrents voisins gèrent en temps réel, il faut donc que nous aussi », ou bien encore « à la dernière conférence, ils ont dit que c’est l’avenir ». Sans remettre en cause le besoin de chacun, qui peut être parfaitement justifié, ainsi que tous les cas d’analyse qui se rapprochent de besoins transactionnels tels que la détection de la fraude, la publicité en ligne ou la mesure de qualité d’une production ou d’un service, nous pouvons nous poser la question de l’utilité du temps réel en gestion.\n\nIl y a toujours des décisions simples qui peuvent être prises de manière quasi automatique et rapide. Mais, de plus en plus aujour­d’hui, l’avantage concurrentiel provient de décisions complexes, fondées sur des revues holistiques. Choisir les meilleures options avec finesse ne peut plus dépendre d’une seule personne (le manager omniscient et omnipotent) et doit plus souvent se reposer sur un travail d’équipe concerté et réfléchi.\n\nOn pourrait également arguer que, si des décisions peuvent et doivent être prises en temps réel, alors, ce n’est peut-être pas le rôle d’un humain : la machine pourra agir non seulement plus vite mais aussi 24 heures sur 24. L’humain conservera une supervision, rythmée par les alertes de la machine, et pourra se consacrer à des tâches plus valorisantes que scruter un écran en temps réel.\n\n\\> Pourquoi cette ruée vers les volumes et la vélocité ?\n\nLes progrès techniques ont donc permis l’utilisation de grands volumes de données à haute vitesse, mais ce n’est pas parce que nous pouvons le faire que nous devons le faire. L’éternelle course au « toujours plus » entre praticiens mise à part, ce mouvement a été largement entretenu parce que volume et vélocité pouvaient être « vendus ». Ce besoin de Big Data, décrété à l’unisson par des éditeurs de logiciels, des fabricants de machines, des consultants, des start-up en intelligence artificielle ou en Machine Learning, la presse et la communauté des conférenciers ont poussé à de nouveaux investissements tous azimuts. En fin de compte, la plupart des problèmes métier n’avaient pas vraiment changé, mais il fallait tout à coup du Big Data pour les résoudre.\n\nLa majorité des défis d’analyse posés par des problèmes complexes n’avaient pas besoin de beaucoup plus d’artillerie lourde. Ce qui avait manqué jusqu’à présent était :\n\n– l’engagement des équipes dans la pratique de l’analytique, par absence d’éducation, de volonté ou de support de leur direction ;\n\n– la capacité du métier à connecter l’ensemble des données nécessaires à la résolution d’un problème du fait de leur nature et de leur variété ;\n\n– la nécessité d’employer de nouvelles méthodes peu maîtrisées par les opérateurs métier pour détecter des signaux dans des situations de plus en plus complexes.\n\n\\> La variété, le Graal du Big Data\n\nIl y a vingt ans, lors de la démocratisation de la Business Intelligence, les analyses qui permettaient d’avoir un avantage concurrentiel pouvaient se limiter à une vue de nos ventes, achats, production, etc., par client, fournisseur, pays, région, produit, année, budget, réel, devise, etc. Humainement, nos yeux, nos doigts (sur le clavier ou la souris), notre cerveau et d’une manière générale nos sens pouvaient appréhender ces données sous forme de quelques tableaux ou graphiques et détecter les points à traiter. Nous pouvions même arguer que notre expérience et notre œil aiguisé n’avaient pas besoin de tout cela pour connaître une situation.\n\nAujourd’hui, l’abord des problèmes avec ces jeux de données limités n’apporte plus vraiment davantage. Ces rapports doivent maintenant tenir compte de la diversité des facteurs nécessaire pour appréhender une situation dans sa globalité et surtout considérer le monde dans toute sa complexité et ses évolutions rapides. C’est dans ces contextes que les approches d’intelligence artificielle ont pris tout leur sens : nous avons besoin de suppléer nos limitations cognitives et cérébrales avec des outils qui peuvent traiter des situations beaucoup trop complexes pour un humain.\n\nDonnées d’hier et d’aujourd’hui\n\n  \nDans cette représentation de ce que pourrait être la donnée d’hier (grisée) comparée à celle que nous pouvons récupérer aujourd’hui pour une même problématique, il est clair que nous pouvons nous contenter de nos sens pour comprendre une logique produit/marché/client/fournisseur, mais que l’ajout des données des réseaux sociaux, du commerce en ligne, des capteurs sur les produits, sur les chaînes de production, de voix du centre d’appels ou de photos rend ce paysage incompréhensible sans l’aide d’outils d’analyse.  \nNous devons considérer l’Intelligence non pas comme artificielle, mais augmentée. Ses algorithmes vont nous donner des superpouvoirs de vision dans des enchevêtrements a priori inextricables de data !\n\nIA + personnes = intelligence augmentée",
          "Created At": "2025-09-06T13:22:34.091+02:00",
          "Updated At": "2025-09-06T13:22:34.091+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE086_1757157753085",
          "Status": "Ready For Refine",
          "Text": "Spécialiste de longue date de l’intelligence artificielle, Gini Rometty, qui a dirigé l’équipe qui a préparé Watson, une IA, chez IBM, voit l’avenir du travail dépendre de la capacité des machines à travailler aux côtés des humains. « Nous résoudrons tant de grands problèmes, mais ce sera un monde d’hommes et de machines », dit-elle. « Ils devront donc travailler ensemble, et cela va signifier beaucoup de changements. »\n\nAvec cette variété croissante des sources et des types de données, les défis de leur jointure ou de la maîtrise de leur qualité se multiplient. L’importance des clés et des Master Data unifiées s’est renforcée : il ne suffit pas de déverser des tonnes de données dans une grande base pour qu’elles s’organisent, s’harmonisent et se connectent par magie. Si chaque émetteur de données doit en augmenter sa maîtrise, les équipes devront aussi bâtir des processus collaboratifs performants, sans lesquels ces visions holistiques de problèmes ne pourront pas être traitées… de façon holistique.",
          "Created At": "2025-09-06T13:22:34.091+02:00",
          "Updated At": "2025-09-06T13:22:34.091+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE087_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Les domaines du Big Data\n\nLe terme Big Data regroupe en sept lettres une multitude de domaines. Les querelles de jargon créent souvent des écrans de fumée qui nous empêchent de les comprendre. Dans un contexte professionnel, sans être statisticien, mathématicien ou analyste, nous pouvons appréhender ces techniques (voir chapitres 2 à 5). En effet, quel que soit le niveau de ces disciplines, nous devrons toujours retrouver :\n\n– la question métier qui doit sous-tendre tout projet analytique ;\n\n– la logique des flux et des briques technologiques qui les supportent : la data sera toujours extraite, préparée, stockée, traitée et représentée en vue d’une action ;\n\n– les éléments de structure de la donnée : les clés, les attributs, les métriques et les données calculées ;\n\n– l’impératif de la qualité de données et des données maîtres.\n\nAvec ces éléments en tête, passons en revue les termes qui se sont imposés en Big Data.\n\n\\> Qu’est-ce que l’intelligence artificielle ?\n\nLa définition exacte de l’IA fait l’objet d’un débat permanent. Selon la personne à qui vous posez la question, data scientists, data engineers, chercheurs, etc., la définition que vous obtenez peut varier.\n\nD’une manière générale, l’intelligence artificielle fait référence à la capacité d’une machine à faire preuve de toute forme d’intelligence afin d’accomplir une tâche donnée. Très souvent, surtout dans le monde des données, nous associons l’IA à l’apprentissage automatique (Machine Learning) et à l’apprentissage profond dans le cadre de l’analyse prédictive.\n\n\\> Qu’est-ce que le Machine Learning ?\n\nMachine Learning fait référence à un groupe de techniques utilisées par les data scientists et les data analystes qui permettent aux ordinateurs d’apprendre à partir des données. Le Machine Learning utilise des principes statistiques pour créer des modèles capables d’effectuer des tâches de prédiction, telles que la classification, la régression, ou de groupement, telles que le clustering.\n\n\\> Qu’est-ce que le Deep Learning ?\n\nDeep Learning peut être considéré comme une forme spécialisée d’apprentissage automatique, par laquelle un modèle est cons­truit pour simuler le comportement du cerveau humain : les réseaux neurones. Il est utilisé pour effectuer des tâches prédictives telles que la classification et la régression, mais, en raison de sa structure complexe, ses applications sont beaucoup plus profondes que celles d’un simple modèle d’apprentissage automatique.\n\nUn exemple d’application du Deep Learning est le développement de voitures sans conducteur, dans lesquelles le modèle doit être capable de détecter des objets tels que les arbres, les feux de signalisation et les limites de la route.\n\nL’IA par ceux qui la pratiquent  \n\nJ’ai demandé à trois experts leur définition de l’IA. Leurs définitions sont assez alignées et très humaines.  \nSarah Aerni, directrice IA et Machine Learning chez Salesforce : « Avec l’intelligence artificielle, il s’agit vraiment de faire en sorte que les machines pensent comme les humains, en leur apprenant à nous faciliter les choses. Lorsqu’elles automatisent des choses très fastidieuses, elles permettent aux humains d’atteindre leur niveau supérieur dans ce qu’ils font. Puis elles leur permettent d’en faire plus, de rendre tout plus intelligent, d’améliorer leur expérience. Dans tous les contextes où nous avons accès aux données, nous pouvons augmenter notre compréhension et notre expérience, et permettre aux humains qui font leur travail au jour le jour de devenir des super humains. »  \nRobert Brown, Vice-Président du Centre pour le futur du travail, chez Cognizant : « Il y a toujours l’idée de la super intelligence, c’est-à-dire l’IA Terminator qui se déchaîne, dans l’imagination populaire. C’est vraiment l’intrigue d’Hollywood. Je pense que là où l’action se situe pour la plupart des entreprises et pour les gens, c’est l’apprentissage automatique. Il est appliqué à de nombreux problèmes pour l’humanité, pour les entreprises et pour chacun d’entre nous individuellement. Et cela offre des possibilités d’atteindre l’excellence dans tout ce que nous faisons. »  \nDan Feld, Director Enterprise Business Devices & Services chez Google : « L’IA, pour moi, est une opportunité commerciale de se développer, et c’est même une opportunité personnelle pour nous tous. Si vous regardez tout ce que nous avons fait jusqu’à présent avec des bases de données et des systèmes basés sur des données dans les entreprises qui étaient des fragments de systèmes et des fragments de valeurs, nous réussissons à donner un sens à tout cela au prix d’un temps précieux, pris aux dépens de tâches à valeur ajoutée. L’IA nous donne l’opportunité de tout mettre à l’échelle, de relier les points et d’avoir un processus beaucoup plus cohérent et rapide pour que nous puissions nous concentrer sur ce qui compte. »\n\nCes techniques, aussi avancées soient-elles, reposent sur les connaissances opérationnelles des équipes de terrain qui auront elles-mêmes approfondi leurs connaissances par leur analytique conventionnelle. Les considérer comme des baguettes magiques, actionnées dans des bureaux clos, en se passant de la réalité de terrain, est un mythe. Si les data scientists se rapprochent progressivement des métiers, ces derniers doivent également appren­dre quelques fondamentaux pour faciliter les échanges et la compréhension mutuelle. C’est que nous allons maintenant aider à faire.",
          "Created At": "2025-09-06T13:22:34.092+02:00",
          "Updated At": "2025-09-06T13:22:34.092+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE088_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Trois grandes approches algorithmiques à connaître pour mieux travailler avec les data scientists\n\nLes mathématiques et les statistiques apportent une aide précieuse lorsque nos sens et nos analyses classiques ne peuvent plus cerner une situation. Nous avons à notre disposition trois classes d’algorithmes qui se rapprochent fortement de nos pratiques intuitives ou manuelles.\n\n\\> Les régressions\n\nLes régressions ont un objectif simple : trouver la courbe qui correspond le mieux à un ensemble de mesures, dans le but de prédire leur valeur.\n\nL’algorithme apprend d’un échantillon de données x donnant chacune un résultat y, et propose une fonction mathématique pour ajuster une courbe qui passe au plus près de tous les points. Cette courbe peut être une droite – on parle alors de régression linéaire –, ou bien une courbe, exprimée par un polynôme – on parle alors de régression polynomiale. Une fois l’équation de cette courbe établie, nous pourrons ensuite calculer, pour tout x, une prédiction lui correspondant avec, pour la régression linéaire, une équation de type y = ax + b, et, pour une régression polynomiale, une équation de type y = anxn + … + a2x2 + ax + c.\n\nIllustration 31. Régression linéaire à gauche et régression polynomiale à droite\n\nNotre expérience, notre œil ou un simple graphe nous permettent d’appréhender intuitivement ces corrélations. Les mathématiques nous aident à valider et à affiner cette perception.\n\nL’exemple de la corrélation de la pointure de pieds avec la taille d’un homme ou d’une femme illustre une certaine corrélation (voir illustration 32). Notons la formule pour prédire la taille de l’individu en fonction de la taille de son pied : y = 3x +53,4 sur la base de l’échantillon étudié (100 personnes). Le fitting de la ligne n’est pas parfait, mais cela donne une idée du fait que plus on est grand, plus on a tendance à avoir des grands pieds !\n\n» Essayez sur vous-même et appliquez l’équation : quels résultats trouvez-vous ?\n\nIllustration 32. Corrélation entre taille et pointure des individus\n\nDes courbes seillantes\n\nUn des points à toujours vérifier lors de l’application d’algorithmes de régression et de classification sont les sous ou surajustements (under et overfitting). Le défaut d’overfitting apparaît quand la courbe tente de trop coller aux données, en prenant en compte des points hors norme et introduisant des biais dans l’analyse à vouloir trop connecter les points intermédiaires. Le défaut d’underfitting se présente lorsque la courbe omet de représenter des tendances et propose un modèle qui ne tient pas compte de certaines subtilités du jeu de données.  \n  \nIllustration 33. Exemples d’overfitting (à gauche)  \net d’underfitting (à droite)\n\n\\> Les classifications\n\nLes modèles de classification ne prédisent pas une valeur continue, mais plutôt l’appartenance d’une donnée à une « classe », un groupe, une catégorie. Les algorithmes de classification vont nous permettre de prédire plusieurs états, tels que « toxique » ou « non toxique », « va acheter » ou « ne va pas acheter », « fraude » ou « pas fraude », « spam » ou « non spam », « survivant » ou « victime ». Ils peu­vent également être utilisés pour déterminer l’appartenance d’une donnée à un groupe particulier tel qu’une espèce ou un groupe démographique.\n\nLes « classifications binaires » testent l’appartenance à deux classes. Lorsqu’il y en a plus de deux, le problème est appelé « classification multiclasse ».\n\nSur un des exemples précédents, nous avions évoqué une liste de clients d’une entreprise de télécommunication. À partir des attributs et des métriques de chacun des clients et d’une base de comportements observés, il est possible à l’algorithme de prédire ceux qui auront une propension à changer de fournisseur d’accès.\n\nIllustration 34. Prédiction du Churn (propension d’un client à quitter son fournisseur) à partir de plusieurs attributs et métriques collectés dans le système client\n\nMême si nous laissons souvent le choix de l’algorithme aux mathématiciens de la Data Science, sachons qu’il existe plusieurs groupes d’algorithmes pour la classification :\n\n– la régression logistique : variation de régression linéaire qui effectue une régression, puis utilise un certain seuil pour prendre une décision de classification ;\n\n– les voisins les plus proches : mesure de distance utilisée pour trouver les voisins d’un point de données, prise de décision de classification à partir de la proximité ;\n\n– les arbres de décision : structures arborescentes où une classification est faite à travers une série de petites décisions qui aboutissent finalement aux feuilles d’un arbre ;\n\n– les forêts aléatoires : interrogation d’un groupe d’arbres, chacun avec une partie aléatoire des données d’entraînement, afin de prendre la décision de classification par consensus ;\n\n– naïve bayes : statistiques bayésiennes appliquées aux données pour prendre une décision de classification.\n\n\\> Le clustering\n\nLes algorithmes de clustering sont différents des deux précédents au sens où ils n’apprennent pas d’un échantillon de données. Leur objectif principal est de découvrir des similarités dans des ensem­bles de données et d’identifier des groupes : des clusters. Ces relations fournissent des informations que nous n’aurions peut-être pas pu forcément voir ou postuler nous-mêmes. Ces algorithmes sont puissants, populaires et fondamentaux. Cependant, ils peuvent être difficiles à interpréter.\n\n→ Des algorithmes sur les iris\n\nUn exemple classique et assez pertinent de clustering est celui du clustering des familles d’iris, rien qu’à partir des caractéristiques de leurs pétales et de leurs sépales. Non seulement l’algorithme arrive naturellement à l’identification de trois groupes, mais aussi, ces groupes correspondent à ceux que les botanistes avaient déjà identifiés. Le graphique ci-dessous montre comment les fleurs mesurées, représentées chacune par un point et ses coordonnées x, y et z, en fonction de la longueur et de la largeur de ses pétales et de ses sépales, se regroupent et se distinguent les unes des autres.\n\nIllustration 35. Groupements d’iris en fonction de la taille de leurs sépales et de leurs pétales",
          "Created At": "2025-09-06T13:22:34.094+02:00",
          "Updated At": "2025-09-06T13:22:34.094+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE089_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Les biais en algorithme\n\nLa découverte d’un biais dans la sélection des données.\n\nConnaître la classe d’algorithme à utiliser est donc relativement simple. Choisir la version de l’algorithme en question peut être fait par essais successifs ou par consultation avec un expert.\n\nLe principal risque d’un projet de Data Science ne se situe pas à ce niveau. Outre le point de la préparation de donnée et de sa qualité, ce sont les biais que nous allons insérer plus ou moins inconsciemment dans le processus qui peuvent, à eux seuls, conduire à de grosses erreurs dans les résultats ou les interprétations.\n\nLe biais algorithmique est le reflet des préjugés humains dans le développement, la formation et l’application d’un algorithme.\n\nCes biais sont exprimés dans les choix concernant la sélection des données, l’identification des variables et la façon dont un modèle est appliqué. On trouve ainsi :\n\n– le biais de la question : la question que nous posons ne contient-elle pas un biais implicite dans notre préférence pour une réponse facile ? (voir chapitre 2, Comment poser la bonne question) ;\n\n– le biais de sélection : l’échantillon à partir duquel nous formons un modèle est-il influencé par le contexte dans lequel nous appliquerons le modèle ? Avons-nous pris des données suffisamment nombreuses, diverses, réparties ?\n\n– le biais de disponibilité : fondons-nous nos travaux à partir d’un échantillon de commodité, parce qu’il est disponible ? Sélectionnons-nous ou construisons-nous des variables parce qu’elles sont pratiques et faciles à mesurer ou parce qu’elles sont corrélées et en ligne avec notre objectif ?\n\n– le biais de confirmation : le résultat de notre modèle reflète-t-il la réalité ou simplement nos attentes (biaisées) à son égard ?",
          "Created At": "2025-09-06T13:22:34.095+02:00",
          "Updated At": "2025-09-06T13:22:34.095+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE090_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Pour conclure\n\nLa formidable opportunité des Big Data réside dans cette capacité à pouvoir mieux comprendre des situations complexes grâce à la data. La technologie a fait tomber beaucoup de barrières pour les volumes et des rapidités de traitement : les défis demeurent pour aligner les équipes et la gouvernance des données afin de produire des datas alignées et des dynamiques de collaboration fructueuses.\n\nNous avons vu dans le chapitre précédent que les fondamentaux de la maîtrise de la donnée ne sont pas complexes. Ils reposent sur une logique plus proche du bon sens que de la science dure. La technologie est accessible sans longue courbe d’apprentissage.\n\nPourquoi ne voit-on pas de culture et de maîtrise data s’instaurer globalement et durablement dans les organisations en dehors des équipes digitales, de Data Science, voire informatiques ? Une des raisons est le facteur humain. En plus des connaissances théoriques et pratiques nécessaires, nous devons emprunter de nouvelles façons de raisonner, de travailler ou de diriger.\n\n------------------------------------------------------------------------\n\n14. SQL nous permet d’accéder à des bases de données et de les manipuler. SQL est devenu une norme de l’American National Standards Institute (ANSI) en 1986 et de l’Organisation internationale de normalisation (ISO) en1987.\n\n15. « Se dit d’un logiciel dont le code source est libre d’accès, réutilisable et modifiable (Linux, par exemple). \\[Recommandation officielle : logiciel libre.\\] » (Larousse)\n\n16. Source des données : kaggle.com\n\n17. Tinker M.A., The Effect of Slanted Text upon the Readability of Print, Journal of Educational Psychology, vol. 45, p. 287-291, 1954. Dans Tinker M.A., Bases for Effective Reading, Lund Press, Minneapolis, p. 136, 1965.\n\n18. Tinker M.A., Bases for Effective Reading, Lund Press, Minneapolis, p. 136, 1965.\n\n19. Tinker M.A. et Paterson D.G., Readability of Mixed Type Forms, Journal of Applied Psychology, vol. 30, p. 631-637, 1946. Dans Tinker M.A., Bases for Effective Reading, Lund Press, Minneapolis, p. 138, 1965.\n\n20. Tinker M.A. et Paterson D.G., Studies of Typographical Factors Influencing Speed of Reading : IX. Reduction in Size of Newspaper Print, Journal of Applied Psychology, vol. 16, p. 525-531, 1932. Dans Tinker M.A., Bases for Effective Reading, Lund Press, Minneapolis, p. 137, 1965.\n\n21. L. Sweeney, Simple Demographics Often Identify People Uniquely. Carnegie Mellon University, Working Paper 3. Pittsburgh, 2000.\n\nCHAPITRE 6  \nRelever le défi humain de la data\n\nLa suppression d’Excel dans les processus d’analyse.\n\nLes projets décisionnels laissent souvent de côté le facteur humain, qui est pourtant à l’origine de la plupart de leurs échecs. Si nous considérons les autres éléments de la data :\n\n– les technologies actuelles sont matures ;\n\n– les développeurs sont légion pour résoudre tous types de problème ;\n\n– les programmes, les réseaux et les systèmes d’exploitation sont fiables (souvenons-nous des écrans bleus Windows et des temps de démarrage interminables) ;\n\n– les interfaces sont de plus en plus ergonomiques ;\n\n– les supports d’utilisation sont adaptables à notre contexte (écrans de grande taille, mobiles, écrans tactiles, etc.).\n\nAujourd’hui, les solutions analytiques, bien implémentées, dans un contexte adapté, fonctionnent. Se plaindre d’une solution qui ne marche pas, c’est souvent se tromper de cible pour ne pas blâmer l’équipe qui l’a choisie ou qui l’utilise de manière mal optimisée.\n\nLa data n’est pas davantage coupable des échecs. La data est là. L’élément bloquant est souvent notre incapacité à la numériser, à la capturer, voire à la traiter efficacement :\n\n– soit nous posons des problèmes beaucoup trop compliqués ou utopiques, et effectivement nous n’aurons jamais la donnée fiable suffisante dans des délais et des coûts raisonnables ;\n\n– soit la donnée n’a pas la qualité suffisante, n’est pas dans des formats exploitables ou a été rendue complexe à centraliser, et, dans ce cas… c’est la faute des humains qui la saisissent. Une machine bien paramétrée ne produit pas de la mauvaise donnée, elle produit ce pour quoi elle a été programmée. Une data « est » : sa fluidité ne dépend que de la chaîne analytique ;\n\n– enfin, dans un contexte normal d’entreprise, la data est toujours dans des volumes maîtrisables par les technologies actuelles.\n\nComme dans la plupart des crashs, des accidents ou des piratages, en analytique, la principale source de problème va être… humaine.\n\nL’histoire de la data est pavée d’échecs\n\nCela n’a pas changé depuis près d’une décennie selon plusieurs sources :  \n– juillet 2019 : VentureBeat AI rapporte que 87 % des projets de science des données ne parviennent jamais à la production ;  \n– janvier 2019 : une enquête NewVantage rapporte que 77 % des entreprises indiquent que l’adoption par les entreprises des initiatives de Big Data et d’IA continue de représenter un défi majeur. Cela signifie que trois quarts des logiciels en cours de construction collectent apparemment de la poussière. Aïe !  \n– janvier 2019 : Gartner affirme que 80 % des processus analytiques ne fourniront pas de résultats commerciaux jusqu’en 2022 et que 80 % des projets d’IA resteront de « l’alchimie, dirigée par des magiciens » jusqu’en 2020 ;  \n– novembre 2017 : Gartner affirme que 60 % des projets \\#bigdata ne parviennent pas à franchir les étapes préliminaires. Oups, le cabinet signifiait 85 % en fait ;  \n– novembre 2017 : CIO.com répertorie sept moyens infaillibles d’échouer lors de l’analyse. « Le plus gros problème dans le processus d’analyse est de ne pas savoir ce que vous recherchez dans les données », déclare Tom Davenport, conseiller principal chez Deloitte Analytics ;  \n– mai 2017 : Cisco rapporte que seulement 26 % des répondants au sondage réussissent avec les initiatives IoT (taux d’échec de 74 %).\n\nBeaucoup d’échecs peuvent être évités en prenant en compte correctement le maillon humain. Dans ce chapitre, nous allons voir quels sont les éléments à intégrer dans nos projets pour prendre en compte cette dimension. Nous allons apprendre à les reconnaître et à les nommer.\n\nNous allons considérer différentes clés pour les résoudre, sachant qu’elles ne seront pas exhaustives et 100 % éprouvées, car tellement dépendantes de notre environnement. Un facteur primordial pour réussir à gérer les aspects humains sera avant tout le facteur temps : celui que nous emploierons pour accompagner, persuader ou influencer nos équipes en nous basant sur toute notre expérience et nos qualités humaines de manager du changement. Ce temps, c’est celui que nous allons gagner par la maîtrise des outils et de la donnée.",
          "Created At": "2025-09-06T13:22:34.095+02:00",
          "Updated At": "2025-09-06T13:22:34.095+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE091_1757157753085",
          "Status": "Ready For Refine",
          "Text": "# Le Bad Buzz de la data\n\nNous avons vu jusqu’à présent que les principes techniques et data n’ont pas à s’encombrer de termes complexes, encore moins de buzz. Il n’en est pas de même pour les humains : en voulant masquer notre ignorance ou feindre notre assurance, nous allons être tentés d’habiller le domaine de l’analytique de termes ou d’expressions sans base rationnelle. Ces postures vont alimenter d’autres postures chez nos collègues et créer des situations d’incompréhension, de frustration ou de conflit, comme nous allons le voir dans les exemples ci-après.\n\nLes expressions toutes faites mais toutes vides\n\nLes choses que l’on ne veut pas s’avouer",
          "Created At": "2025-09-06T13:22:34.096+02:00",
          "Updated At": "2025-09-06T13:22:34.096+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE092_1757157753085",
          "Status": "Ready For Refine",
          "Text": "# L’effet Dunning Kruger de l’analyste\n\nDunning et Kruger sont deux psychologues américains qui ont étudié le biais de surconfiance, qui, depuis la publication de leurs travaux en 1999, porte souvent leurs noms.\n\nCet effet suggère que moins nous connaissons un sujet, plus nous avons tendance à avoir des avis tranchés et résolus. En d’autres termes, l’humain surestime souvent son niveau de compétence.\n\nEn analytique, cette attitude est particulièrement problématique, car nous allons :\n\n– aborder des nouveaux problèmes avec trop de certitudes. Nous allons négliger des pistes de questionnement au profit d’hypothèses simplistes venant d’un faisceau de connaissances restreint. Les techniques de Design Thinking ou d’aide à la formulation de questions pertinentes vont aider à mitiger cet effet ;\n\n– sursimpliflier le travail des autres. Par exemple, nous allons atten­dre de la magie de l’informatique ou de la Data Science, sans connaître les subtilités et les difficultés de leur métier. Résumer le travail d’un analyste à « il faut juste éviter le garbage-on garbage-out (mauvaise qualité de data en entrée, mauvaise qualité de donnée en sortie) » ou bien celui d’un data scientist à « tu prends juste beaucoup de données et un algorithme, et on va trouver des réponses incroyables » sont des mythes encore bien présents dans les entreprises ;\n\n– surestimer nos compétences en oubliant nos lacunes. La fierté de démontrer notre savoir-faire data, souvent ésotérique aux yeux des autres, et l’accès à des capacités de traitement quasi sans limites nous confortent dans ce sentiment d’omnipotence. Nous le retrouvons même au niveau des opérateurs tableurs qui vous assureront qu’ils pourront continuer à relever tous les défis de la Data Science sur leur outil.\n\nPour éviter ces risques finalement assez fréquents, quatre recettes simples fonctionnent :\n\n– cultivons la diversité ;\n\n– pratiquons l’art de la question ;\n\n– faisons de la recherche des angles morts de l’analyse un moment d’équipe convivial, amusant. Jouer les Sherlock Holmes en équipe peut aboutir à des séances de brainstorming vraiment productives ;\n\n– lisons, écoutons les chercheurs du domaine, les vrais, pas ceux qui agitent les réseaux sociaux de la data. Le problème est que les bons scientifiques sont aussi de très bons pédagogues et peuvent nous renforcer dans notre Dunning Kruger. Allons plutôt chercher les vidéos et les articles au-dessus de notre zone de confort et savourons tous le chemin qu’ils pourraient nous rester à parcourir.\n\nL’astrophysique : l’anti-Dunning Kruger\n\nJe suis devenu passionné d’astrophysique et d’astrophotographie, et chaque nouvelle lecture d’article sur le sujet me rappelle à quel point je ne sais rien, à quel point l’humanité ne sait pas grand-chose. Nous constatons que des découvertes qui nous paraissent évidentes sont en fait très récentes, comme la notion de galaxies par Edwin Hubble qui remonte tout juste à 1924. Il suffit d’ouvrir un magazine pour se rendre compte de l’abondance des découvertes et des remises en cause en continu du domaine. Il m’arrive souvent de finir une lecture et de comprendre que je n’ai vraiment pas tout saisi. Quand je retourne à la data après ce bain de jouvence cognitive, je me sens mieux armé pour maîtriser mes biais.",
          "Created At": "2025-09-06T13:22:34.097+02:00",
          "Updated At": "2025-09-06T13:22:34.097+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE093_1757157753085",
          "Status": "Ready For Refine",
          "Text": "# La courbe de changement digital\n\nLe démarrage de la courbe de changement digital.\n\nInspiré des travaux de la psychologue Elisabeth Kubler Ross sur la courbe de deuil, ce mécanisme va constituer un frein à notre engagement et à celui de nos collègues. Il décrit par étapes les phases par lesquelles nous passons tous à l’annonce d’un changement et comment ces étapes vont constituer un frein à l’adoption des nouvelles dispositions.\n\nÀ ce stade du livre, nous sommes probablement bien avancés dans cette courbe du changement vis-à-vis de la data. Mais qu’en est-il de notre entourage professionnel ?\n\nPrenons pour exemple le parcours que nous suivons à l’annonce de la mise en place de nouveaux processus analytiques qui vont s’accompagner de changements d’outils, de méthodes et de responsabilités.\n\nFigure 14. Courbe de changement",
          "Created At": "2025-09-06T13:22:34.097+02:00",
          "Updated At": "2025-09-06T13:22:34.097+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE094_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## L’annonce\n\nLorsque la nouvelle du changement est annoncée, les décideurs en ont souvent fini avec leur propre courbe de changement : par contre, nous n’avons encore aucune idée de ce qui nous attend. Peut-être avons-nous été sensibilisés lors de réunions préparatoires, mais notre courbe ne commencera que lorsque le changement sera annoncé et irrévocable. Les décideurs doivent être à ce stade absolument assertifs et définitifs dans leur communication : le projet aura lieu.\n\nUne fois ce déclencheur activé, nous allons tous suivre avec une célérité et une intensité variables les étapes de la courbe.\n\nRassurons-nous, l’optimisation des processus ne rime pas avec la réduction des effectifs.\n\nDans un contexte toujours plus complexe et changeant, rares sont les entreprises qui pourraient clamer l’atteinte de l’excellence ultime en analytique. Les progrès continus dans ce domaine ne sont que des pas vers un horizon qui recule sans cesse. Les gains de temps doivent être constamment réinvestis pour progresser vers de nouvelles limites. Dans ce contexte, il est souvent contre-productif d’éliminer des collaborateurs riches d’expérience et de connaissance du métier au motif que certaines de leurs tâches peuvent être automatisées. Ce ne sont pas les machines et les nombres qui apporteront l’innovation, la pensée créative et la culture d’entreprise, mais bien ces collaborateurs libérés de nombreuses tâches manuelles et fastidieuses. De nouvelles disciplines doivent venir compléter nos transformations digitales telles que le Design Thinking ou l’Open Innovation (voir annexe Sea Matilda Bez).",
          "Created At": "2025-09-06T13:22:34.098+02:00",
          "Updated At": "2025-09-06T13:22:34.098+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE095_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## La descente irrationnelle\n\n\\> Le choc\n\nCette étape résulte de la combinaison de la surprise, de la peur soudaine et d’un sentiment de perte de contrôle. Les changements entrevus ou imaginés suite à l’annonce ne sont pas encore bien compris et nous n’avons pas encore le niveau d’information, voire de formation, pour les appréhender sereinement. Nous nous sentons dépassés par notre entourage qui est déjà au fait ou qui fait mine de comprendre. Faute de points de repère, nous sommes à court de toute réaction dans l’instant.\n\nCette phase peut rapidement enchaîner sur les deux étapes suivantes. Elle peut aussi se matérialiser plus longuement par un arrêt de toutes choses en cours.\n\nIl n’y a pas de recette pour gérer le choc, si ce n’est d’être le plus affirmatif et définitif possible dans l’annonce pour ancrer la décision comme étant définitive et ensuite accepter avec empathie ses effets. Cette fondation sera nécessaire pour la gestion des étapes suivantes.\n\n\\> Le déni, le refus\n\nNotre réflexe suivant sera de revenir à ce que nous connaissons. L’issue la plus simple et la mieux connue est de ne rien faire et de rester dans notre zone de confort, ne fût-elle pas si confortable que cela. Nous allons donc tout faire pour empêcher le changement, en commençant par nier son existence.\n\nParmi les techniques souvent employées nous retrouvons :\n\n– « j’étais là avant toi » : l’ancienneté et l’expérience vont servir d’arguments pour dénigrer le changement. Qui sommes-nous pour oser bousculer un statu quo qui marchait si bien ?\n\n– « j’ai mon réseau dans la société et ce n’est pas toi qui vas tout changer » : brandir le spectre du réseau d’opposants est une autre tentative d’intimidation. Si nous persistons dans notre décision, nous serons confrontés à une mutinerie des relations de nos équipiers dont nous n’avons pas idée ;\n\n– « on a déjà essayé et cela ne marche pas » : faire l’amalgame de tous les projets passés pour conclure que notre nouvelle tentative ne marchera pas.\n\nComment réagir ? Nous sommes toujours dans une phase émotionnelle dans laquelle nos points rationnels porteront peu. Nous devrons rester sur des postures d’écoute, de pédagogie, mais de fermeté quant à la décision.\n\n» Avez-vous déjà reconnu ces signes d’opposition exprimés lors de réunions data ? Repérez les croisements de bras, les soupirs, les affirmations que tout va bien, qu’il ne faut rien changer.\n\n\\> La colère ou la panique\n\nCe cocktail d’émotions et de perception est le parfait catalyseur pour la phase qui suit, dernier ressort d’expression quand nous perdons pied : la colère ou la peur panique. Lors de cette phase, ce n’est pas notre raison qui gouverne. Nous nous sentons en danger de perdre ce que nous avons construit, ce qui a contribué à notre routine, à notre succès et à notre reconnaissance.\n\nPour le responsable projet, ce n’est pas une phase gérable. Il faut que colère se passe. N’ajoutons pas à la colère en nous laissant emporter. Après tout, cette colère n’est pas dirigée contre nous mais plus contre notre projet.\n\nLe sourire et Antoine de Saint-Exupéry\n\nMes projets data m’ont permis d’apprendre d’experts sur les sujets de gestion des phases de changement. Deux approches m’ont marqué, tant j’ai payé le prix de les avoir négligées et tant elles sont simples à mettre en place.  \nLa première est de conserver le sourire dans les réunions. J’avais été surpris par un consultant qui avait travaillé avec moi sur des projets data très politiques. Alors que les détracteurs du projet l’attaquaient systématiquement lors des meetings de suivi, faisant usage de mauvaise foi et parfois de malhonnêteté intellectuelle, il conservait un sourire, simple, honnête, naturel. Cela avait le don de ne pas alimenter le conflit et conduisait plus rapidement à un retour au calme. Quand je lui ai demandé comment il faisait pour garder son calme, sa réponse fut : « Ce n’est pas à moi qu’ils en veulent. C’est au projet. Moi, je sais que j’ai fait le boulot professionnellement. » Depuis, je fais en sorte de sourire plus et cela contribue invariablement à désescalader les débuts de conflit !  \nLa seconde approche dérive de la première. Ne mettons pas le projet comme un élément de discorde entre nous et nos interlocuteurs. Dans un face-à-face, l’énergie sera invariablement dirigée contre la personne. Si nous nous tournons tous vers un tableau où le projet est décrit et dirigeons nos énergies et nos critiques vers ce tableau, alors ce sera moins à la personne que nous en voudrons. Ce changement de posture peut désamorcer des discussions houleuses.  \n« Aimer, ce n’est pas se regarder l’un l’autre, c’est regarder ensemble dans la même direction. »  \nAntoine de Saint-Exupéry, artiste, aviateur, écrivain (1900-1944)",
          "Created At": "2025-09-06T13:22:34.099+02:00",
          "Updated At": "2025-09-06T13:22:34.099+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE096_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## La transition émotionnelle\n\nLes phases précédentes, défensives et offensives, n’ont pas abouti à faire dérailler le projet. L’initiative progresse. La courbe de changement digital va s’aplatir et entamer deux étapes pendant lesquelles nous allons digérer le changement.\n\n\\> La dépression\n\nL’inéluctable est devant nous. Nous avons un sentiment de défaite, mais également de peur de l’inconnu. Nous allons avoir besoin de nous recentrer, de trouver de nouveaux repères.\n\nPour l’équipe en charge, c’est le début d’une reprise de contact avec des bases plus rationnelles. L’empathie et l’écoute vont être clé pour rassurer nos collaborateurs : un projet data est là pour valoriser nos expériences, pas pour les détruire.\n\n\\> La justification\n\nLe sens du projet va être rationalisé. Une fois le spectre des dangers immédiats évacués (perte de responsabilité, inutilité du rôle), après l’acceptation de la perte programmée de sa feuille Excel ou de ses processus tant chéris, la phase de reconstruction mentale autour du nouveau paradigme de travail commence.\n\nÀ la lumière des promesses d’une analytique bien huilée, les processus actuels apparaissent enfin tels qu’ils sont : douloureux et chronophages. La promesse d’engagements plus responsabilisants et de plus de temps pour soi va contraster avec les tâches quotidiennes ennuyeuses et débilitantes.\n\nSans peindre un avenir trop simple et trop rose, car les initiatives analytiques sont une longue suite d’échecs tendant vers des succès, conforter la raison d’être de ces changements avec sincérité va permettre de prendre ce tournant et d’en sortir en pleine accélération.",
          "Created At": "2025-09-06T13:22:34.099+02:00",
          "Updated At": "2025-09-06T13:22:34.099+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE097_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Le retour au rationnel\n\n\\> La négociation\n\nLe sens du projet va apparaître dans les esprits et nous devons être prêts à nous faire pousser par cette vague d’adhésion.\n\nC’est toute l’abnégation de l’équipe qui rentre en jeu car, pour nous, il s’agit de reprendre la main sur un domaine que nous croyions perdu. Il faut s’effacer pour que les évidences du projet deviennent les nôtres. L’initiative, la demande de participation, de prise en charge de tests ou de définitions de processus doivent à ce stade être sciemment décentralisés.\n\nFaire place libre pour cette motivation est aussi une des clés du succès : qui mieux que chacun d’entre nous pourra optimiser l’application de ces nouvelles approches à son propre domaine ? Si des bases de processus claires ont été définies, nous pouvons les mettre en œuvre au mieux pour nos besoins personnels.\n\n\\> L’engagement\n\nNous y sommes. Peut-être que les décideurs auraient pu accélérer l’adhésion par une communication plus large et plus en avance de phase. Nous nous rendons compte que ce temps non technique et non consacré à la data est absolument nécessaire pour arriver à un résultat. L’engouement des décideurs pour avancer vite et apporter des victoires rapides (quick wins) peut se payer cher au départ d’un projet s’ils n’embarquent pas les équipes.\n\nIl faudra s’armer de patience car, si certains d’entre nous mettent juste quelques minutes, ou quelques heures, pour d’autres ce sera des jours, voire des semaines ou des mois… voire jamais pour accepter le changement.\n\nLa norme du 3-4-3 encore appelée EAO (Enthousiastes, Attentistes, Opposés)\n\nJuste fondée sur l’observation, cette norme peut nous permettre de décoder les postures de chacun de manière simple, sans être toutefois trop simpliste, et donc d’autoévaluer le niveau d’acceptation de notre projet.  \nÀ chaque démarrage de projet data, une minorité sera prête à nous suivre. Avec un minimum de questions, en reposant sur la confiance, ces supporteurs de la première heure vont jouer un rôle primordial :  \n– ils vont nous économiser de l’énergie en management et en persuasion ;  \n– ils vont accepter les premières erreurs et les échecs inhérents aux projets qui commencent ;  \n– ils vont nous apporter nos premières victoires.  \nCe sont les enthousiastes. Sur dix équipiers, souvent deux, trois, voire quatre, s’engagent avec nous sans hésiter.  \nÀ l’opposé du spectre, nous retrouverons invariablement les opposés. Ils sont entre deux ou trois à refuser tout changement ou toute concession. Nous commettons souvent la même erreur à leur encon­tre : la surfocalisalisation. En effet, ils vont prendre beaucoup de notre temps et de notre énergie pour peu de résultats. Ils sont souvent peu enclins à évoluer. Plus ils auront d’attention, plus ils seront écoutés. Dans les étapes initiales, ils risquent de casser des dynamiques fragiles. S’il faut écouter toutes les voix, il faut aussi savoir faire preuve de pragmatisme et penser au projet et à ceux qui comptent dessus.  \nMon expérience m’a appris que, malheureusement, beaucoup de ces batailles pour convaincre les opposés sont vaines à court terme. L’éducation, la patience, le coaching ou l’impulsion très forte d’un leader devront prendre le relais.  \nEnfin, au milieu, nous avons le groupe de trois ou quatre observateurs indécis qui, soit par manque d’informations soit par prudence, attendent de voir comment le projet va prendre forme. Si nos trois enthousiastes sont notre relais et démontrent des résultats, alors ce groupe intermédiaire sera tenté de suivre et, de trois à quatre supporters, nous passerons à six à huit engagés, et nous aurons commencé une adhésion durable.  \nL’adhésion à 100 % est un mirage que j’ai trop longtemps cherché. Une approche plus réaliste type 3-4-3 m’aurait évité bien des soucis, des nuits blanches et des conflits.  \nUne question m’est souvent posée : que faire des irréductibles ? Accompagnement, formation ou positionnement sur des nouvelles fonctions moins exigeantes en digital doivent faire partie des options. L’erreur à ne pas commettre, néanmoins, ne se situe pas dans le déroulé de ces plans d’aide, mais dans le recrutement des prochains collaborateurs. En effet, si nous nous devons d’aider nos équipiers, nous sommes impardonnables si nous continuons à utiliser les critères de recrutement des décennies précédentes. C’est à nous de travailler avec nos ressources humaines pour que les valeurs d’agilité, d’acceptation du changement et d’ouverture aux nouvelles approches data fassent partie des critères de recrutement.\n\nL’intégration de ces étapes du changement va servir de guide pour mieux connaître le niveau d’engagement des équipes et adopter un rythme en harmonie avec ce que l’organisation peut suivre.",
          "Created At": "2025-09-06T13:22:34.100+02:00",
          "Updated At": "2025-09-06T13:22:34.100+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE098_1757157753085",
          "Status": "Ready For Refine",
          "Text": "# Le YBG-IBG (JSP-TSP en français)\n\nImaginons-nous discuter avec un collègue ou notre patron. Animés par la passion de notre projet, nous évoquons autour d’un café les formidables opportunités à venir. Alors que nous débordons d’enthousiasme, notre interlocuteur nous coupe en plein élan : Pourquoi te mettre tout ce travail sur le dos ? Pourquoi prendre tant de risques ? Entre toi et moi, dans 18 mois, Tu Seras Parti et Je Serai Parti (You’ll Be Gone and I’ll Be Gone).\n\nAujourd’hui, nous ne faisons plus carrière : l’impact d’initiatives même moyen terme a peu de chance d’être apprécié pendant que nous sommes en fonction. De plus, les mesures de performance sont souvent faites au trimestre par des dirigeants qui eux aussi changent de plus en plus vite. Des projets structurants, inscrits dans la durée ont peu de chance d’être reconnus. Pire encore, avec les pivots opérationnels réguliers opérés à chaque changement de leader, il y aura de fortes chances que nos choix d’hier ne soient plus dans la ligne du parti, voire non compris par la nouvelle garde.",
          "Created At": "2025-09-06T13:22:34.101+02:00",
          "Updated At": "2025-09-06T13:22:34.101+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE099_1757157753085",
          "Status": "Ready For Refine",
          "Text": "# L’obstacle du management",
          "Created At": "2025-09-06T13:22:34.101+02:00",
          "Updated At": "2025-09-06T13:22:34.101+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE100_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Les motivations\n\nÀ la gestion court terme d’objectifs, à la rotation des postes évoquée plus haut va s’ajouter un biais important dans l’attribution des bonus et autres reconnaissances.\n\nLes processus analytiques optimisés vont permettre de gagner beaucoup de temps qui va pouvoir être investi en conversations exploratoires, en discussions de résolution de problèmes, en GEMBA (voir chapitre 2, Comment poser la bonne question), en tests de nouvelles options, voire en temps pour soi. Malheureusement, aux yeux d’un management sensible à l’effort et au temps passé au bureau, ce temps libre et serein ne vaudra jamais des heures en panique, des journées sans fin ou des week-ends au bureau.\n\nLa capacité d’analyse, de simulation et de réflexion va aussi nous permettre d’anticiper ou d’aborder les constantes évolutions de notre environnement. Nouvelles normes comptables, nouvelles réglementations, nouvelles situations à mesurer : si nous maîtrisons la donnée, son traitement et ses flux grâce aux outils et aux méthodes simples décrites plus haut dans ce livre, nous pourrons concentrer nos efforts sur des résolutions en équipe avec de bonnes chances de réussite. De ce fait, nous aurons moins souvent l’occasion de nous présenter en pompier, héros du jour, qui vient sauver une situation perdue, au prix d’un travail acharné. Nous rendons nos tâches plus simples, plus faciles à gérer, et ainsi moins sujettes à l’admiration et surtout à la récompense.\n\n» Qu’allez-vous choisir ? Un modèle de récompense éprouvé, qui marche à coup sûr, ou prendre le risque de livrer des projets innovants qui ne seront pas reconnus à leur juste valeur ?",
          "Created At": "2025-09-06T13:22:34.102+02:00",
          "Updated At": "2025-09-06T13:22:34.102+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE101_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## La transparence\n\nLa transparence n’est pas forcément ce que nous voulons vraiment. C’est une vertu appréciable chez les autres. Rendre l’information fluide et donner à chacun la possibilité d’analyse détaillée est louable, mais cela conduit aussi à changer les rapports de force entre employé et dirigeant. L’omniscience théorique du chef, qui pouvait éventuellement se justifier par une expérience et une formation prestigieuse, se retrouve à la merci de la capacité d’analyse d’une situation d’un collaborateur.\n\n» Allez-vous avoir envie de donner ce pouvoir à l’équipe qui vous entoure, pour qu’ils risquent de voir que le roi est nu ?",
          "Created At": "2025-09-06T13:22:34.103+02:00",
          "Updated At": "2025-09-06T13:22:34.103+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE102_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Responsabilité et culpabilité\n\nLa montée en compétences de collaborateurs et la création de chaînes d’informations fluides vont également remettre en cause un mécanisme de défense efficace jusqu’à présent : « le responsable mais pas coupable ». Quand la responsabilité qui va avec le titre et le salaire peut être déconnectée de la culpabilité, il est plus facile de se défendre. En cas de problème, nous pourrons arguer que nous ne pouvions pas connaître les détails de ce qui se passait ou que l’on ne nous a pas tout dit. Nous avons une barrière de protection qui fonctionne pour tout.\n\nUne analytique et un reporting efficaces réduisent considérablement ce mécanisme de défense. Nous allons être constamment avertis des événements qui comptent, des problèmes qui surviennent ou de signaux avant-coureurs de crise : nous n’aurons plus d’excuse de ne pas savoir et ne pas avoir agi en conséquence. Nous serons alors responsables et potentiellement coupables.\n\nL’affaire du sang contaminé : responsable mais pas coupable\n\nLe 4 novembre 1991, Georgina Dufoix déclare sur TF1 : « Je me sens profondément responsable ; pour autant, je ne me sens pas coupable, parce que vraiment, à l’époque, on a pris des décisions dans un certain contexte, qui étaient pour nous des décisions qui nous paraissaient justes. »  \nCette déclaration est faite dans le cadre de l'affaire du sang contaminé, un scandale sanitaire, politique et financier qui a touché la France dans les années 1980 à la suite d'infections au VIH par transfusion sanguine. Les révélations qui sont faites sur le comportement des décideurs médicaux, administratifs et politiques font état de graves négligences, de mesures inefficaces, de retard de prise de décision et de la défaillance des milieux impliqués. En conséquence, plus de 550 personnes, hémophiles et patients hospitalisés, ont été contaminées par le VIH.  \nLes protagonistes de l’affaire se défendent notamment en invoquant les incertitudes médicales au début de l’épidémie. Ils plaident la difficulté d’analyser une situation et d’aligner les décisions à l’époque du drame. Si le 20 Décembre 1991, la responsabilité de l'État est reconnu pour « ne pas avoir autoritairement et sans délai mis fin à la distribution des produits confinés » indiquant que « la révélation de catastrophe sanitaire annoncée commandait qu'il soit mis fin autoritairement et sans délais à la distribution de produits sanguins conta­minés. », au final aucun des acteurs de cette affaire ne sera jugé coupable au motif que les connaissances scientifiques de l’époque ne permettaient pas de prendre d’autres décisions.",
          "Created At": "2025-09-06T13:22:34.103+02:00",
          "Updated At": "2025-09-06T13:22:34.103+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE103_1757157753085",
          "Status": "Ready For Refine",
          "Text": "# Les dynamiques d’équipe",
          "Created At": "2025-09-06T13:22:34.104+02:00",
          "Updated At": "2025-09-06T13:22:34.104+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE104_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Les visages de l’opposition\n\n\\> Geoffrey Effraie\n\nComme nous l’avons vu dans la courbe du changement digital, le toboggan émotionnel n’épargnera personne. Pour Geoffrey, c’est la peur de tout perdre qui va primer. En voyant ses outils, ses processus ou ses secrets disparaître avec les nouvelles solutions, il va percevoir les menaces de perdre ses responsabilités, son poste, voire son aura.\n\n\\> Claire Excel\n\nLes tableurs restent LA solution décisionnelle la plus utilisée. Pour beaucoup d’entre nous, c’est un outil que nous avons dû utiliser faute d’alternatives disponibles pour analyser la donnée et compenser sa mauvaise qualité par des modifications de dernière minute. Mais c’est aussi un faire-valoir de poids, tant il permet de produire de tableaux et de graphiques raffinés. Grâce à ses macros, c’est aussi le terrain de démonstration de force : on se démarque avec du code souvent ésotérique aux yeux de l’équipe, qui produit des animations de cellules clairement indicatives d’un niveau de complexité de programmation avancé.\n\nImposer des outils d’analyse et des processus intégrés de préparation de données va rendre obsolète les approches tableurs jus­qu’alors tant reconnues. Pour Claire Excel, c’est l’annonce de la fin d’une ère de reconnaissance et d’admiration. Notre projet ne pèsera pas bien lourd face à ce risque de perte de statut.\n\n→ Je ne l’ai plus jamais regardé de la même manière\n\nPendant mes années parmi les grands acteurs de la Silicon Valley, j’ai été amené à travailler avec de brillants jeunes diplômés, frais émoulus des plus grandes universités américaines. L’attrait de la « Baie » nous permettait de recruter les meilleurs, le « top 1 % » des promotions. Parmi eux se trouvait Claire, une jeune femme aussi brillante que sympathique avec toute l’équipe. Elle s’était imposée par sa maestria sous tableur, au point d’avoir obtenu des PC ultraperformants pour faire tourner ses macros et ses calculs dans des temps raisonnables. Son nom était devenu la référence en la matière et nous avions cessé d’aller chercher sur l’Internet des réponses à des problèmes Excel : il suffisait d’aller demander à Claire. Elle ne faisait pas partie de mon groupe data, elle était nouvellement arrivée dans les équipes finance que je supportais.\n\nCe soir-là, je finissais une longue journée et je m’apprêtais à partir. Claire semblait désespérée face à ses quatre (oui, quatre) écrans de lignes et de colonnes. Un rapport était dû le soir même et plus rien ne marchait. Je décidais de lui venir en aide : au bord de la crise de nerfs, elle ne refusa pas ce support. N’ayant pas le temps de me replonger dans la complexité de son document, je choisis de reculer pour mieux sauter : j’extrayais l’ensemble des données en un jeu de données tabulaires. Je reproduisis ensemble les « recherchev » sous forme de jointures propres. J’appliquais enfin dans ma base de données les précalculs nécessaires. En une dizaine de minutes à remettre d’aplomb son processus, j’avais obtenu une donnée raffinée capable de fournir non seulement les tableaux attendus, et ce, avec plus d’agilité. La rapidité des calculs était sans commune mesure : nous étions passés de dix minutes à quelques secondes.\n\nJe me tournai vers Claire et lui proposai de parler à son superviseur : lui dire qu’elle n’avait pas besoin de finir ce soir dans la précipitation, qu’il valait mieux qu’elle reprenne les choses correctement avec une approche plus orientée data et qu’elle présente ses résultats et ses progrès le lendemain.\n\nAvec un sourire rempli d’empathie et de gentillesse, elle me répondit d’une voix assurée : « Merci Gauthier, c’est vraiment une super approche, mais, tu sais, j’adore mon Excel. » Je ne l’ai plus jamais regardée de la même manière…\n\n\\> Larry Big Data\n\nLarry a les yeux qui brillent : les nouvelles technologies, les jeux de données massifs sont autant de défis qu’il veut enchaîner. Peu importe si les affaires courantes ne sont pas traitées : les succès apparents des projets spectaculaires que peu de collègues comprennent masqueront tout cela.\n\nVenir avec des initiatives pragmatiques et ciblées, qui ne brilleront pas par ce qu’elles sont, mais par ce qu’elles livrent, ne va pas être aligné avec les choix politiques des Larry.\n\n\\> Rose Royce\n\nRose sait que tout échec peut coûter cher en politique. Elle connaît les risques des projets data. Elle connaît les taux d’échec impressionnants. Plutôt que de se former et de monter en compétences, elle va choisir de prendre une assurance : opter pour une solution chère et connue dont on ne pourra pas lui reprocher le choix. Si sa société est réputée, elle sait aussi que consultants et éditeurs feront tout pour lui éviter un échec. Enfin, une implémentation d’un logiciel « Rolls Royce » est un fait de guerre qui aura de la valeur dans un CV.\n\nCes choix fonctionnent avec des managers peu au fait des options ou des collègues partageant la même vision. Ils peuvent conduire à acquérir des technologies beaucoup trop compliquées pour les équipes en place. Ils peuvent créer des approches beaucoup trop lourdes pour l’organisation. Pire encore, parce que le nom ne fait pas tout, ces options peuvent même ne pas remplir des objectifs clés du projet et aboutir à des fiascos monumentaux.\n\nDans le milieu informatique, il y a un viel adage qui a la vie dure : « Quoi qu’il arrive, on ne se fait jamais renvoyer si on choisit SAP, Salesforce ou Oracle. »\n\n\\> Anton Python\n\nAnthon a délaissé Excel depuis longtemps : tout est codage et algorithme, pratiques qu’il maîtrise parfaitement.\n\nLe choix de standardiser, de démocratiser et de pérenniser les processus data ne peut s’accommoder de la prolifération de bouts de codes aux mains d’une personne. Si le Data Wrangling par le langage python est très performant, les risques présentés par la maintenance de ces pages de codes peuvent devenir délicats à gérer.\n\nMettre en place des traitements centraux de la donnée, confier leur développement à des véritables codeurs qui créent des programmes documentés, optimisés et maintenus va constituer une menace pour les Anthon, qui risquent de perdre leurs prérogatives de codeurs.\n\n» Regardez autour de vous pour identifier les rôles que chacun tient ? En avez-vous identifié d’autres ?\n\nÀ la décharge de cette équipe, la proposition de nouvelles pratiques et technologies sonne souvent comme une agression. Ima­ginons un instant que le travail tel que nous l’avons accompli depuis des années soit remis en cause en l’espace d’une annonce lors d’une réunion à laquelle nous n’étions pas préparés. Qu’allons-nous entendre ? « Nous allons tous faire un nouveau projet qui rendra tout ce que l’on fait meilleur » ou bien « le travail que vous avez fait jusqu’à présent ne vaut rien et nous allons vous en donner un nouveau, celui que (moi) j’ai défini, et vous allez voir, il sera nettement mieux ».\n\nVous comprenez que cette perception ne fera pas apparaître vos projets sous des auspices favorables.\n\nLes Pionniers portent invariablement les blessures issues de ces confrontations. On me demande souvent si j’ai compté les coups dans le dos reçus lors de mes projets data. Je suis passé à autre chose, mais j’explique souvent que ce sont des coups de face auxquels il faut s’attendre, tant l’anxiété des collaborateurs peut être forte, même si nous avons les meilleures intentions.",
          "Created At": "2025-09-06T13:22:34.105+02:00",
          "Updated At": "2025-09-06T13:22:34.105+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE105_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Nos superpouvoirs\n\nL’environnement humain aura plus de raisons de s’opposer aux projets data que de les soutenir. Avons-nous des raisons de ne pas nous sentir à la hauteur des exigences des nouveaux enjeux et de nous sentir dépassés ?\n\nLes défis du digital vont se reposer sur la compréhension de nouvelles techniques qui peuvent être apprises. Ils vont aussi reposer sur de nouvelles attitudes qui vont nous permettre d’embrasser le changement. Nous avons tous dans notre humanité ces capacités et il nous suffit d’aller les puiser. Elles pourront être affûtées avec le temps, mais elles ne sont pas liées à une notion de quotient intellectuel ou de niveau d’études.\n\nPour réussir en analytique, nous devrons nous reposer sur quatre super pouvoirs :\n\n– le pouvoir de dire « je ne sais pas » : cela devrait être le point de départ de toute démarche analytique nouvelle. Ouvrir le champ des possibles va permettre d’éviter les biais et les réponses routinières ;\n\n– le pouvoir de dire « pourquoi » : une réponse en analytique doit ouvrir une nouvelle question. Si une fois un problème résolu nous nous arrêtons là, alors bâtir des systèmes et des processus n’a pas de sens. C’est cette dynamique qui pousse au progrès et à l’innovation ;\n\n– le pouvoir de savoir dire « j’ai échoué » : c’est un lieu commun que nous oublions en entreprise : il n’y a point de succès sans une longue suite de petits échecs. Oser échouer est une qualité pour avancer. Il faut juste savoir calibrer ses échecs et éviter les coups de poker trop gros ;\n\nAvant-poste Silicon Valley\n\nPour de nombreuses entreprises, avoir son centre de recherche ou d’innovation en Silicon Valley est un must. Quand vient le choix de qui y envoyer, la sélection se fait logiquement sur les salariés les plus méritants. Mais quelle grille de lecture est utilisée pour ce choix ? Le salarié dont le chemin de carrière a été parfait est-il le meilleur pour le poste ? Grenouiller, gravir les échelons, savoir agir politiquement sont des qualités essentielles au siège social : elles deviennent de véritables obstacles dans un milieu d’innovation et de prise de risque. Ne faudrait-il pas privilégier ceux qui ont essayé beaucoup de choses, échoué, appris, osé ?\n\n– le pouvoir de « vouloir faire » : ce dernier pouvoir illustre la motivation à apprendre pour comprendre et maîtriser. L’analytique d’aujourd’hui ne fait que reproduire des mécanismes ancestraux de collecte de données, de raisonnement et de prise de décision. Les technologies qui soutiennent ces processus ne sont pas complexes et peuvent être comprises, quel que soit le niveau auquel nous souhaitons les utiliser. Nous sommes tous en capacité de pouvoir faire. Acquérir cette confiance va nous permettre d’oser nous engager sur les projets analytiques.\n\n→ Les principes fondateurs du leadership\n\nCes principes fondateurs de la UC Berkeley Haas School of Business définissent les qualités recherchées à la Business School. On retrouve parfaitement, dans l’état d’esprit managérial recherché, les qualités d’un bon analyste.\n\nEspérons que ces états d’esprit se rejoignent dans les organisations de demain pour éviter les antagonismes évoqués dans les paragraphes précédents.",
          "Created At": "2025-09-06T13:22:34.105+02:00",
          "Updated At": "2025-09-06T13:22:34.105+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE106_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## La force de notre humanité et de la diversité\n\nL’ultrafocalisation de l’opinion publique, des médias, et des éditeurs et des fabriquants sur les aspects technologiques a fait oublier l’importance clé de l’humain.\n\nLa machine n’innove pas, la machine doit apprendre de l’humain avant d’apprendre elle-même. Si nous ne jouons pas notre rôle créatif et éthique, rien de nouveau ou de bon sortira des algorithmes.\n\n« Les ordinateurs sont inutiles. Ils ne savent que donner des réponses. » Picasso\n\nLe moteur du progrès en analytique peut être à court terme la capacité de traitement ou de mesure de données, mais, sans but intelligent, nous savons que faire de la data pour faire de la data n’a aucun sens. C’est l’apport de l’humain qui va donner son sens à l’analytique : à la fois sa direction et sa signification.\n\nNotre responsabilité est finalement assez lourde dans la programmation de ces machines et dans les résultats qu’elles vont fournir. Dans un monde connecté et complexe, il devient de plus en plus délicat de prendre des décisions pertinentes, justes éthiquement et respectueuses. Nous ne pouvons pas nous seuls porter cette responsabilité. Une équipe d’analystes non inclusive de la diversité de notre société n’est pas équipée pour prendre ces décisions qui influenceront les algorithmes et les interprétations des résultats. Il nous faudra les voix les plus diverses pour non seulement prendre en compte la complexité et la diversité de nos problèmes, mais également pour nous assurer que nos réponses ne soient pas biaisées et discriminantes. Nous n’avons pas la vérité innée : une équipe diverse nous permettra d’apporter une certaine raison dans nos quêtes et nos décisions.\n\nLa diversité est la clé d’une analytique holistique, éthique et non biaisée.",
          "Created At": "2025-09-06T13:22:34.106+02:00",
          "Updated At": "2025-09-06T13:22:34.106+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE107_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Le vrai visage du data scientist\n\nLa fonction de data scientist, décrétée « métier le plus sexy22 » par la célèbre Harvard Business Review a trop souvent été limitée à sa nature mathématique, statistique et Big Data. Cette notion se fonde sur la définition de DJ Patil, data scientist américain, qui définissait un rapport clair avec la notion de sciences.\n\nNous ne pouvons pas tous devenir ce data scientist-là : les compétences en sciences et l’affinité avec les nombres et les formules mathématiques ne sont pas pour tous (et heureusement).\n\nRapidement, les entreprises se sont rendu compte qu’elles avaient besoin d’un autre type de profil, un profil qui aurait peut-être moins de compétences mathématiques, mais plus de connaissances métier, de compétences data, de capacité à « hacker » des problèmes, de qualité de présentateur et de savoir être.\n\nIl est rare que ces qualités soient réunies en une seule personne, et celle-ci serait certainement onéreuse à recruter. En plus, quand bien même l’organisation pourrait attirer de tels talents, la difficulté est de les retenir, tant ils sont convoités et surtout délicats à satisfaire en termes de poste et de responsabilité.\n\nUne approche alternative à chercher ce mouton à cinq pattes va consister à assembler ces qualités au sein d’une équipe avec une diversité de profils. Ce data scientist virtuel pourra être construit par la formation des talents existants suivant les affinités de chacun : certains trouveront un intérêt dans la technologie, d’autres dans les datas, dans l’investigation ou dans les algorithmes et le codage.",
          "Created At": "2025-09-06T13:22:34.106+02:00",
          "Updated At": "2025-09-06T13:22:34.107+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE108_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Les nouveaux champs de développement\n\nEn comprenant que la data n’est pas une fin en soi, et que c’est la libération de nos potentiels de réflexion et d’action qui va compter, nous allons devoir apprendre à dompter de nouveaux mécanismes de pensée et de comportement. Nous avons évoqué plus haut les approches de Design Thinking. Nous verrons comment développer notre potentiel avec l’Open Innovation (voir annexe Matilda Bez).\n\n------------------------------------------------------------------------\n\n22. Davenport T.H. et Patil D.J., Data Scientist : The Sexiest Job of the 21st Century, Harvard Business review, octobre 2012.\n\nCHAPITRE 7  \nConstruire des processus efficaces, légers et durables\n\nL’annonce de la reprise du processus de reporting en début de mois.",
          "Created At": "2025-09-06T13:22:34.107+02:00",
          "Updated At": "2025-09-06T13:22:34.107+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE109_1757157753085",
          "Status": "Ready For Refine",
          "Text": "# Le défi des processus lean\n\nLean veut dire léger, fin, élégant. Ce terme est particulièrement adapté pour décrire des processus analytiques performants. En effet, la qualité d’un processus va résulter de plusieurs paramètres dont l’harmonisation va compter plus que la maximisation de certains en particulier.\n\nNous avons vu dans un exemple précédent comment une obsession pour la digitalisation d’un processus tournait au désastre opérationnel par une concentration sur les seuls aspects technologiques (voir chapitre 4, Quelle est l’utilité de la technologie en data ?). Nous avons également constaté l’intégration de solutions « Rolls Royce » qui, tout en étant les meilleures du marché, se montrent particulièrement inefficaces au sein de chaînes de traitements mal adaptées pour ces gros outils. De même que la concentration sur l’éradication d’intervention humaine peut engendrer des complexités techniques qui créent plus de risque qu’elles n’en résolvent, à vouloir laisser à la machine le traitement automatisé de tous les cas possibles, on crée des usines à gaz délicates à auditer et à maintenir, là où l’humain pourrait juste apporter un peu de jugeote.\n\nL’optimisation d’un processus est un exercice qui doit se concentrer sur le résultat global du processus et non ses maillons, car le premier sera toujours limité par la performance, la qualité ou la fiabilité des derniers.\n\n→ Quel processus va finir en premier ?\n\n1\\. Saisie manuelle 5 minutes – Contrôle 20 secondes – Analyses Excel 10 minutes – Reporting 2 minutes.\n\n2\\. Correction des données source 10 minutes – ETL 20 secondes – Analyses BI 5 minutes – Reporting 2 minutes.\n\nLes deux processus finiront en même temps, même si le second est quasiment entièrement digital. Dans cet exemple, le problème est a priori la complexité de la capture de l’information, qui, lorsqu’elle est automatisée, doit se faire sur de la donnée très bien calibrée. Cette phase peut nécessiter un gros investissement temps, alors qu’une capture manuelle peut intégrer cette complexité naturellement et rapidement. L’arbitrage se fera certainement sur la fiabilité d’une saisie manuelle et de son contrôle, comparée à celle d’une préparation manuelle des données avant un ETL normalement 100 % fiable.\n\nL’optimisation de maillons indépendants du reste de la chaîne analytique est tentante, car elle permet à une équipe de livrer un résultat visible et récompensable plus rapidement. Elle ne s’encombre pas des tenants et des aboutissants du processus et surtout évite les politiques. Le data pionnier devra systématiquement arbitrer entre ces victoires rapides pas toujours durables et les initiatives à long terme moins motivantes.\n\nL’utilisation de l’IA pour une optimisation holistique des processus  \n\nWendy Pfeiffer, CIO de Nutanix, élue directrice informatique de l’année en 2019 à Berkeley, travaille de manière très innovante sur le sujet de l’optimisation des processus.  \nJe veux regarder au-delà du simple gain de temps ou de la complexité. Je veux comprendre aussi comment le processus s’insère dans la journée d’un collaborateur pour maintenir un épanouissement et une contribution de valeur de sa part. Le constat est le suivant : réduire une journée à la supervision d’une succession de tâches automatisées n’est pas motivant, voire débilitant. Accélérer tous les processus permet certes de gagner du temps, mais supprime le repos masqué contenu dans certaines tâches : en effet, certaines opérations nous donnent le loisir de prendre du recul, d’entretenir une conversation ou même de « débrancher ». Si nous nous débarrassons de tous ces temps « faibles » ou lents de notre journée, n’embarquons-nous pas notre cerveau dans une course effrénée peu propice à la création ? Je crois à la valeur des enchaînements de tâches qui créent des équilibres dans la journée ou dans la semaine. C’est en variant nos activités et nos rythmes de pensée que nous pouvons voir apparaître de nouvelles choses. C’est en alternant les pensées rapides, instinctives, émotionnelles avec les pensées lentes et plus réfléchies que nous nous donnons la chance de ne pas sombrer dans les biais cognitifs propres à chaque mode de pensée23. Mais comment trouver ces équilibres ? Comment appréhender les métriques permettant de mesurer cette harmonie ? S’il est facile de chronométrer une tâche ou de compter des étapes, comment pouvons-nous capturer le côté profondément humain dans les dynamiques de leur exécution et de leur enchaînement ? Comment ne pas perdre le plus beau potentiel de valeur ajoutée, de créativité et d’innovation qu’est notre contribution en tant que personne ? Afin d’identifier ces mécaniques subtiles, je travaille avec des solutions d’IA qui vont être capables d’identifier ces signaux faibles et d’aider à prendre des décisions plus fines que celles à partir du simple « emporte-pièce » de réduction de temps et de gestes, déjà bien éprouvé par le passé. Les algorithmes vont nous aider à traiter une donnée beaucoup plus variée et représentative d’un contexte de travail, pour trouver l’optimisation entre la productivité pure et la culture de la valeur de chaque individu (voir chapitre 6, La force de notre humanité et de la diversité).",
          "Created At": "2025-09-06T13:22:34.108+02:00",
          "Updated At": "2025-09-06T13:22:34.108+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE110_1757157753085",
          "Status": "Ready For Refine",
          "Text": "# Chasser les processus Creeper\n\nLe terme Creeper fait référence à la plante grimpante ou à la liane qui s’enchevêtre autour d’un corps, ou encore aux créatures qui rampent lentement. Ce sont les processus qui, par leur complexité, leur sensibilité à l’erreur ou leur aspect manuel, nous minent au quotidien. Ils requièrent une concentration inutile et consomment beaucoup de notre temps et de notre énergie.\n\nPire encore, ces processus contraignent les collaborateurs de notre environnement à travailler avec une complexité que nous leur imposons et à eux-mêmes développer des usines à gaz pour s’interfacer avec les mécaniques creeper que nous créons. Progressivement, notre désinvolture dans la création de processus « lianes » peut impacter la performance d’organisation tout entière.\n\nUn exemple classique est celui de la template Excel. Cette pierre d’angle de la majorité des processus de reporting ou budgétaires est souvent créée avec l’objectif d’émuler une application de saisie, de contrôle de donnée, d’analytique et de reporting. Ces créations sont gorgées de formules, de colonnes et d’onglets cachés, de protections et de macros. Elles sont également conçues avec une certaine idée de l’expérience utilisateur. Si elles font la fierté de leurs concepteurs, elles sont bien souvent énergivores et chronophages pour les organisations :\n\n– leur maintenance est complexe et souvent maîtrisée par une seule personne ;\n\n– la saisie des données par les utilisateurs est complexe, répond à beaucoup de règles et se fait de manière manuelle, ligne à ligne ;\n\n– elles ne permettent pas d’import direct et massif de données : il faut passer par la saisie ;\n\n– les capacités d’analyse des saisies sont limitées. La template impose des pivots et des graphiques créés en central qui ne sont pas forcément concentrés sur les points pertinents en local ;\n\n– à vouloir accommoder dans un seul document collecte, stockage et reporting, nous abandonnons les principes FAIR de la donnée. Celle-ci est collectée et stockée de manière compliquée et rarement tabulaire, qui nécessite des mécanismes d’extraction et de consolidation complexes en contradiction avec les bases d’une donnée fluide.\n\nTout cela pourrait être si simple si la donnée était collectée sous forme de table, avec en amont et en aval de la chaîne des analystes capables d’auditer et d’analyser ces données suivant les méthodes que nous avons vues plus haut.\n\n→ La fameuse template des cours de change\n\nC’était un monument pour tous les départements finance de ce groupe mondial : le fichier Excel des cours de référence. Mis à jour chaque mois, ce tableau cosmétisé et optimisé pour une impression irréprochable était envoyé aux trente filiales. À première vue, cette centralisation de la diffusion de la donnée permettait son contrôle et son uniformité globale. Mais cette approche déclenchait des heures de travail dans chacune des trente équipes finance locales. En effet, une fois le document reçu, ces taux devaient être renseignés dans les systèmes locaux pour prendre effet. Mensuellement, c’était la tâche de trente analystes de saisir ligne à ligne ces chiffres, un doigt sur la feuille, une main au clavier. Avec les dix minutes de repointage et les corrections d’erreurs, c’était 30 x 45 = 1 350 minutes, soit plus de 22 heures qui étaient dépensées chaque mois pour une tâche sans aucune valeur ajoutée.\n\nLa simple mise à disposition d’un fichier plat sur le réseau, suivie d’un import ou ETL dans les systèmes locaux, aurait été quasi instantanée et sans risque d’erreur de saisie. Pourtant, cette mécanique de diffusion et de ressaisie a perduré pendant des années.\n\n» Prenez le temps de réfléchir à ces processus Creeper qui ralentissent votre quotidien. Décomposez-les en actions simples, appliquez les principes FAIR de la donnée et appuyez-vous sur des outils adaptés. Des dizaines d’heures d’économie vous attendent.",
          "Created At": "2025-09-06T13:22:34.110+02:00",
          "Updated At": "2025-09-06T13:22:34.110+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE111_1757157753085",
          "Status": "Ready For Refine",
          "Text": "# La valeur ajoutée des étapes du processus analytique\n\nToutes les phases d’un processus analytique ne se valent pas. De la question à l’action, vous pouvez découper votre chaîne en neuf grandes étapes, indépendantes de votre industrie ou de votre technologie.",
          "Created At": "2025-09-06T13:22:34.110+02:00",
          "Updated At": "2025-09-06T13:22:34.110+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE112_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## L’optimisation des neuf étapes d’un processus analytique\n\n\\> Neuf étapes vers le data nirvana\n\nAfin d’optimiser un processus analytique, vous devez en comprendre ses phases clés. Lors de la revue des systèmes et de la chaîne de traitement de la donnée, vous avez déjà constaté l’existence d’étapes invariables.\n\nFigure 15. Les neuf étapes du processus analytique\n\nComprendre la raison d’être de chacune de ces phases va vous permettre ensuite d’appliquer vos optimisations.\n\nRéfléchir\n\nNous avons vu que les organisations nous poussent à l’action et au mouvement comme synonymes de travail. Dans le contexte de l’analytique, vous ne pouvez pas vous ruer sur la construction de rapports sans au préalable savoir ce que vous cherchez. En faisant cela, vous entrez dans la logique « faire et défaire, c’est toujours travailler » qui peut être coûteuse en temps et en moyen, et qui va vous détourner d’une réflexion posée et pragmatique. Cette phase de réflexion est l’alpha de tout processus décisionnel. Elle va aboutir à la question pertinente qui vous servira de guide tout au long de la chaîne d’analyse.\n\nUne fois la question posée de manière pertinente, le processus peut continuer.\n\nIdentifier\n\nCette phase consiste à identifier les données nécessaires à une analyse complète et juste. Le choix des informations que vous allez collecter va influencer non seulement la largeur et la profondeur de votre analyse, mais aussi sa pertinence. En vous ruant sur des indicateurs mal choisis, vous pouvez obtenir des informations fausses ou, pire encore, insidieusement biaisées.\n\nCapturer\n\nL’objectif est de collecter de manière efficace et durable. Vous allez devoir avant tout chose sécuriser des sources de données numérisées et fluides. Si vos sources sont sur des formats papier, il faudra les mettre au format électronique (sous peine de consacrer des heures de ressaisie manuelle). Une fois cette phase essentielle réalisée, vous allez créer les processus de collecte qui vont alimenter votre chaîne d’analyse. Il s’agit à ce stade de créer des mécaniques capa­bles de récupérer votre information rapidement, de manière répétée si besoin et peu sensible aux fluctuations de volumes.\n\nPréparer\n\nC’est 80 % du travail du Pionnier de la data et de tout analyste en général. Les phases de profilage, de nettoyage, d’alignement, de modélisation, de précalcul sont essentielles pour rendre vos jeux de données propices à des analyses rapides, justes et pertinentes (voir chapitre 5, Tirer le meilleur de sa donnée avec une bonne préparation).\n\nStocker\n\nLe stockage va consister en la mise à disposition de votre donnée raffinée pour vos propres analyses ou pour des communautés plus larges d’analystes. Vous allez choisir les modes de stockage les plus adaptés à vos contraintes de volume, de rapidité et de complexité de calculs ou d’agilité de maintenance.\n\nAnalyser\n\nSur la base d’une donnée organisée, vous pouvez vous concentrer sur la recherche des réponses ou la validation de vos hypothèses. L’ajout de dernières retouches de calculs ou d’agrégation va vous permettre de détecter en tableaux ou en graphiques dynamiques ce que vous cherchez.\n\nPrésenter\n\nUne fois les éléments clés identifiés pour formuler votre réponse, vous allez devoir créer le média qui correspondra le mieux à votre audience. Les options sont multiples et ce qui doit vous guider, c’est l’impact. Sans une prise de décision ou une action, les six étapes précédentes n’auront servi à rien.\n\nCommuniquer\n\nIl s’agit de porter le message à votre audience et de le livrer de la manière la plus concise possible. Ce qui compte, c’est l’étape qui va suivre.\n\nAgir\n\nNous y sommes enfin. La promesse de « la visibilité pour l’action » (insight to action) enfin réalisée, le vrai travail commence. Votre expérience, votre expertise, voire votre intuition, qui avaient été clés à la première étape, se retrouvent de nouveau au cœur de cette phase. Votre contribution en tant que professionnel réfléchi, créatif ou inspiré va vous permettre de prendre les (bonnes) décisions et de mettre en œuvre les bonnes actions.\n\n\\> Les appuis techniques et méthodologiques à chaque étape\n\nMaintenant que vous avez détaillé les étapes de production de vos analyses, comme sur une chaîne de production industrielle, vous allez pouvoir réfléchir aux optimisations de chacune d’elles, en gardant en tête que votre processus ira toujours à la vitesse de son maillon le plus lent.\n\nLa priorité est de classer les tâches selon le niveau de valeur que vous apportez en tant qu’humain. Dans la figure 15, les étapes qui nécessitent votre contribution sont identifiées par les blocs avec les petites silhouettes.\n\nPour l’ensemble des phases, vous pouvez augmenter vos capa­cités en vous appuyant sur la chaîne technique décisionnelle et les méthodes décrites plus haut.\n\nOù apportons-nous une vraie valeur ajoutée ?\n\nParmi les phases qui nécessitent notre engagement, nous retrouvons la réflexion, ainsi que la décision et l’action. Même si nous automatisons le processus de décision et les actions qui suivent, n’oublions pas qu’à la base la machine devra être programmée pour suivre nos directives. Si nous négligeons la logique que nous codons dans la machine ou bien si la donnée que nous apportons aux algorithmes est biaisée, alors les décisions et les actions seront mauvaises.\n\nLa manière avec laquelle nous allons préparer nos datas est aussi clé pour la qualité de notre analyse.\n\nL’analyse à suivre est évidemment là où nous allons exprimer pleinement notre esprit critique.\n\nEnfin, pour la présentation de nos résultats, ce sont plus nos capacités de communicant ou de pédagogue qui vont être mises en tension.\n\nDes gains de performance seront immédiatement effectifs par la simple application de briques technologiques ou méthodologiques décisionnelles (voir chapitre 4, Comprendre les solutions BI ou décisionnelles) :\n\n– le Design Thinking va vous aider à formuler la bonne question en prenant en compte le maximum d’options et en limitant vos biais cognitifs ;\n\n– le Design Thinking encore va vous guider pour définir les meilleures options pour vos décisions et vos actions ;\n\n– la réflexion large, la collaboration inside out et outside in proposées par les approches Open Innovation va vous procurer l’inspiration et les idées dont vous allez avoir besoin pour mettre en pratique au mieux les conclusions de vos analyses ;\n\n– l’inclusion de la diversité dans vos équipes va vous aider à formuler vos questions de manière plus juste et moins influencée par vos biais culturels et sociaux ;\n\n– la technologie va également décupler votre puissance de feu :\n\n• pour la préparation et la modélisation de données, vous allez disposer des fonctions de profilage de vos solutions analytiques ou de certains langages tels que le Python ou R que vous pourrez apprendre progressivement pour devenir la Data Avant-Garde ;\n\n• pour l’analyse, les outils de Business Intelligence qui vont vous permettre de travailler sur des millions de lignes en quelques clics de souris ;\n\n• pour la présentation claire et impactante de vos conclusions, les solutions de visualisation avancées, souvent incluses dans les applications de Business Intelligence, vont vous permettre de visualiser de façon simple des contextes complexes ;\n\n• le Python ou le R vont venir briser les plafonds de volume et de complexité qui vont arriver aux limites des solutions traditionnelles. Soyez néanmoins rassuré, les solutions décisionnelles viennent sans difficulté à bout de plusieurs millions de lignes sur un ordinateur standard ;\n\n• la préparation de données va être prise en charge par des ETL agiles. Vous pourrez faire appel aux ETL couramment utilisés dans l’entreprise, s’ils sont accessibles et utilisables par des utilisateurs métier, aux ETL orientés utilisateurs tels qu’Alteryx, aux ETL intégrés dans les outils de Business Intelligence comme dans Pyramid Analytics, PowerBI ou Tableau Prep. Vous pourrez également automatiser vos traitements manuels par l’application de mini-programmes codés en SQL, Python ou R ;\n\n– une autre partie de la préparation, la modélisation de vos données, va pouvoir aussi bénéficier de solutions techniques. Si certains systèmes de bases de données ou leurs outils de gestion permettent la mise en œuvre rapide de modèles grâce à des interfaces graphiques, la plupart des outils BI intègrent la possibilité de développer en local des modèles orientés métier, dans l’attente d’une industrialisation par les services informatiques ;\n\n– enfin, pour l’homogénéisation, la normalisation et la mise en conformité de vos données, dernière facette de la préparation, l’ETL accompagné d’une solution de MDM ou de gestion de qualité de données va pouvoir prendre en charge une grosse partie des transformations de masse.\n\n» Prenez un peu de recul sur vos processus actuels et considérez combien de temps vous pouvez gagner.\n\nOù n’apportons-nous aucun réel avantage ?\n\nLa capture et le transport de la donnée de nos bases sources à nos bases décisionnelles (fichiers texte ou bases de données) sont une tâche mécanique que la machine pourra faire beaucoup mieux que nous.\n\nLe stockage de nos données, leur archivage, leur mise en commun ne sont pas non plus un bon investissement de notre temps.\n\nEnfin, la transmission d’un résultat, qui ne doit pas être confondue avec la discussion qui suit la réception du message, relève également d’une action assez simple et répétitive. Au sein d’une entreprise connectée, passer des heures à envoyer des rapports n’est plus acceptable.\n\nLa bonne nouvelle est que nous avons dans la pile des systèmes décisionnels des gisements d’automatisation et de productivité énormes :\n\n– vous l’avez certainement deviné, l’ETL va encore jouer un grand rôle dans la phase de capture de données. Il va vous permettre de récupérer des volumes massifs, aux heures choisies, de systèmes sources à des systèmes cibles en conservant l’intégrité de vos données ;\n\n– la base de données est bien évidemment l’outil parfait pour centraliser, conserver et mettre à disposition vos informations ;\n\n– enfin, les solutions de Business Intelligence intègrent aujourd’hui de nombreuses options qui vont éviter les longues listes de distribution d’e-mail :\n\n• l’accès à des rapports et à des tableaux de bord en ligne ;\n\n• la mise à disposition des classeurs d’analyses en tant que tels pour donner non seulement les résultats des analyses, mais aussi leurs détails ainsi que la possibilité de rebâtir si besoin des tableaux complémentaires. Les classeurs PowerBI ou Tableaux contiennent par exemple la donnée et les analyses, et permettent à une tierce partie de retravailler si besoin sur les chiffres ;\n\n• et si l’audience est toujours en attente de documents attachés à des e-mails, nous pourrons utiliser les fonctionnalités de diffusion automatisées de rapports filtrés des outils de BI sur le périmètre de responsabilités de chacun.\n\nDes gains de temps immédiats\n\nMis à part pour les phases de réflexion et d’action où l’arbitrage vitesse/qualité varie en fonction des urgences et des risques à gérer, imaginons les gains de temps que ces approches peuvent apporter à court terme. Ces chiffres reflètent de manière conservatrice mon expérience au contact des équipes que je forme.\n\nTableau 9. Temps comparés entre un processus tableur et un processus s’appuyant sur des outils et des pratiques analytiques FAIR\n\nEn général, les cas que je rencontre sont encore plus exemplaires.\n\n→ Le facteur 10\n\n– une société pharmaceutique, qui a formé des pionniers comme vous, a en l’espace de deux mois réduit de 10 heures des processus analytiques qui prenaient chaque mois 12,5 heures par la simple application des approches décrites dans ce chapitre, sur la base des compétences acquises dans cet ouvrage ;\n\n– un groupe automobile a transformé un processus d’une journée en un processus d’une heure en appliquant simplement les bonnes technologies sur une donnée rendue tabulaire ;",
          "Created At": "2025-09-06T13:22:34.111+02:00",
          "Updated At": "2025-09-06T13:22:34.111+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE113_1757157753085",
          "Status": "Ready For Refine",
          "Text": "– sur mes postes de contrôleurs, nous mesurions une division par dix des temps de traitement de processus d’analyse tableur, par l’application des approches FAIR, des bonnes technologies et par le courage de changer.\n\n→ La règle des 20-20\n\nÀ la suite d’un atelier dans une institution bancaire, j’ai observé une réduction de soixante fois le temps d’un processus analytique, passant de 20 heures par mois à 20 minutes. Imaginez que c’est comme participer à un marathon de 42,2 km et obtenir un avantage de 41,5 km sur ses pairs (ceux qui ont des processus manuels et basés sur des feuilles de calcul). Même le meilleur athlète ne pourra jamais nous battre. Avec une telle avance, nous aurons le temps de réfléchir à notre foulée, à notre respiration, de faire des selfies et de profiter de l’ambiance de la foule de supporters. Non seulement nous gagnerons à chaque fois avec une avance écrasante, mais nous aurions en plus le loisir de travailler à peaufiner notre avantage compétitif sur les prochaines compétitions. Cela semble presque injuste, n’est-ce pas ?\n\nDans le cas en question, Virginia, notre héroïne, était chargée de superviser la qualité des rapprochements bancaires de trente filiales du groupe à travers le monde. La diversité des systèmes et l’absence de normalisation des processus ne permettaient pas d’effectuer automatiquement un rapprochement complet en central. Même si elle était consciente que l’analyse n’apportait pas beaucoup d’informations ou de feedback opérationnel, elle devait la fournir chaque mois.\n\nPour sortir de cette routine peu valorisante, ce que Virginia a mis en place n’a rien de sorcier. Comme beaucoup d’analystes, elle n’avait jamais suivi de formation aux bases de la data. Elle a choisi d’acquérir les compétences « Pionnier » et a surmonté son appréhension du changement ainsi que son attachement aux tableurs qui faisaient partie de son quotidien.\n\nMais comment a-t-elle fait pour réaliser cette spectaculaire optimisation ? Comment a-t-elle pu passer de 20 heures de production de rapports systématiquement en retard à des analyses proactives établies en 20 minutes ?\n\nElle a transformé trente feuilles de calcul avec des onglets mensuels en une seule table de données, avec des dates et des identifiants bien définis. Elle a mis en place une collecte automatisée des données lorsque cela était possible ou a exploité les échanges de fichiers plats lorsque les interfaces étaient trop complexes à programmer. Elle a conçu des rapports automatisés qui pouvaient ensuite être envoyés non seulement à la direction de manière agrégée, mais aussi à chaque filiale comme aide à l’exploitation et à la décision. Elle a commencé à sécuriser d’autres données en rapport avec les entrées et les sorties des comptes bancaires, telles que les pays d’origine et de destination ou les personnes émettrices en interne. Elle a naturellement commencé à poser de nouvelles questions sur les risques, la fraude et la conformité, et a apporté une véritable valeur à son travail.\n\nEn quelques semaines à peine, elle avait déjà repoussé les limites de ses analyses et exprimait déjà le besoin d’appliquer des algorithmes de classification à son trésor de données.\n\nVirginia a découvert que les données sont comme n’importe quel matériau. Elles sont collectées sous différentes formes et doivent être transformées, assemblées et livrées à un destinataire. Elle a appliqué les principes de base qu’il faut connaître pour traiter les données correctement et avec efficacité, tels que :\n\n– la structure de la chaîne d’approvisionnement des données, du producteur au consommateur ;\n\n– la notion de tables, de clés et de jointure ;\n\n– les concepts de faits, de dimensions et de données de base ;\n\n– les bases de la gestion des données de base, de la transformation et de la qualité des données ;\n\n– l’usage des ETL pour capturer les bonnes données et fournir une préparation automatisée ;\n\n– les bases de données relationnelles et la modélisation des données ;\n\n– la gestion des Master Data.\n\nElle s’est ouverte avec courage à de nouvelles solutions. Jusqu’à présent, les feuilles de calcul avaient été son seul atout pour tous les défis analytiques. Elle a démystifié le battage médiatique autour du Big Data et a clairement identifié ce dont elle avait besoin d’un point de vue métier.\n\nElle est devenue une force de proposition pour les projets analytiques, car elle arrivait non seulement avec des questions métier claires, mais aussi avec des données classées et connectées qu’elle avait accumulées pour son analyse principale.\n\nVirginia avait entendu parler de la règle des 80/20 entre le traitement et l’analyse des données. Elle venait d’en créer une nouvelle : la règle des 20-20, où 20 heures d’analyses manuelles fastidieuses se transforment en 20 minutes d’informations à forte valeur ajoutée !\n\nRespectons-nous\n\nUn déclencheur de rationalisation de processus peut être aussi de poser un nouveau regard sur l’exécution de nos tâches quotidiennes en analytique.\n\nEn concentrant notre temps sur nos tableurs et nos processus manuels, la majorité de nos actions va consister à enchaîner des tâches peu valorisantes et passionnantes. Comme me l’avait décrit un confrère, non sans une pointe d’humour, une fonction d’analyste se résume assez bien par :\n\n– des copier-coller de formules et de données. Cette discipline est souvent le sport national en analytique. Cette aérobic pour nos phalanges n’est pas la meilleure utilisation de nos talents ;\n\n– des calculs relevant du cours élémentaire qu’avec application nous recopions tous les jours au travers de nos jeux de données ;\n\n– du coloriage glorifié représenté par nos longues séances de cosmétisation de tableaux et de graphiques, comme à l’époque de nos ateliers de peinture et de dessin à la maternelle.\n\nEst-ce raisonnable après des années d’études ou d’expérience d’en arriver là ? Aussi satisfaisantes ou rassurantes puissent être ces tâches, il y a de fortes chances qu’elles ne soient pas en phase avec notre fonction, notre salaire et notre amour-propre.\n\nSommes-nous prêts à quitter ce confort sclérosant et auto­limitant ?\n\nOsons le repositionnement homme/machine\n\nL’application de technologie à une donnée fluide n’a pas d’effet magique automatique. Un outil et une donnée FAIR ne feront gagner du temps que s’ils sont positionnés et utilisés correctement.\n\nSouvent, nos processus ont été bâtis avec des contraintes et des connaissances différentes de celles d’aujourd’hui. Si nous n’avions pas accès à des solutions pour une meilleure performance ou n’avions pas de motivation à travailler plus vite, alors rien ne nous contraignait à penser à la création de processus performants. La durée et la complexité d’un processus peuvent être considérées comme une fatalité immuable. De surcroît, l’habitude, les reprises de procédures à chaque nouvel arrivant sur le poste ajoutent peu à peu les couches de complexité et gomment progressivement le rationnel de certaines étapes de nos processus.\n\nL’implémentation d’une nouvelle solution ne peut être optimale si les fondamentaux du processus qu’elle couvre ne sont pas revus. La difficulté est que, lorsqu’une portion de chaîne d’analyse est modernisée, il faudra souvent ajuster :\n\n– le processus concerné par la nouvelle solution pour profiter pleinement de ces fonctionnalités plus performantes et également des nouvelles opportunités apportées, que nous n’avions jamais considérées auparavant ;\n\n– les processus amont et aval qui connectent avec le processus en question. En effet, si notre portion de processus devenue lean doit se connecter sur des processus manuels et à haut risque d’erreur, alors l’investissement n’aura qu’une portée limitée sur l’ensemble du processus.\n\nUne vraie optimisation de processus s’étend souvent bien au-delà de son point de démarrage, ce qui rend ces initiatives délicates, si nous voulons les mener de manière holistique. Non seulement nous allons devoir apprendre à travailler différemment, mais nos partenaires internes vont devoir aussi évoluer.\n\nL’âne et le camion\n\n  \nImaginons que nous sommes un fermier des années 1910 de Menlo Park et que, chaque semaine, nous nous rendons avec notre âne à San Francisco, 45 km au nord, pour vendre nos récoltes maraîchères. C’est une habitude, une tâche que nous, nos parents et nos grands-parents avons effectuée depuis des décennies.  \nUn jour, notre âne se faisant vieux, nous décidons d’utiliser l’automobile de notre voisin. Nous la rendons quelques heures après, sans qu’elle ait bougé. Cette machine est sans intérêt : elle n’avance pas avec la carotte, elle est très inconfortable quand on est assis sur le toit et les paniers d’osier n’ont pas d’endroit pour s’accrocher sur les côtés du véhicule. En plus, elle ne rentre pas dans l’étable !\n\nSi nous ne changeons pas notre processus, ainsi que nos habitudes amont et aval, nous n’allons pas pouvoir tirer le meilleur parti de cette innovation technologique.",
          "Created At": "2025-09-06T13:22:34.111+02:00",
          "Updated At": "2025-09-06T13:22:34.112+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE114_1757157753085",
          "Status": "Ready For Refine",
          "Text": "# L’accélération par le RPA (Robot Process Automation)\n\nCette pratique est devenue très à la mode depuis le début des années 2010. Elle consiste à mettre des robots pour automatiser les tâches administratives et analytiques.\n\nLes robots RPA vont réaliser de nombreuses actions simples à la place des utilisateurs. Ils vont pouvoir se connecter aux différentes sources de données ou applications, transférer des données ou des fichiers, et réaliser des opérations manuelles au sein de nos logiciels bureautiques tels que des copier/coller, des sauvegardes, des saisies des formulaires, etc.\n\nCette approche est très séduisante et très efficace, mais elle peut aussi se révéler avoir l’effet d’un cautère sur une jambe de bois, et être un pis-aller qui ne résout pas le problème fondamental du processus alambiqué que nous cherchons à améliorer.\n\nSi une chaîne analytique contient suffisamment de copier-coller et de manipulations de fichiers pour justifier un projet de RPA, c’est peut-être qu’il y a déjà trop de copier-coller et de manipulations au départ. Essayer de les confier à des robots logiciels va s’avérer une tâche délicate de programmation pour réaliser toutes les tâches manuelles et prendre en compte les subtilités des données.\n\nUne remise à plat du processus avec une approche telle que celle qu’appliquent les Pionniers peut largement suffire à atteindre les objectifs de performance souhaités. Par contre, si l’organisation n’a pas la capacité d’adaptation de ces processus et encore moins de ceux amont et aval, alors, ce pansement RPA sera la solution.\n\n→ La fameuse template des cours de change (suite)\n\nDevant l’ampleur du temps passé à ressaisir les données chaque mois, il fut décidé de mettre en place du RPA.\n\nPendant deux mois, une équipe de consultants et d’employés planchèrent sur l’automatisation du processus en développant un petit robot (littéralement affiché, dansant à l’écran lors de sa mise en action) capable d’ouvrir le fichier Excel, de choisir le bon onglet, de repérer les bonnes colonnes, d’ouvrir l’application cible et ensuite d’y copier-coller pendant une quarantaine de secondes les taux.\n\nLe coût du projet s’est monté pour la phase 1 à 200 000 euros. Il a toutefois rapidement montré des limites. La stabilité du processus n’était pas garantie : des machines locales n’arrivaient pas à supporter l’enchaînement. Certaines versions de systèmes d’exploitation n’étaient pas compatibles avec le robot. De plus, les fonctionnalités ne s’intégraient pas avec tous les systèmes du groupe.\n\nFinalement, les gains de temps ne se sont appliqués qu’à un tiers des filiales et chaque modification de la donnée centrale impliquait une reprogrammation coûteuse.\n\nUne refonte du processus n’aurait-elle pas été plus simple et moins onéreuse ?",
          "Created At": "2025-09-06T13:22:34.112+02:00",
          "Updated At": "2025-09-06T13:22:34.112+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE115_1757157753085",
          "Status": "Ready For Refine",
          "Text": "# L’optimisation des processus analytiques",
          "Created At": "2025-09-06T13:22:34.113+02:00",
          "Updated At": "2025-09-06T13:22:34.113+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE116_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## L’excellence est dans la répétition\n\n« L’excellence est un art que l’on n’atteint que par l’exercice constant. Nous sommes ce que nous faisons de manière répétée. L’excellence n’est donc pas une action mais une habitude. » Aristote.\n\n\\> Répéter pour apprendre\n\nL’excellence d’un processus ne se décrète pas sur un diaporama ou un guide de procédure. Il n’est pas garanti pas l’observation stricte des bonnes pratiques. Ancré dans des principes évidemment sains et orienté par des objectifs clairs, un processus va devoir faire l’objet d’une amélioration continue. Il devra tenir compte des imprévus, des nouvelles découvertes, des nouvelles solutions que nous ajoutons ou des évolutions de besoin ou de priorité. Définir ex nihilo et une fois pour toutes des processus analytiques est souvent une gageure, devant la complexité et la diversité des éléments humains, techniques et data qui pourront survenir par la suite.\n\nComme pour les implémentations de solutions techniques, commencer petit, procéder par itération et ajuster en faisant est une recette infaillible pour définir et mettre un processus sur le rail de manière durable.\n\n\\> Répéter pour optimiser\n\nUne fois lancé, il sera nécessaire de conserver une récurrence suffisante pour ne pas perdre la main. Un processus réalisé trop rarement va être difficilement optimisable. Si cette tâche a véritablement une valeur ajoutée, il ne faut pas hésiter à en augmenter la fréquence. Cette accélération des rythmes de publication va permettre de démarrer des cercles vertueux avec :\n\n– l’acquisition de réflexes de traitement. À la manière d’un sportif, plus vous allez répéter les gestes et plus vous allez les réaliser vite et sans effort ;\n\n– la maîtrise des données entrantes avec une période plus courte à analyser à chaque itération. Revenir sur trois ou six mois de données sera plus délicat que revenir sur les trente derniers jours ;\n\n– la correction des données ou des traitements plus fréquents. Vous allez progressivement identifier les erreurs qui se nichent dans vos processus et, itération après itération, vous allez pouvoir les corriger, tendant ainsi vers le zéro défaut.\n\nVous allez alors arriver au paradoxe des processus analytiques lean : un processus effectué mensuellement peut finalement prendre moins de temps qu’un processus trimestriel. Une fois la contrainte initiale de la fréquence passée, qui va être coûteuse en temps et en effort, vous allez mettre en route un moteur d’optimisation et entrer dans une phase de gain croissant et surtout de visibilité accrue sur votre activité.\n\n\\> Répéter pour se tromper\n\nÀ moins d’appartenir au cercle restreint des individus qui ne se trompent jamais ou qui font bien du premier coup, votre parcours analytique sera immanquablement un parcours d’erreurs et de corrections. Se tromper et avoir à recommencer est frustrant de prime abord, mais c’est aussi une opportunité pour répéter une tâche et réfléchir à son exécution optimale. Si nous nous trompons dix fois, nous ferons le geste dix fois et, tout en apprenant à le faire naturellement, peut-être nous poserons-nous la question de comment le faire de manière plus simple, confortable, en un mot, de manière optimisée.\n\n→ Prendre le coup de main\n\nLors d’une présentation logicielle, cette directrice financière regarde faire le démonstrateur. Faute de pouvoir travailler sur sa machine habituelle, le processus présenté « bugue » à plusieurs reprises. Puis ce sont des problèmes réseau qui viennent arrêter le déroulement de la session.\n\nConfus, navré, le commercial se confond en excuses. Il vient de recommencer la démo pour la nième fois, progressant chaque fois un peu plus, mais n’arrivant pas au bout, pour des raisons techniques indépendantes. À sa surprise, la directrice financière lui répond : « Ne vous inquiétez pas, en vous regardant faire, je commence à prendre le coup de ce qu’il faut faire. Je vois exactement comment la solution va s’insérer dans mes processus. »",
          "Created At": "2025-09-06T13:22:34.113+02:00",
          "Updated At": "2025-09-06T13:22:34.113+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE117_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## L’organisation des contrôles et les check-lists\n\nAvec l’automatisation des processus analytiques, nous allons progressivement avoir moins de visibilité détaillée sur ce qu’il se passe à chaque étape. En théorie, ce n’est plus un problème de ne plus voir chaque ligne et chaque colonne de chaque jeu de données si nous savons que leur traitement sera fait par la machine, sans erreur.\n\nCe qui demeure à contrôler est néanmoins :\n\n– ce qui arrive à l’entrée du processus et ce qui en sort par comptage ou sommation des lignes entrantes et sortantes ;\n\n– si les données maîtres et les données de référence sur lesquelles vont s’appuyer les préparations de données sont correctes et complètes par profilage ;\n\n– si les calculs et les algorithmes ne contiennent pas d’erreur ;\n\n– si les résultats des traitements sont cohérents par profilage et test aléatoire.\n\nDans le prolongement de votre logique d’efficacité, vous allez vous appuyer sur les rapports d’exécution des différentes solutions de vos chaînes analytiques :\n\n– les rapports d’ETL vont vous apporter une information précieuse sur les volumétries captées et chargées ;\n\n– des rapports analytiques automatisés vont vous permettre de compter et de valider les formats de vos données pour éviter l’oubli d’information qui pourrait ne pas avoir été remontée dans les systèmes ;\n\n– des revues analytiques vont suivre la cohérence des données maîtres ou de référence d’une période sur l’autre : lesquelles sont les nouvelles, lesquelles ont disparu, quelle est leur valeur par rapport aux périodes précédentes, etc. Des rapports bâtis une fois pour toutes pourront vous donner cette vue instantanément ;\n\n– les résultats de tests de jeux de données, recommandés principalement dans les premières itérations du processus. Vous allez valider le processus en suivant une donnée de bout en bout et en validant transformations et calculs. La sélection de ces données test pourra se faire de manière aléatoire ou être concentrée sur des données sensibles.\n\nIl est à noter que, compte tenu de la puissance de calcul des outils actuels, vous allez pouvoir faire porter ces contrôles sur 100 % des données. Vous pourrez vous référer au chapitre 5 sur le profilage pour plus de détail sur les techniques d’analyse de la qualité de données.\n\nLes check-lists sont le média idéal pour centraliser ces contrôles et valider que :\n\n– les tâches ont bien été réalisées par les machines ;\n\n– les rapports d’exécution (logs) ont été contrôlés sur leurs points clés ;\n\n– les rapports d’analyse de profilage ont été revus et visés par un opérateur.\n\nEn quelques minutes, vous aurez validé la qualité des données entrantes, l’exécution des traitements de préparation et la cohérence des paramètres utilisés pour ces derniers.\n\nPar itération, vous allez progressivement raffiner vos contrôles ou ajouter des tests et, en quelques itérations, vous serez en mesure d’avoir l’assurance de la qualité de votre processus et surtout de démontrer que vous avez mis en place toutes les diligences. En plus, valider une check-list, c’est aussi matérialiser que les diligences nécessaires ont été faites.\n\nIl restera bien sûr la validation des données de fait, ces chiffres saisis par les opérateurs ou remontés par d’autres équipes. C’est ce que la phase d’analyse permettra de faire. Avec les heures gagnées par l’automatisation des contrôles, vous pourrez enfin vous y consacrer de manière plus approfondie.",
          "Created At": "2025-09-06T13:22:34.114+02:00",
          "Updated At": "2025-09-06T13:22:34.114+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE118_1757157753085",
          "Status": "Ready For Refine",
          "Text": "# Extraire les trésors de l’intelligence collective",
          "Created At": "2025-09-06T13:22:34.115+02:00",
          "Updated At": "2025-09-06T13:22:34.115+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE119_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## La dynamique de collaboration\n\nLes processus d’analyse ont tous leur dose de pression, d’urgence, de concentration. Si nous les ajoutons à nos biais, à nos erreurs, à nos omissions et à nos prédispositions pour certaines tâches analytiques plutôt que d’autres, nous avons un cocktail assez propice aux problèmes.\n\nCes risques croissent d’ailleurs significativement lorsque les processus sont complexes et manuels, comme peuvent l’être des processus tableur.\n\nAfin de pallier ce problème, vous pouvez :\n\n– vous reposer sur vos qualités de concentration, d’attention et de minutie ;\n\n– passer plus de temps, travailler plus lentement et vérifier scrupuleusement les détails de chaque traitement.\n\nCes deux attitudes vont permettre de grandement limiter les risques d’erreur, mais elles vont le faire au détriment de la performance du processus et surtout de la réflexion stratégique autour du problème. Si nous sommes constamment dans les détails, comment pouvons-nous réfléchir de manière globale ou innovante face à un problème ? De plus, nous n’avons pas tous les mêmes prédispositions pour l’attention aux détails.\n\nPlus que s’en remettre à sa seule expérience pour assurer toutes les facettes de compétences stratégiques et tactiques, s’appuyer sur une diversité de collaborateurs va permettre d’assembler le portefeuille de compétences requis pour des analyses pertinentes.\n\nEssayez de travailler en binômes, voire en trinômes, pour traiter chaque aspect du processus analytique de la meilleure façon. Choisissez de vous associer avec des collègues dont les qualités et les centres d’intérêt sont complémentaires aux vôtres. Ce sera de surcroît un parfait antidote pour les effets Dunning Kruger (voir chapitre 6, L’effet Dunning Kruger de l’analyste).\n\nSi vous avez une préférence pour la réflexion plus globale, associez-vous avec des talents qui ont une passion pour les détails. Si vous êtes plus attiré pour la résolution de problèmes techniques et l’optimisation de processus, travaillez avec des professionnels ancrés dans la réalité du terrain.\n\nCette approche s’éloigne des stratégies de recrutement des années 2010, où des groupes se concentraient uniquement sur le recrutement de MBA, grandes écoles pour toutes les tâches analytiques. Les équipes se retrouvaient alors avec des armées de jeunes talents, âgés de 21 à 25 ans, sans grande expérience opérationnelle, issus des mêmes environnements sociaux et culturels, hommes, caucasiens pour la plupart. Si, sur le papier, la surenchère sur l’excellence académique peut sembler légitime, dans le contexte analytique, elle est assez contre-productive.\n\nAvec uniquement des analystes surdiplômés fondus du même moule, nous nous retrouvons avec les mêmes façons de penser, les mêmes ambitions concurrentielles, le délaissement des tâches moins nobles et pourtant clés (telles que la préparation de données) ou le pointage de détail. Cela engendre des processus analytiques déséquilibrés, avec des trous sur les phases que ces équipes ne daignent pas traiter.\n\nLa fiction dépasse la réalité ?\n\nS’il nous fallait uniquement les meilleurs des meilleurs pour réussir, comment ont fait L’Agence tous risques, Scoubidou, Le Club des 5, L’équipe de France victorieuse de la coupe du monde de 1998, le village d’Asterix, Tintin et ses amis, Harry Potter pour venir à bout de toutes leurs aventures ?\n\nN’est-ce pas l’alchimie de la diversité des personnages qui permet au groupe de prévaloir ?\n\n» Recherchez dans vos films, vos livres préférés ou vos expériences personnelles quand se sont manifestées ces dynamiques de collaborations inclusives ?\n\nLe courage que nous avons évoqué au chapitre 6 sur les facteurs humains va devoir être employé pour s’associer avec des individus « différents ». Nous allons devoir oser exposer nos résultats, nos logiques, nos processus, nos erreurs à des vues différentes. Cela va nous sembler vexant d’être pris en défaut sur une faute d’étourderie ou une erreur de calcul. Mais, au lieu de prendre cet exercice comme un jugement de notre travail, considérons-le comme une collaboration pour améliorer un processus analytique. Coupons le cordon ombilical qui nous unit à notre reporting pour nous concentrer sur sa finalité et non sa forme.\n\n→ Merci Marie\n\nJ’ai découvert l’importance de la diversité en analytique avec Marie, une de mes premières collaboratrices.\n\nAvec plus de trente ans d’ancienneté dans la société, Marie avait acquis une connaissance des rouages de l’organisation sans pareil. Elle avait aussi un sens aigu du détail et une patience inépuisable pour certaines tâches de précision. Ces qualités, que j’ai travaillées avec le temps, notamment avec l’astrophotographie, me manquaient beaucoup à l’époque. De plus, nous étions en pleine reconstruction (révolution) de notre analytique et en situation de crise : mon esprit était accaparé par des défis techniques, opérationnels, politiques des nouveaux choix en Business Intelligence.\n\nPendant des années, nous avons peaufiné nos synergies. Nos progrès ont permis le bouclage de projets reconnus dans plusieurs grands médias dont le Financial Times. Ce fut pour moi un déclic.\n\nNous considérions nos analyses et nos reportings récurrents comme des outils de réflexion et non des fins en soi. Nous travaillions dans un but commun, sans revendication de propriété d’un rapport et nous n’avions aucun problème à se faire corriger l’un par l’autre. Nous avions conscience de nos points faibles respectifs et n’étions jamais dans le jugement de l’un ou de l’autre : nous faisions chacun du mieux possible.\n\nCombien de fois Marie m’a-t-elle pris en défaut, corrigé des erreurs, ou challengé un résonnement ! Combien de fois ai-je gentiment repris Marie pour des tâches mal optimisées, ou l’ai-je engagée sur des évolutions de processus ?\n\nNous savions nous sortir l’un l’autre de nos zones de confort et nous nous rappelions régulièrement à l’ordre pour corriger nos faiblesses.\n\nNotre faculté de remise en cause va aussi être testée par ces collaborations. La récurrence des processus analytiques contribue à notre excellence, mais aussi à notre complaisance. À force de faire des choses, nous perdons du recul, la routine s’installe et notre jugement ou notre attention s’altèrent. Les collaborateurs que nous invitons dans notre processus apporteront naturellement le regard critique qui s’estompe progressivement de notre côté.\n\n→ Quel est le point commun entre un cockpit d’avion de ligne, une salle de chirurgie et… une équipe d’analystes performante ?\n\nL’aviation et la médecine ont rapidement reconnu les avantages de collaboration saine au sein d’équipes de travail. Les conséquences dramatiques d’erreurs ont certainement poussé à réfléchir et à formaliser des protocoles dans ce domaine. Même si un commandant ou un chirurgien dirige, il est crucial que leurs équipes puissent pointer des omissions, des erreurs ou présenter des idées.\n\nLe plus gros crash de l’aviation civile, dans les îles Canaries, illustre les risques d’une relation hiérarchique trop forte qui neutralise la conversation et la critique. Dans ce drame, ce sont deux Boeing 747 qui se sont percutés sur la piste, tuant 585 personnes. À la suite d’une série d’événements improbables qui s’enchaînent, les deux avions se retrouvent sur un aéroport sous-dimensionné et sous-équipé. Le commandant du premier Boeing, fort de caractère, pressé par le temps et excédé par la situation qui les a amenés là (je vous recommande la lecture des détails de ces événements dans les nombreux articles sur le sujet) va choisir d’engager un décollage en plein brouillard, sans visibilité sur le bout de la piste. Agacé, à bout, il va le faire sans respecter les protocoles de confirmation de décollage. Son copilote, qui note le problème ne va toutefois pas oser suffisamment fermement faire part de son doute sur l’autorisation de décollage. Entre la communication radio dégradée avec une tour de contrôle dépassée par la situation et la figure autoritaire du commandant, alors figure de premier plan dans la communication de la compagnie aérienne, le copilote ne va pas contester la décision de mettre les gaz. Le Boeing, rechargé en kérosène, finira sa course en frappant de plein fouet l’autre Boeing toujours en manœuvre à l’autre bout de la piste.\n\nCes drames, liés à la confusion entre hiérarchie, compétence et raison sont aussi malheureux en médecine comme lors de cet incident sur une patiente dans un hôpital anglais. La victime décédera des suites d’une anesthésie suivie d’un arrêt de ses fonctions respiratoires, à peine la chirurgie commencée. Dès le problème constaté, le chirurgien et l’anesthésiste avaient pourtant déroulé les protocoles nécessaires. La patiente ne réagissant pas, ils avaient fait appel à des confrères experts présents dans les locaux. Les tentatives d’intubation n’aboutissaient pas et la tension était montée rapidement. Une équipe des meilleurs spécialistes s’affairait encore autour de la patiente quand elle a rendu son dernier souffle. Aucun d’entre eux n’aura fait attention à l’infirmier trop timide pour s’imposer, qui avait préparé ce qui aurait certainement sauvé la victime : un kit de trachéotomie.\n\nLa majorité de nos analyses ne mettent pas entre nos mains la vie de patients ou de passagers, mais nous pouvons beaucoup appren­dre de ces disciplines et de leur gestion de processus.\n\nApprenons à encourager la collaboration bienveillante. Osons exposer nos faiblesses pour mieux les compenser avec l’aide de nos collaborateurs.",
          "Created At": "2025-09-06T13:22:34.115+02:00",
          "Updated At": "2025-09-06T13:22:34.115+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE120_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Le cercle vertueux Analytiques terrain – Informatique – Data Science\n\nUne session de travail Métier-Data Science-Informatique sans une culture data dans l’entreprise.\n\n\\> Un passé d’initiatives déconnectées\n\nLes synergies interpersonnelles doivent être complétées par des synergies interéquipes. Cela apparaît comme une évidence, mais nous continuons à voir des schémas de fonctionnement non propices à des collaborations allant au-delà de la relation client/fournisseur entre les différents départements intervenant sur la chaîne analytique.\n\nLes vagues de digitalisation successives ont vu par exemple le lancement de programmes souvent déconnectés :\n\n– les formations ou l’acculturation des équipes dirigeantes sur des approches de pointe à travers des formations exécutives ou des séries de conférence ;\n\n– les investissements lourds en nouvelles technologies par les services techniques ;\n\n– la création ex nihilo d’équipes digitales ou de Data Science ;\n\n– l’équipement de tous les collaborateurs avec des outils BI non maîtrisés ;\n\n– la prise de position affichée de faire du digital par l’achat de technologies gadgets dans les équipes terrain.\n\nCes stratégies manquent l’objectif principal de la transformation digitale : gagner en temps et en visibilité pour développer l’entreprise de manière durable. Elles ne contribuent pas à bâtir une culture data universelle (data culture at scale) dans l’entreprise, qui permettra à toutes les forces vives de l’organisation d’apporter leur valeur. Elles se concentrent sur chacun des étages de la pyramide de l’organisation data sans se préoccuper des courroies de transmission entre les niveaux. Elles restent sur le modèle : l’informatique choisit la technologie, les équipes data font les analyses et la majorité des salariés de l’entreprise sont cantonnés à être servis en analytique, sans vraiment contribuer.\n\nComment cette structure va-t-elle pouvoir changer de dynamique ? Le démarrage de cette dernière ne pourra pas être décrété par la création d’équipes dédiées ou l’investissement en matériel : ce sont les data pionniers, nous tous, qui allons être la fondation de cette transformation.\n\nFigure 16. Dynamique synergétique d’une data culture at scale\n\n\\> Le Pionnier catalyseur\n\n« Avec de grands pouvoirs viennent de grandes responsabilités. » Oncle Ben à Spider Man\n\nÉquipé des compétences partagées dans ce livre, le Pionnier acquiert une nouvelle autonomie. Au quotidien, nous pouvons mettre à jour nos analyses, oser poser de nouvelles questions et bâtir des chaînes analytiques légères et agiles qui vont pouvoir traiter la majorité des problèmes posés au quotidien. Nous allons appuyer nos initiatives sur les fondements d’une donnée fluide et durable en travaillant à partir de tables et d’outils décisionnels. Ce comportement responsable et professionnel va créer de nouvelles relations avec les équipes data et informatiques qui supportent les métiers :\n\n– nous sommes des interlocuteurs éclairés, qui connaissent les limites et les opportunités de la technologie ;\n\n– nous travaillons notre donnée avec des normes et des méthodologies compatibles. Même si nous devons travailler en local indépendamment des systèmes centraux pour des raisons pratiques, nous savons conserver notre donnée fluide et nos processus analytiques clairs ;\n\n– nous venons demander du support technique moins souvent et, quand nous le faisons, nous sommes souvent matures sur notre problème et avons probablement déjà testé de nombreuses options ;\n\n– nous savons nous aligner sur une gouvernance de la donnée que nous comprenons et à laquelle nous contribuons ;\n\n– nous ne sommes plus attentistes ou renfermés, nous sommes devenus de vrais partenaires des équipes support.\n\n\\> Les équipes data et informatiques libérées\n\nLes équipes techniques vont (enfin) échanger avec des interlocuteurs qui connaissent les bases de l’analytique :\n\n– elles vont être engagées sur des projets initialement testés par les métiers et dont la valeur a été validée ;\n\n– elles vont reprendre des maquettes ou des processus propres, facilement transposables dans leurs solutions ;\n\n– elles vont pouvoir profiter de vrais débats sur des arbitrages que les Pionniers comprennent enfin (limites transactionnel/décisionnel, modélisation et jointure, technique ETL, etc.).\n\nMoins sollicitées et interrompues par des demandes pas toujours pertinentes, les équipes support vont pouvoir se concentrer sur leur mission à valeur ajoutée ou sur les défis de demain tels que :\n\n– la cybersecurité et la protection des données ;\n\n– la mise à l’échelle des infrastructures et la maîtrise des coûts pour relever les défis d’intelligence artificielle de demain ;\n\n– la gouvernance sur des données de plus en plus variées ;\n\n– le respect de la vie privée dans le stockage et le traitement des données.\n\nEn tandem avec les métiers, les équipes data et informatiques vont permettre le développement de recherche et d’analyses toujours plus ambitieuses. Pionniers, informatique et équipe data vont pouvoir se tourner vers la Data Science avec non pas des illusions en tête, mais des questions très précises, soutenues par une donnée claire et des systèmes puissants.\n\n\\> La Data Science impactante\n\nGrâce aux synergies Pionnier/informatique/équipe Data, les équipes de Data Science vont enfin bénéficier :\n\n– de questions métier intéressantes ;\n\n– de la donnée organisée et propre ;\n\n– de systèmes performants ;\n\n– de partenaires techniques et métier qui travaillent en équipe.\n\nC’est la fin du problème récurrent des équipes de Data Science qui parcouraient les couloirs à la recherche de projets et qui finissaient inévitablement par construire une maquette (proof of concept) sans réelle application.\n\nAccompagnées par toute l’entreprise, ces équipes peuvent bâtir des analyses avancées pertinentes, moins biaisées et surtout comprises par les acteurs concernés. Les scientists deviennent une partie intégrante de la chaîne.\n\nLa culture à l’échelle de l’entreprise est là.",
          "Created At": "2025-09-06T13:22:34.116+02:00",
          "Updated At": "2025-09-06T13:22:34.116+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE121_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Vers une intelligence de l’essaim\n\n« Ce n’est pas la queue qui remue le chien, mais l’inverse. » Anonyme\n\nBâtir une culture large de la donnée, c’est offrir à chacun le bénéfice de la visibilité sur son activité et son environnement, et surtout le luxe du temps pour se poser, réfléchir et innover.\n\nQuand chacun des collaborateurs gagne en plus la capacité quasi instantanée, massive et sans effort de partager ses informations et ses analyses, alors il se crée naturellement un maillage de synergies sans limites.\n\nAucun contributeur individuel ne peut remplacer le fourmillement d’idées de data pionniers opérationnels débarrassés de leur limitation analytique et de leurs biais.\n\nL’autre vertu de l’intelligence collective est que, plus que jamais dans notre société de diversité, nous avons tous un rôle à jouer.\n\n------------------------------------------------------------------------\n\n23. Lire l’ouvrage de Daniel Kahneman, Prix Nobel d’économie, Les deux vitesses de la pensée (Thinking, Fast and Slow) (Flammarion, 2012).\n\nConclusion\n\nLe Pionnier part à l’assaut de nouveaux défis ! Il va leur manquer.\n\nNous venons de couvrir les bases essentielles pour avancer en confiance dans le monde de l’analytique. Vous avez tout ce qu’il faut pour réussir et prendre votre rôle et vos responsabilités de pionnier de la data.\n\nVotre aventure entre désormais dans une nouvelle phase : celle de la pratique et de l’amélioration continue. Forts de vos compétences, vous allez devoir vous reposer sur ces mêmes valeurs qui vous ont fait investir ce temps de lecture et d’apprentissage : le courage, la curiosité, la détermination.\n\nUne aventure de découverte et d’apprentissage vous attend et il vous faudra du cœur pour avancer, faire face aux obstacles techniques, data et humains. Ce chemin sera une succession d’essais, d’erreurs, d’échecs qui vous mèneront aux victoires, qui ouvriront de nouveaux défis ou poseront de nouvelles questions.\n\nIl vous faudra aussi du cœur pour vous attaquer aux longues phases de préparation et de nettoyage de données ou encore aux tests de vos processus pour vous assurer de leur stabilité. Même si ces tâches peuvent être réparties ou déléguées, il vous incombera de vous investir dans ces dernières. En tant qu’analystes responsables, nous nous devons de travailler en amont sur la qualité et la fluidité de la donnée : notre responsabilité ne peut plus se limiter à notre pré carré. En plus de votre maîtrise des outils analytiques, la qualité de vos résultats se joue autant dans votre travail avec vos contributeurs que dans votre influence sur vos lecteurs et dans vos échanges avec l’informatique. N’invoquons plus systématiquement leurs erreurs en cas d’échec : la réussite de vos analyses est de votre responsa­bilité.\n\nLe cœur sera également le moteur des qualités requises pour devenir un pionnier de la data. Plus qu’une intelligence brute ou académique, ce sont les « quotients » collaboration, curiosité ou encore empathie dans lesquels vous devrez puiser. Les règles techniques et data sont établies : la grande inconnue sera votre environnement organisationnel et humain.\n\nNous devrons également sortir de l’obsession digitale pour ne pas oublier que les vrais défis ne sont pas la data, mais bel et bien ceux de l’excellence opérationnelle, décisionnelle, de l’innovation et de la réinvention continue de nos métiers dans un monde en accélération perpétuelle.\n\nVotre humanité, enfin, sera votre plus précieuse alliée pour garantir une data éthique, humaine et juste. Les lois, les règlements ou le politiquement correct ne seront jamais suffisants pour s’ériger en garde-fous des abus de la data. Les options que vous choisirez devront l’être avec nuance. En complément des lois, vous devrez vous fonder sur votre culture, votre expérience et vos valeurs de respect, d’inclusion, de liberté. C’est un défi énorme qui se pose à nous tous.\n\nComprendre ou maîtriser les machines et le traitement de données est une priorité aujourd’hui pour que nous puissions nous consacrer beaucoup plus à éviter que 1984 soit demain.\n\nCe livre est une humble contribution pour donner à tous les clés pour réussir nos premiers pas dans le monde de la data. C’est notre responsabilité de pionnier de contribuer à cette data éthique, pertinente et humaine.\n\nÀ vous de jouer ! Rejoignez les data pionniers !\n\nANNEXES  \nTémoignages de data pionniers et paroles d’experts",
          "Created At": "2025-09-06T13:22:34.117+02:00",
          "Updated At": "2025-09-06T13:22:34.117+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE122_1757157753085",
          "Status": "Ready For Refine",
          "Text": "# Hayat Kuna, Pierre-Antoine Bolmont, Benjamin Butez",
          "Created At": "2025-09-06T13:22:34.117+02:00",
          "Updated At": "2025-09-06T13:22:34.117+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE123_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Témoignages croisés de data pionniers\n\nHayat Kuna est manager de la transformation digitale chez Atos. Elle intervient depuis plus de quinze ans sur des problématiques de transformation des entreprises dans le marketing et la supply chain, dans le secteur industriel ou public. Elle participe également au développement de la formation interne.\n\n« J’accompagne mes clients pour mettre en place des solutions digitales ou favoriser l’acculturation digitale. Dans mes missions, le management de la data prend une place de plus en plus importante, d’où la nécessité de bien gérer les données avec le respect des réglementations RGPD (Règlement général sur la protection des données) également. »\n\nPierre-Antoine Bolmont a six ans d’expérience chez Decathlon : quatre en magasins, puis deux en service RH international. Il a travaillé notamment pour le déploiement international d’outils de formation et de montée en compétences. Aujourd’hui, il est chef de projet People Data, le projet de data RH de Decathlon au niveau international, et leader des référentiels métiers et compétences.\n\n« J’accompagne l’acculturation data au sein de notre domaine RH toujours au niveau international. »\n\nBenjamin Butez, de formation métier RH avec de fortes appétences pour l’analytique et le digital, travaille depuis une dizaine d’années sur la mise en place de processus de reporting dans le domaine des ressources humaines.\n\n« Je travaille à la conception de solutions analytiques plus agiles et performantes, pour mieux accompagner le développement des talents de l’entreprise et présenter par l’exemple le potentiel d’une data maîtrisée. »\n\n\\> Quels sont les défis de la transformation ?\n\nH.K. Lors de projets d’accompagnement de transformation digitale ou de mise en place de solutions digitales, nous pouvons nous sentir limités pour discuter et challenger sur des problématiques de données entre métier et informatique par manque de vocabulaire et de connaissance sur comment bien gérer la data.\n\nP.-A. B. L’un de nos plus grands défis est d’engager toutes les parties prenantes du problème à résoudre. Il faut prouver et mettre en avant les bénéfices pour que chacun ait l’envie d’être acteur et promoteur de la démarche. Si un coéquipier comprend et identifie l’intérêt de récolter, de partager et de maintenir de la donnée fiable pour lui, il sera très facile de soutenir et de maintenir la démarche dans le temps.\n\nB. B. Les freins principaux qu’on retrouve sur le chemin de la mise en place d’une culture de la data sont à mon sens la fiabilité des données et la facilité d’utilisation. Il est essentiel de prendre en compte ces deux enjeux dans la construction des outils et des datas mis à disposition des équipes.\n\n\\> Quelles sont les missions du data pionnier qui vous tiennent à cœur ?\n\nChanger pour grandir\n\nB. B. Les pionniers doivent intégrer les nouveaux outils et les nouvelles méthodes pour optimiser leurs processus de reporting. Ils doivent progressivement abandonner les approches tableur, pour travailler avec des outils spécialisés dans le traitement ou la visualisation des données. Concrètement, cela permet de remplacer l’exécution de tâches répétitives et manuelles à fort risque d’erreur. L’évolution des processus apporte aussi un gain de temps de production et une meilleure qualité des analyses.\n\nChallenger pour progresser\n\nH.K. Le data pionnier doit faire le lien fonctionnel entre les métiers et l’informatique. Il doit pousser ou faciliter la formulation de questions pertinentes et contribuer à leur bonne interprétation par les services informatiques et data. Il va travailler aussi en amont sur la data pour vraiment identifier celle qui apportera de la valeur au métier, pour ses analyses, ses processus ou ses applications.\n\nStructurer pour unifier\n\nP.-A. B. Il est aussi primordial de connaître et de maîtriser un langage commun autour de la data et de le diffuser le plus largement possible. Le data pionnier doit prendre le temps pour poser les fondements du Master Data Management. Il doit connaître le niveau de qualité de ces données utiles et le maintenir à un haut niveau. Il devra savoir communiquer l’importance de la gouvernance de la donnée et anticiper les difficultés d’implémentation potentielles, sous peine de rendre caduque l’ensemble de la démarche.\n\nRester humain\n\nP.-A. B. L’intuition et l’expérience acquises puis la data permettent de garantir une prise de décision humaine et éclairée. Promouvons une data humaniste et non robotique. Il convient de ne pas réduire la décision finale via le prisme unique de la data. Être 100 % data-driven n’aurait que peu de sens et la force de ces démarches en serait fortement réduite. Tout est question d’équilibre !\n\n\\> Quelles révolutions apportent les data pionniers\n\nGagner du temps pour se consacrer aux nouveaux défis\n\nB. B. La prise de conscience générale sur l’importance de la data est venue bouleverser la manière dont les directions RH gèrent les données des collaborateurs. Historiquement, la grande majorité du reporting RH consistait à produire du reporting réglementaire. Désor­mais, de plus en plus de dirigeants demandent de produire et de suivre du reporting opérationnel afin de piloter l’activité avec des indicateurs clés. Même si les principaux indicateurs RH ont peu changé (suivi des effectifs, absentéisme, turnover, etc.), c’est leur analyse qui a évolué. En tant que Pionnier, je fais des analyses approfondies dans les différents niveaux de l’organisation en croisant différentes dimensions comme l’âge, l’ancienneté, la localisation, le secteur d’activité. Grâce aux dizaines d’heures gagnées chaque mois, je peux me consacrer à de nouveaux chantiers pour développer encore plus le rôle de la RH. Dans un environnement économique concurrentiel, je peux contribuer au développement du capital humain, autant pour des enjeux de productivité que dans un con­texte de guerre des talents.\n\nApporter une puissance d’action sur toute la chaîne de la data\n\nH.K. Le data pionnier possède la puissance de préparation de la donnée : il peut attaquer toutes sources pertinentes pour l’analyse. Grâce à une solide base méthodologique, il va également bien préparer les données, phase cruciale en analytique. Par exemple, dans le cadre d’une mission pour mettre en place une nouvelle application, il était nécessaire de faire une reprise des plusieurs millions de données clients. Il a été demandé au métier de valider l’extraction avant de charger les données dans la nouvelle base. Sur quels critères le métier pouvait-il s’appuyer et vérifier que ces données étaient propres ? Devait-il tester par échantillons ou vérifier ligne par ligne ? Quelle méthode statistique devait-il employer ? La technique et les outils du data pionnier ont permis d’analyser ce fichier plus sereinement, plus rapidement et in extenso… En quelques minutes, sans passer par des formules sur fichier Excel. Autre exemple, dans un contexte de marketing multicanal, le data pionnier va pouvoir accompagner le métier pour mieux utiliser toutes ses données « silotées » grâce à des développements agiles. Il l’accompagnera pour construire, joindre, agréger, calculer les données plus facilement. Il rendra le métier autonome par la mise en place d’analyses souples et rapides. Il pourra ensuite être un lien entre le data scientist et le métier pour les analyses avancées qui suivront.\n\n\\> Quels sont vos facteurs clés de succès ?\n\nApprenez !\n\nH.K. Il ne faut plus « avoir peur de la data ». C’est un premier cap à passer pour des fonctionnels comme moi. Démystifions le vocabulaire autour de la data. Apprenons à comprendre la data, comment choisir les indicateurs et comment mieux analyser pour répondre au plus près aux enjeux du métier. Cela permet d’être plus dans l’anticipation et de décider plus facilement.\n\nP.-A. B. Côté RH, Il faut prévoir un plan d’accompagnement, avec notamment des parcours complets de formation. La clé est de présenter la data comme un outil d’aide à la prise de décision ou permettant d’identifier des signaux faibles difficilement perceptibles, mais toujours en gardant en tête que la gestion de l’humain nécessite l’intuition et l’expérience métier. Il faut définir une stratégie progressive de mise à disposition d’outils pour des utilisateurs peu habitués à traiter des données.\n\nPratiquez !\n\nH.K. Comme tout nouveau domaine, par la répétition, nous devenons meilleurs. Le nombre de données devenant croissant de plus en plus, tout comme leurs combinaisons, une pratique au quotidien permet d’acquérir des automatismes.\n\nB. B. La performance du processus se traduit par l’automatisation pour réduire la charge de réalisation tout en garantissant la fiabilité. Il faut pratiquer pour optimiser le temps disponible en limitant les efforts nécessaires pour produire les reportings récurrents afin de garder du temps pour l’analyse des données.\n\nPartagez !\n\nB. B. il faut éviter de construire des outils destinés à des experts. Un outil très puissant permettant des analyses approfondies mais complexe ou peu ergonomique ne sera pas ou peu utilisé. L’expérience utilisateur est un facteur clé d’appropriation des outils de reporting.\n\nH.K. Être data pionnier est une attitude, une posture, une culture à acquérir au sein de l’entreprise. Cette nouvelle compétence s’ajoute à d’autres comme travailler en agile. Le rôle d’ambassadeurs des data pionniers dans les entreprises est clé pour pouvoir travailler dans le même sens.\n\nP.-A. B. Le plus grand défi pour nous est de faire de l’usage de la data un réflexe quotidien, tant dans le partage et le suivi de nos données que dans l’utilisation de ces données, et ce, pour tous nos collaborateurs, quel que soit leur rôle. L’enjeu n’est pas d’en faire un projet data à l’échelle de l’entreprise ; mais de distiller la composante data dans l’ensemble de nos processus métier. Cela prend forme pour nous par le cas d’usage du capital humain. Nous souhaitons continuer à développer, à monitorer et à veiller sur ce capital au quotidien et nous souhaitons y impliquer l’ensemble de nos collaborateurs. À nous de bien déployer cette culture du jeu pour que chacun d’entre nous puisse y prendre part.\n\n\\> Un dernier mot ?\n\nH.K. Dorénavant, la donnée ne sera plus du domaine exclusif des data scientists ou des business analysts : il y a encore souvent un monde entre ces derniers et les métiers. Qu’ils soient issus des départements techniques, data ou de l’opérationnel, les data pionniers sont le chaînon manquant.\n\nB. B. Accompagné pendant toutes ces années par Excel pour le traitement et l’analyse des données, je n’ai que récemment découvert de nouveaux outils et de nouvelles méthodes pour optimiser mes processus de reporting. L’opportunité de devenir data pionnier m’a permis de découvrir un nouveau monde, celui de la data et de redécouvrir mon monde : celui du capital humain.\n\nP.-A. B. Pensons d’abord métier avant de penser data ! La data est « au service de » et non un sujet à part entière. Pensons avec notre tête, notre cœur, nos tripes et nos datas. Ne pensons pas que data ! La data vient compléter, conforter et consolider notre intuition, notre background et notre réflexion. Partageons notre enthousiasme. Travaillons tous ensemble pour que la data ne vienne jamais remplacer notre background et pour qu’elle vienne servir uniquement nos enjeux.",
          "Created At": "2025-09-06T13:22:34.118+02:00",
          "Updated At": "2025-09-06T13:22:34.118+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE124_1757157753085",
          "Status": "Ready For Refine",
          "Text": "# Capucine Ortoli",
          "Created At": "2025-09-06T13:22:34.119+02:00",
          "Updated At": "2025-09-06T13:22:34.119+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE125_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Nos attitudes face à la transformation digitale et aux changements\n\nCoach, consultante et accompagnatrice de transformation individuelle et systémique : « Le plus humain, c’est le plus efficace durablement. »\n\n« Je suis une actrice du changement profond et durable de la culture d’entreprise vers plus d’humanité et d’efficacité. Ma singularité s’inscrit dans le lien entre la recherche du sens, l’intention profonde de vos actions, plus d’humanisme en entreprise dans le concret opérationnel. J’aide les individus, les leaders et les organisations à défier le statu quo et à donner naissance à de nouvelles façons d’interagir, de faire, de penser, de collaborer et d’être et je les accompagne pour le vivre au quotidien. »\n\n\\> Quel point de départ d’une transformation : théorie X ou Y ?\n\nLa théorie X de Douglas McGregor présuppose que l’homme n’a pas envie de travailler. Il faut l’y obliger, et donc le contrôler. L’homme chercherait dans cette théorie la sécurité, à ne pas prendre de risque et à rester toujours dans sa zone de confort. Il ne mobiliserait son intelligence que pour contourner les règles qui le gênent. Face à ces croyances, le seul mode de management qui paraît possible est un management autoritaire, qui lui-même accentue l’aversion du travail.\n\nL’homme est vu comme « une ressource à gérer » et non comme un être humain plein de potentiel à réaliser.\n\nLa théorie Y pose un autre regard : l’homme a une motivation intrinsèque pour le travail et un désir de contribuer ; il se réalise s’il est impliqué profondément dans l’organisation. Plus le salarié est impliqué dans l’organisation, plus il fait ses propres choix, plus il est engagé dans ses tâches. Plus un salarié est engagé, plus il se responsabilise et s’auto-organise. Ainsi s’installent la confiance, la liberté et la responsabilité dans le cadre d’une vision partagée des buts de l’entreprise.\n\nNous retrouvons ces deux facettes dans les projets de transformation digitale !\n\n\\> Dans quel cadran vivons-nous notre projet digital ?\n\nIl y a quatre territoires dans une transformation durable, profonde et performante à la fois humainement et économiquement.\n\nFigure 17. Matrice de quatre territoires pour guider une transformation digitale humaine24\n\nUne transformation durable passe par chacun de ces espaces. Le plus important est la flèche circulaire au milieu. Il ne s’agit pas d’abord de changer les pratiques ou de travailler sur les individus, mais bien d’agir sur chaque cadran simultanément. Si un cadran n’est pas pris en compte dans la transformation, attendons-nous à des résistances !\n\nUne entreprise industrielle, avec laquelle j’ai travaillé, initiait sa transformation principalement sur deux axes : individuel qui ne se voit pas (développement personnel, leadership) et collectif (méthodes agiles, lean par exemple). Le résultat : une forme d’individualisme s’est développée et certains leaders sont partis, faute d’un projet commun inscrit dans la profondeur (le collectif qui ne se voit pas). N’appuyer que sur deux cadrans sans appuyer sur les autres crée des effets de bord contre-productifs.\n\n\\> Quelles sont nos postures individuelles lors de transformations (digitales) ?\n\nNous allons nous concentrer sur la partie « individu » sur laquelle le data pionnier peut travailler individuellement.\n\nDans tous les projets, de transformation digitale ou pas, nous allons adopter des postures. Le principe n’est pas d’imaginer que nous allons changer l’autre ou nous-même immédiatement, mais juste de voir le mécanisme en route. Notons que c’est toujours plus facile de voir l’autre dans sa posture que de se voir soi-même.\n\nNos postures peuvent alimenter deux cycles : le cycle du drame ou le cycle de l’innovation et de la transformation féconde. La première étape, c’est la prise de conscience de chaque individu et des équipes pour identifier la dynamique dans laquelle nous nous trouvons.\n\nLes postures qui recyclent les drames dans l’entreprise : le triangle de Karpman « victime-sauveur et persécuteur »\n\nPrenons pour exemple un projet de développement de reporting où trois équipes sont impliquées : une équipe de développement, une équipe data, deux managers d’équipe. L’équipe de développement est en retard sur le livrable. Le cycle du drame suivant peut se mettre en place :\n\n– les chefs d’équipe accusent l’équipe de développement : « C’est de votre faute, vous êtes en retard. » Ils prennent le rôle de persécuteur, ils accusent les autres ;\n\n– l’équipe de développement rétorque : « C’est de notre faute, on ne peut rien faire, nous n’avons pas les moyens de travailler plus vite : nous ne sommes pas assez nombreux, on nous a imposé des délais de livraison trop courts. » Elle prend le rôle de victime qui ne peut rien faire, qui est impuissante et désarmée. Elle subit ce qui arrive ;\n\n– l’équipe Data arrive et propose : « Ne vous inquiétez pas, nous allons vous aider, nous pouvons nous rendre disponibles pour vous aider à travailler, nous allons négocier avec le client interne de nouveaux délais. » Elle se pose en sauveur de la situation sans que personne n’ait rien demandé.\n\nLe cycle du drame va commencer et se perpétuer. Il sera alimenté par les changements de postures des protagonistes. Regardons par exemple les chefs d’équipe qui se posent en persécuteurs. Ils prennent aussi le rôle de victimes des deux autres équipes : ils ne peuvent rien faire pour les aider, ils se sentent impuissants, et ils pourraient à terme tenter de sauver la situation au côté de l’équipe Data.\n\nPour nous aider à identifier nos postures, voici quelques éléments de repère\n\nAfin d’éviter les spirales négatives, nous devons prendre conscience individuellement ou collectivement des postures que nous prenons. Nous devons sentir l’apparition de ces triangles dramatiques qui génèrent des peurs et sont parfois vécus comme des menaces.\n\nL’idée n’est pas de faire disparaître ces mécanismes, mais d’identifier si notre posture est adaptée à la situation : est-ce que cela sert la transformation digitale d’entreprise ?\n\n\\> Quelles sont les postures à adopter pour sortir du triangle du drame et entrer dans le cycle de l’Innovation ?\n\nPrendre conscience du triangle dramatique qui se joue et se rejoue permet de prendre le recul nécessaire et de voir que d’autres postures sont possibles et souhaitables dans l’entreprise pour plus d’épanouissement et de succès pour tous. Nous pouvons bâtir un autre triangle : le coach, le créateur et le challenger.\n\nPour nous ouvrir à ces postures de créateur, de challenger, de coach, nous pouvons utiliser ces questions.\n\nDans l’exemple précédent, les équipes peuvent créer une dynamique positive :\n\n– les chefs d’équipe peuvent se positionner en coach : « Nous allons profiter de cette situation pour revoir nos processus. Si nous ne prenons pas ce temps, la situation se reproduira » ;\n\n– l’équipe de développement se présente en challenger : « La fixation de délais trop courts que nous n’osons pas remettre en cause nous met une pression inutile » ;\n\n– l’équipe Data se positionne en créateur : « Nous allons intercéder auprès du client interne pour obtenir des délais plus raisonnables. Nous devons changer cette dynamique et nous allons proposer une nouvelle approche dans la préparation de la donnée. »\n\nLes projets data se gagnent et se perdent avec le facteur humain. Il ne s’agit pas de mettre des étiquettes sur nos rôles et de ranger nos initiatives dans des cases, mais de trouver des repères pour nous situer dans les différentes dynamiques que nous rencontrons. Ces axes d’analyse apportent un guide à nos réflexions dans des situations complexes qui peuvent rapidement dégénérer et augmentent nos chances de naviguer dans les écueils du travail en équipe.",
          "Created At": "2025-09-06T13:22:34.119+02:00",
          "Updated At": "2025-09-06T13:22:34.119+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE126_1757157753085",
          "Status": "Ready For Refine",
          "Text": "# Chantal Buard",
          "Created At": "2025-09-06T13:22:34.120+02:00",
          "Updated At": "2025-09-06T13:22:34.120+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE127_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## La chaîne analytique au-delà du monde de l’entreprise, pour contribuer à un monde meilleur\n\nChantal Buard est la cofondatrice et P.-D.G. d’Impact Atlas, une plateforme technologique permettant de prouver et d’améliorer les programmes sociaux et humanitaires à travers le monde. Auparavant, Chantal a été cofondatrice et directrice digitale chez Amplifier, une agence de stratégie et d’impact social travaillant avec des visionnaires et des investisseurs dans les programmes d’impact social à grande échelle. À travers son expérience chez Amplifier, Chantal a cocréé deux autres solutions numériques : l’une pour la gestion des océans et l’autre pour la pêche aux États-Unis, en mettant en place la gestion de quotas. Elle a aussi créé un des groupes de philanthropie parmi les plus grands et innovants au monde, ainsi qu’une initiative collaborative portant sur l’extrême pauvreté, propulsant au niveau planétaire « l’approche de Graduation » qui a reçu le prix Nobel en 2019. Née à Paris, Chantal vit depuis vingt-cinq ans dans la Silicon Valley.\n\n\\> Pourquoi avoir créé Impact Atlas ?\n\nImpact Atlas a pour but de contribuer à la résolution des grands problèmes économiques, sociaux et écologiques de notre époque par une contribution holistique des enjeux. C’est une plateforme technologique qui permet de gérer l’ensemble des données des programmes sociaux ou humanitaires dans des environnements isolés, souvent pauvres et à faible connectivité. Impact Atlas permet de vraiment connaître ce qui se passe sur le terrain, parfois en temps réel, et de remonter une information aux responsables de programmes pour les analyses, les chercheurs et les donateurs ou les sponsors. La solution est déployée dans quinze pays à travers le monde et apporte une véritable transparence sur les projets complexes.\n\n\\> À quels défis répondez-vous avec Impact Atlas ?\n\nNous faisons tout d’abord face à la grande distance physique et à la difficulté de connecter des données locales très fines et variées à des objectifs macro. Les objectifs et les indicateurs centraux émis par les instances internationales et les associations humanitaires se résument souvent à des mesures très agrégées qui n’expriment pas la complexité d’une situation locale et qui sont construites sur la base d’une donnée parcellaire du fait de la difficulté de collecte de l’information. Dans les pays sur lesquels nous intervenons, les environnements très faiblement équipés ainsi que les conditions de terrain difficiles (absence de budget, d’électricité, de personnel formé, etc.) rendent la chaîne analytique beaucoup moins triviale à gérer qu’en entreprise.\n\nL’éloignement, la diversité des données ainsi que le manque de formation « data » des personnels en local accentuent également les problèmes de qualité de données. La majorité des informations clés n’est pas encore digitalisée et va nécessiter une intervention manuelle pour être captée. Ces phases de récupération de données vont devoir être simples et rapides : il n’est absolument pas concevable de faire passer de longs questionnaires à des populations qui se battent au quotidien pour faire vivre leurs familles. Enfin, les saisies ou les captages d’informations vont être sujets à des risques d’erreur du fait de l’environnement humain et technique et vont devoir être modérés par des recoupements et des inférences avec d’autres données captées.\n\nEnfin, la valeur des données locales ne se matérialise que lorsque celles-ci sont rassemblées, homogénéisées et mises en relation dans des modèles qui vont aider les analystes à comprendre la situation dans son ensemble et dans la durée.\n\nImpact Atlas apporte une solution robuste et simple, compatible avec ces milieux peu propices au déploiement de technologies. Nous couvrons les derniers kilomètres de la donnée pour assurer la collecte durable et précise des informations réellement pertinentes pour la compréhension et le suivi des causes que nous supportons.\n\n\\> Comment cela marche concrètement ?\n\nImpact Atlas va tout d’abord limiter la barrière technique inhérente à chaque capture de données. Nous travaillons sur des appareils mobiles à grande autonomie, qui peuvent fonctionner sans réseau. Ils peuvent capturer la donnée partout où un humain peut aller. Nos collecteurs peuvent parfois faire plusieurs jours de trajet pour arriver dans les villages reculés : ils synchronisent leur application avant leur départ pour charger les questionnaires et feront de même à leur retour ou dès l’accès à un réseau pour remonter les informations collectées.\n\nLa barrière de la langue est aussi traitée par un design multilingue des interfaces pour prendre en compte les dialectes locaux.\n\nNous travaillons aussi à une collecte « intelligente » de l’information. Nous nous concentrons sur la donnée nécessaire et nous évitons les questionnaires à rallonge qui découragent et finissent par manquer de consistance. Nos systèmes de collecte s’adaptent aux données précédemment captées pour progressivement compléter les données manquantes ou éventuellement demander des données complémentaires suite à une observation particulière. Par exemple, si, lors d’une rencontre, la présence d’enfants est remarquée dans une ferme, alors des questions sur leur scolarisation pourront être proposées.\n\nNous sommes également soucieux de la qualité de l’information collectée. Compte tenu de la dissémination des endroits de collecte, des personnels que nous ne pouvons pas tous motiver et former de la même manière, nous apportons une attention particulière à la validation des prises de mesures. Nous ne pouvons pas imposer des contraintes complexes qui seraient trop lourdes et qui nuiraient à la fluidité du processus. Nous travaillons beaucoup avec la géolocalisation des saisies pour nous assurer que la mesure est bien faite in situ. Nous collectons également des photos prises par les appareils mobiles qui nous apportent non seulement une preuve visuelle et des coordonnées géographiques, une validation de l’identité des personnes sondées, mais également de nombreuses informations supplémentaires que nous pourrons interpréter et capturer par la suite, notamment grâce à des solutions de reconnaissance d’image.\n\nNous centralisons ensuite toutes ces informations en un modèle de données holistique, vraiment représentatif d’une situation terrain, témoignage des progrès concrets d’une initiative.\n\nFinalement, nous apportons la transparence nécessaire sur ces projets complexes, grâce à des mesures pertinentes, validées et régulières.\n\n\\> Quelles sont les clés de réussite d’un projet avec Impact Atlas ? Quelles leçons un data pionnier peut-il en retenir ?\n\nPosez la bonne question et choisissez les bonnes données ! Nous avons d’ailleurs souvent recours au Design Thinking pour cerner les problématiques correctement. Sans cette phase initiale, le programme peut perdre tout son impact. Des programmes captent trop de données et se noient dans leur analyse. D’autres mesurent ce qui n’est pas pertinent et n’ont aucune idée de l’impact de leurs actions.\n\nAdaptez-vous au contexte de votre audience. A-t-elle le temps, la formation, les moyens, la motivation ou la formation pour fournir la donnée ? Comment pouvez-vous l’aider ou lui rendre la vie plus facile ?\n\nSoyez créatif et innovant pour aller chercher l’information clé qui vous manque.\n\nRestez toujours respectueux de l’information que vous collectez. Au-delà de nos missions d’analyse, nous avons aussi un devoir moral et légal de respecter la vie privée et l’éthique.",
          "Created At": "2025-09-06T13:22:34.120+02:00",
          "Updated At": "2025-09-06T13:22:34.121+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE128_1757157753085",
          "Status": "Ready For Refine",
          "Text": "# Corinne Plourde",
          "Created At": "2025-09-06T13:22:34.121+02:00",
          "Updated At": "2025-09-06T13:22:34.121+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE129_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Le data pionnier face aux défis de l’éthique en data\n\nCorinne Plourde, Docteur en informatique dans le domaine des bases de données déductives, partage son expérience entre la mise en place de la sécurité de la donnée et de ses processus et les audits de compliance, ISO 27001 comme RGPD. Formatrice et enseignante en université (université Paris 1 Sorbonne, université Jean Monnet Saint-Étienne), Corinne a mis l’humain au centre de ses passions. Elle partage ses compétences entre pédagogie, créativité et qualification dans un poste de directrice pédagogique. Elle allie la recher­che fondamentale, l’expérience dans la montée en compétences et la technique pure. L’informatique fut une de ses passions depuis toute petite et l’apprentissage dans ce domaine, une volonté constante.\n\n« Lorsqu’on utilise le mot « éthique », on vise ce qui est juste. L’éthique nécessite donc une confrontation entre des principes issus des valeurs, du droit et de la réalité vécue. Par conséquent et par définition, il n’y a pas de vérité éthique déjà établie, qui soit valable de la même façon partout et pour tout le monde. Nous devons utiliser des références pour nous guider.\n\nIl est nécessaire que la prise en compte de l’éthique vienne de l’intérieur de la profession. Une pression qui vient de l’intérieur peut permettre de forcer les entreprises à revoir leur politique éthique indépendamment des considérations économiques. L’intégration des considérations éthiques dans la formation des data pionniers va éviter de réduire l’analytique à ses aspects purement techniques. Voici quelques pistes pour nous guider. »\n\n\\> Équilibrer le cœur et le cerveau\n\nAvec le recul que nous possédons aujourd’hui, nous pouvons observer que les avancées dans le monde de l’analytique se composent de deux parties distinctes : celle qui vient du cerveau et celle qui vient du cœur.\n\nLes avancées commencent par la partie cerveau. Et une fois que la conception, la technique, le développement et le design sont réalisés, arrive la question du cœur, de l’humain. Pour parler de « Data éthique », que ce soit par la machine ou par l’humain, nous sommes confrontés aux mêmes défis : comment prédire, prévoir, analyser et construire de nouvelles données tout en contrôlant leur contenu et en ne mettant pas en cause la vie privée des personnes et, par conséquent, sans toucher à leur liberté ?\n\nLe data pionnier doit allier le cœur au cerveau, la morale à la technologie, l’éthique à la data. Il doit aborder toutes les dimensions des avancées humaines du futur sans perdre le sens de ses objectifs. Dans cette perspective, il sera nécessaire de réintroduire dans les projets data, au travers d’approches transdisciplinaires, tout ce qui est constitutif de notre savoir et qui nous préserve dans notre humanisme au sein des organisations.\n\n\\> Comprendre la force et les dangers du Smart Data\n\nLe Big Data proposait la collecte massive de données, sans forcément attacher des objectifs d’analyse précis. Aujourd’hui, on parle désormais de « donnée intelligente » (Smart Data). La donnée intelligente est produite par la construction de modèles de données riches et holistiques, par nos analyses et par nos algorithmes de Machine Learning. La Smart Data représente les données et les informations les plus intéressantes et pertinentes au regard des objectifs de l’entreprise.\n\nLes Smart Data permettent une connaissance très précise des personnes et de leur sociologie. Elles offrent des analyses très détaillées qui sont en mesure de conditionner la réalité de populations entières : profilage, fake news, élections, utilisation abusive des données à caractère personnel.\n\nLe data pionnier doit avoir conscience de la portée de ses pouvoirs en termes de collecte, de traitement, d’assemblage et d’analyse. Il doit être le premier garde-fou des abus.\n\n\\> S’appuyer sur l’éthique par défaut et par conception\n\nLe principe « d’éthique par défaut » reprend le concept du RGPD (Règlement général sur la protection des données) avec la protection de la vie privée par défaut. Il consiste à n’exploiter et donc à ne collecter que les données nécessaires aux traitements répondant aux objectifs établis par l’organisation ou la business unit.\n\nEn complément de l’éthique par défaut, l’éthique par conception (by design) va, elle, gérer la conception des processus. Elle répond à la problématique de contrôle et de maîtrise des effets occasionnés par les nouvelles technologies et analyses.\n\nCes réflexions éthiques en amont des techniques sont essentielles à la pensée d’un projet dans son ensemble. Elles peuvent être soutenues par des méthodologies que l’on retrouve dans le champ du design (scénarios d’anticipation, tests utilisateurs, etc.).\n\n\\> Craindre l’humain plus que la machine\n\nCe n’est pas tant la machine qui est à craindre que les humains qui sont derrière et qui en ont la maîtrise et le contrôle.\n\nLa mise en place de mécanismes de supervision des algorithmes permettant le respect de la vie privée et des résultats socialement acceptables demandent un cadre éthique pour réguler les risques de conception d’un système prédateur, tourné vers son seul intérêt. Ces systèmes sont d’autant plus dangereux qu’ils sont capables de prendre des décisions de moins en moins transparentes et de plus en plus rapidement.\n\nEn imitant les humains, les machines amplifient tous leurs comportements… Sachons faire preuve d’exemplarité dans l’exercice des valeurs qui caractérisent notre humanité et qui se retrouveront dans la programmation des machines.\n\n\\> Faire de la diversité un objectif central\n\nLe manque de diversité dans la communauté des analystes induit une homogénéisation des façons de penser. L’éthique est avant tout une discussion qui est enrichie par l’apport de visions différentes. Sans une diversité de profils, les projets analytiques seront invariablement exposés à des biais plus forts et rarement identifiés comme problématiques.\n\n\\> Maintenir loyauté et vigilance\n\nLe principe de loyauté : tout algorithme, qu’il traite ou non des données personnelles, doit être loyal envers ses utilisateurs, non pas seulement les consommateurs, mais également les citoyens, voire envers des communautés ou de grands intérêts collectifs dont l’existence pourrait être directement affectée.\n\nLe principe de vigilance : il s’agit d’organiser une forme de questionnement régulier, méthodique et délibératif à l’égard de ces objets mouvants, avec la constitution d’un comité éthique par exemple.\n\nEn intégrant ces réflexions dès le début de leur cheminement, les data pionniers se prépareront progressivement aux défis éthiques croissants de leurs analytiques.",
          "Created At": "2025-09-06T13:22:34.122+02:00",
          "Updated At": "2025-09-06T13:22:34.122+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE130_1757157753085",
          "Status": "Ready For Refine",
          "Text": "# Sea Matilda Bez",
          "Created At": "2025-09-06T13:22:34.122+02:00",
          "Updated At": "2025-09-06T13:22:34.122+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE131_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Les défis de l’innovation du data pionnier\n\nMatilda Bez est maître de conférences à l’université de Montpellier. Elle a passé deux ans à Berkeley en tant que post-doc avec Henry Chesbrough, professeur considéré comme le père de l’Open Inno­vation. Durant ces deux années, elle a étudié les défis de l’Open Innovation dans des contextes coopétitifs (quand des concurrents coopèrent).\n\n« Grâce à une analytique performante, les Pionniers disposent de plus de temps pour réfléchir. Comment peut-on élever notre capacité à réfléchir et à innover ?\n\nNous pouvons commencer par voir plus loin dans nos réflexions et leur apporter une dimension sociétale ou environnementale.\n\nSouvent, quand on pense à l’innovation, on pense innovation incrémentale. Il faudrait passer plus de temps à réfléchir à ce qu’on appelle des Moonshots (viser la lune). Ce sont des innovations radicales qui ont pour but de résoudre un besoin sociétal (sustainable development goals). Et pendant longtemps, on a vu le profit et ces objectifs sociétaux comme deux buts opposés. On opposait l’entreprise qui ne veut faire que du profit et l’organisation à but non lucratif qui a pour but de faire du développement durable. Aujourd’hui, on réalise que ces deux buts ne sont pas opposés : la recherche du profit peut consister à chercher à résoudre un but de développement durable.\n\nPar exemple, Enel, une entreprise d’énergie italienne, a décidé de se consacrer à l’accès universel à l’énergie abordable. Cette stratégie ouvre deux opportunités :\n\n– c’est un énorme marché : 1,2 milliard de personnes n’ont pas accès à de l’énergie à l’heure actuelle, et 1,8 milliard de personnes n’y ont pas accès de manière fiable ou à un coût raisonnable ;\n\n– c’est un défi qui va attirer les talents qui ont un vrai potentiel innovant. Ces derniers ne sont plus intéressés par l’argent. Ils veulent que leur métier ait du sens. La recherche d’innovations en développement durable donne une raison d’être à leur engagement. »\n\n\\> Quelles sont les clés pour renforcer son potentiel d’innovation, que l’on soit un individu ou une organisation ?\n\nIl faut d’abord se rendre compte de l’énorme écosystème de start-up qui est présent et prêt à aider l’entreprise à accélérer sa transformation digitale à un coût plus faible et moins risqué. Il ne faut pas tomber dans le piège de vouloir tout faire tout seul, mais au contraire ouvrir les murs de son entreprise à ces start-up et accepter leur aide. Il faut mobiliser ou créer cet écosystème de start-up qui nous entourent pour innover.\n\nPar ailleurs, quand une organisation décide justement de s’engager avec des partenaires de l’écosystème, elle doit se rendre attractive. C’est à l’organisation de séduire ses partenaires dans une collabo­ration. Nous pouvons même dire qu’elle doit faire un « pitch » aux start-up sur les raisons de travailler avec elle.\n\nIl faut également considérer que la compétition n’est pas uniquement au niveau de l’entreprise, mais au niveau de l’industrie. Il y a des industries plus ou moins attractives pour les start-up, surtout celles qui ont le vent en poupe. Comment peuvent-elles attirer des start-up dans leurs industries considérées comme plus traditionnelles ? En collaborant avec leurs concurrents. Par exemple, en créant une base de données commune, elles vont attirer les entrepreneurs talentueux qui veulent tester et développer des algorithmes de Machine Learning ou d’automatisation. Ainsi, Telefonica, Orange, Deutsche Telekom et Singtel ont créé une initiative qui s’appelle Go Ignite, où ils ont mis en commun leur base de données, représentant plus d’un milliard de clients, particuliers et entreprises, sur cinq continents. Et avec cette énorme base de données, ils font des appels aux start-up pour innover sur la 5G, sur la Blockchain et autres sujets de pointe. Ce trésor de données leur permet d’attirer beaucoup plus de start-up qu’ils n’auraient pu le faire séparément.\n\nUn dernier point, c’est ce que j’appelle le multi-unit back-end problem : le problème de l’arrière-ligne des entreprises multi-départements/multifiliales. C’est la difficulté à connecter les équipes de l’entreprise et à les faire collaborer. On sous-estime la compétition qu’il y a entre les départements. Cette absence de collaboration met les transformations digitales en danger.\n\nQuand on réfléchit sur des projets data avec des start-up partenaires, on se focalise souvent sur la relation entreprise-start-up. Comment mettre en synergie deux entités de tailles différentes, avec des façons de fonctionner différentes ? Eh oui, c’est vrai que cela peut être une difficulté, et cette relation ne peut pas marcher si l’on n’a pas fait coopérer les départements en interne en amont.\n\nPour illustrer cela, en l’occurrence un échec, je pense à un groupe bancaire français, qui avait au départ beaucoup de difficulté à avoir des résultats dans leurs initiatives de transformation digitale avec des partenaires start-up. Pourtant, l’entreprise était dans une situation idéale. Les managers étaient impliqués et motivés par le projet. Les différents départements avaient conscience qu’il fallait faire de l’innovation digitale. Il y avait vraiment une demande client pour de meilleurs services ou produits, qui allait nécessiter le développement d’algorithmes de Machine Learning ou d’automatisation. Le groupe avait tout un écosystème de start-up motivées pour travailler avec lui et même un budget de 600 millions d’euros sur trois ans pour créer une équipe dédiée à la mise en place des initiatives. Tout aurait dû fonctionner. Mais, au final, il n’est pas arrivé à avoir de résultats.\n\nQuand ma collègue Thuy Seran et moi avons cherché à comprendre pourquoi, nous n’avons pas identifié de problèmes de collaboration avec les start-up. Tout venait des problèmes internes et de la compétition entre les départements. Nous avons trouvé trois exemples représentatifs :\n\n– dans une mise en commun des données au sein de l’entreprise, partager la donnée ne suffit pas. Il faut que ces données soient de qualité et données à temps. Les différentes équipes avaient peur de partager leurs données, comme leurs meilleurs clients ;\n\n– le deuxième problème était le manque de transparence des équipes et des agences. Pour qu’une collaboration dans la création d’algorithmes de Machine Learning pertinents et utiles réussisse, il faut que les objectifs soient clairs, que les problèmes soient partagés en toute transparence et que toute la donnée pertinente soit mise à disposition. Il faut que les équipes et les partenaires puissent échanger. Mais, là encore, ce partage n’existait pas pour des raisons de méfiance et de peur ;\n\n– en dernier lieu, les synergies ne se produiront que si les équipes prennent du temps pour codévelopper avec les partenaires et pour tester les algorithmes par itération. Si les résultats sont là, ils seront partagés avec l’ensemble des autres équipes, qui n’auront pas contribué à l’effort. Pourquoi devrions-nous prendre le temps et le budget de nos équipes pour des développements, quand d’autres vont en bénéficier gratuitement ?\n\nC’est que nous qualifions de coopétition interne un mélange de compétition et de coopération en interne qui nuit à l’innovation et au progrès.\n\n\\> Comment fait-on pour gérer cette compétition qui peut être si ancrée dans les organisations ?\n\nCela se gère. C’est ce que ce groupe a d’ailleurs entrepris. Ils ont choisi de gérer cette compétition interne avec plusieurs types d’actions :\n\n– ils ont mis en place un système d’anonymisation des données partagées pour éviter que des départements n’accèdent à trop de détails de leurs collègues ;\n\n– ils ont ensuite offert un budget bonus pour l’achat de solutions développées par les start-up pour les départements qui ont donné leurs données ;\n\n– ils se sont rendu compte que partager un besoin entre départements était trop sensible pour les besoins réels du moment. Le groupe a donc incité à partager les problèmes pour lesquels les équipes n’avaient ni d’argent ni de temps à investir pour trouver une solution, évitant ainsi les comportements individuels. Pour les besoins réels, ils proposent des solutions locales qui se limitent au département et à la start-up (les autres départements ne sont pas au courant). Avec cette distinction entre besoin du moment et besoin futur, le problème de la peur ou de la jalousie entre départements était traité ;\n\n– en dernier lieu, ils espèrent motiver les équipes à tester et à construire des solutions en leur donnant un avantage concurrentiel en termes de droit d’utilisation. Si un département développe un algorithme, il disposera d’une période d’exclusivité. Une fois un retour investissement obtenu par l’algorithme, les autres départements pourront utiliser ce dernier. Comme une sorte de brevet interne ! C’est un bénéfice aux Pionniers.\n\n\\> Quels conseils donnerais-tu aux Pionniers ?\n\nJ’aurais trois conseils à donner.\n\n– Acceptez l’attitude « innovation ouverte avec des concurrents » (open coopetitive mindset). Soyez prêt à collaborer avec des concurrents et à en faire des alliés pour être encore plus pertinent dans vos approches analytiques et plus apte à attirer des partenaires et des talents pour collaborer.\n\n– N’ayez pas peur de paraître stupide. C’est un concept que j’ai développé avec Henry Chesbrough lorsque j’étais à Berkeley : Fear of Looking Foolish (« peur d’avoir l’air stupide »). Il y a des idées extraordinaires qui vont rester sur « une étagère inutilisée » car les managers ne vont pas laisser cette idée être développée par quelqu’un d’autre à l’extérieur de l’entreprise. En effet, si cette personne extérieure réussit à transformer l’idée en succès commercial, le manager qui aura laissé l’idée sortir de l’entreprise aura l’air stupide. Le manager peut même avoir peur de représailles : « Comment n’as-tu pas pu voir le potentiel derrière l’idée ? » Alors que, au contraire, il devrait être fier et sa hiérarchie devrait le féliciter d’avoir réussi à trouver la personne adéquate pour réaliser ce succès et idéalement négocier des royalties sur ce dernier.\n\n– Agissez pour limiter les biais et les risques de vos algorithmes de Machine Learning ou d’automatisation, mais aussi des algorithmes de vos concurrents. Il suffit d’une entreprise avec un algorithme aux conséquences dramatiques pour gâcher la crédibilité de toute l’industrie. Comportez-vous comme une fratrie même avec vos concurrents, surveillez leur manière de construire les algorithmes, éduquez-les sur les biais et les risques des algorithmes, et collaborez pour trouver ensemble des solutions.",
          "Created At": "2025-09-06T13:22:34.123+02:00",
          "Updated At": "2025-09-06T13:22:34.123+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE132_1757157753085",
          "Status": "Ready For Refine",
          "Text": "# Nolwenn Godard",
          "Created At": "2025-09-06T13:22:34.123+02:00",
          "Updated At": "2025-09-06T13:22:34.125+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE133_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Couvrez cette domination masculine que je ne saurais voir dans ma data\n\nNolwenn Godard, cadre dans la FinTech, est dotée d’une riche expérience de création de produits et de plateformes dans un environnement mondialisé, dynamique et collaboratif. Elle s’emploie à mettre la technologie au service de la création de valeur et de l’impact social. Diplômée de l’Essec, ses expériences de carrière incluent un passage chez Sofi, PayPal, Ubisoft et une ONG en microfinance au Proche-Orient. Elle a été distinguée comme faisant partie des cent femmes les plus influentes de la Silicon Valley en 2018.\n\n« Nous avons une crise de représentation des femmes dans l’intelligence artificielle », nous dit Fei Li, à quelque dix-huit mille femmes et à moi-même à Grace Hopper 2017. Ces mots m’interpellent. Comment puis-je aider à changer la donne ?\n\nLa quatrième révolution industrielle arrive avec ses grands enjeux de société et ses questions : quel futur commun voulons-nous ? Comment éviter une dystopie à la Black Mirror 25 et tendre vers une société au développement durable, inclusif et écologique ? Comment former et redéployer la main-d’œuvre dont les emplois actuels seront détruits demain par l’automatisation, la communication de machine à machine et l’Internet des objets ? Qui aura accès au développement de compétences nouvelles ? Quelle entreprise ou instance gouvernementale aura accès au rare talent technologique, dans un monde « anumérique », comme il a autrefois été analphabète, pour réaliser ses projets ou se protéger contre les cyberattaques ? Comment s’assurer que les algorithmes seront équitables, à l’embauche, dans l’attribution des peines judiciaires et jusque dans nos livraisons quotidiennes ? Comment faire pour que nous ne vivions pas encore plus dans des mondes parallèles, pour que nous restions « libres » de nos choix et pour que les discriminations passées ne soient amplifiées ni gravées pour toujours dans « le grand code planétaire », sans possibilité de s’en affranchir ni de comprendre son fonctionnement ?\n\nCertes, comme dans d’autres domaines de pointe (nucléaire, médecine, recherche, etc.), l’expertise technique ne sera pas universellement répartie, mais il faut penser et organiser un système assurant le bien-être de l’humanité. Nous devons enfanter le monde de demain ensemble : gouvernements, entreprises, ONG, femmes et hommes politiques, société civile du monde entier. « La technologie est trop importante pour qu’on la laisse aux seuls technologistes26 », comme le dit si justement Azeem Azhar. Les progrès technique et moral n’étant pas corrélés, nous ne pouvons être attentistes.\n\nLa réflexion dans nos entreprises est encore balbutiante et sans commune mesure avec le besoin de penser et d’amorcer le changement. On entend de si beaux discours sur la diversité, l’inclusion, l’appartenance, l’équité et leurs bénéfices au-delà d’une simple justice (d’ailleurs toujours dévalorisée, insuffisante) : de meilleurs résultats financiers, plus d’innovation et de brevets, une amélioration du bien-être et de satisfaction au travail.\n\nMais, au quotidien, rien ne semble bouger. Les femmes exécutives en Tech et Data Science que je rencontre, quels que soient leur grade ou leurs années d’expérience, sont lassées par le décalage entre la rhétorique et la pratique. Pourquoi cette usure ?\n\nLes lignes suivantes vous révèlent ce dont un « je » collectif pourrait témoigner. Partager cette voix plurielle, cet échantillon d’expériences vécues sous différentes formes par des milliers de femmes s’inscrit dans une démarche d’interruption du silence et parfois du déni qui contribuent à perpétuer le système établi.\n\n\\> Une journée d’Ivanna Denisova, data analyste\n\n« Dès l’aube, à l’heure où blanchit la campagne, j’enfile ma tenue de combat (tailleur, jeans ou sari). Comme chaque jour, je suis la seule femme pour environ dix hommes dans mon équipe et dans toutes mes interactions. Les réunions s’enchaînent et l’on me coupe cons­tamment la parole… Que puis-je bien avoir d’intéressant à dire ? À la fin des sessions, on me dit qu’on ne m’entend pas assez, que je ne suis pas assez sûre de moi, que je n’ai pas d’opinion, ou que, si j’apportais de la valeur, on m’écouterait. Sur la visioconférence suivante (par chance on m’écoute), on me dit qu’il faut que je sois plus claire, qu’on ne comprend pas mon propos. Lorsqu’un collègue masculin le reformule à l’identique, on ne manque pas de le féliciter.\n\nMa journée se poursuit avec des rencontres clients et fournisseurs. Ils n’osent pas me demander d’apporter le café, mais me considèrent de facto comme leur scribe. Mes analyses, mes rapports sont commentés “sans moi” : impossible d’interrompre la “partie de ping-pong” qui se joue sous mes yeux…\n\nLors d’un atelier technique en début d’après-midi, il y a forcément un confrère pour m’expliquer d’emblée le Master Data Management, les règles de jointures ou l’ajout d’un script Python : malgré mes dix années d’expérience en Data Science, je n’échappe pas au Mansplaining27. J’ai toujours du mal à m’y faire.\n\nPuis il faut se battre et garder la tête haute… J’ai d’autres projets à défendre. Malgré les succès de mon équipe, je dois insister auprès de ma hiérarchie et des RH pour qu’une de mes employées en congé maternité soit promue. Je me vois aussi refuser un congé pour être auprès de mon père convalescent.\n\nAlors que je demande un peu plus de ressources pour renforcer le contrôle de qualité de nos données, un pair ingénieur me hurle dessus devant mon équipe : “Comment oses-tu demander autant de ressources ? Tu ne vas tout de même pas avoir une équipe plus grosse que la mienne !”\n\nLes quelques femmes autour de moi, qui se plaisent à citer Madeleine Albright28, “il y a une place spéciale en enfer pour les femmes qui n’aident pas les femmes”, ne prennent pas ma défense : elles aussi doivent survivre dans le monde de la data. Comment leur en vouloir ?\n\nUne journée classique dans ma vie de data analyste, déjà parsemée de moqueries en public, d’humiliations et de menaces en entretiens hebdomadaires, ou de pression psychologique, se termine par l’inévitable harcèlement frontal lorsque nous nous retrouvons en dehors du bureau pour célébrer un succès en équipe.\n\nIl se fait tard, mon patron m’annonce en off que l’on va me retirer ce que j’ai construit à partir de rien. Mes systèmes d’analyse, mes algorithmes, mes travaux de gouvernance de données dont personne ne voulait, les talents que j’aurai embauchés et formés à la data se sont tous transformés en « objet de désir » que plusieurs désormais convoitent. Il me dit qu’il faut laisser la place à Bob sous prétexte qu’il est une star, que je manque de charisme, que je suis la seule du département à ne pas avoir d’enfants, et que je serai donc parfaite pour ce nouveau rôle (un placard) qui nécessite de nombreux déplacements, ou encore que mon succès a offensé un supérieur… La nouvelle de ma « mutation » annoncée, mon patron déclare son aspiration à améliorer la diversité de son équipe.\n\nMe sentant en retrait au terme d’une journée qui s’éternise, il me conseille sous le couvert du conseil amical, de quitter la Tech pour ouvrir un restaurant. Je me prête au jeu et je finis par lui annoncer que je pars rejoindre une start-up en pointe dans l’analytique avancée. Mon manager s’étrangle de rage et dit : “Si j’étais ton père et que tu étais ma fille, je j’interdirais de quitter notre société.”\n\nJe termine cette journée, qui comme les précédentes, m’a montré que je n’étais pas à ma place. Avant d’éteindre ordinateur et téléphone, je prends le temps de revoir les résultats d’une reconnaissance d’images lancée il y deux jours pour la détection de défaut sur des pièces avec les équipes de production. Je viens en renfort boucler les dernières lignes de code sur le développement de l’application de scoring client développé avec la Finance. Je passe encore un peu de temps en ligne avec ma communauté d’analystes pour “cracker” un problème de visualisation de données. Je souris, j’aime mon métier, ces défis et toutes ces collaborations qu’il permet. C’est le domaine dans lequel j’excelle. »\n\n\\> Demain sera-t-il un autre jour ?“\n\nAujourd’hui, 50 % des femmes quittent la Tech avant 35 ans, et à un taux de 45 % plus élevé que les hommes29. Interrogées, seulement 21 % des femmes pensent qu’elles peuvent prospérer dans cette industrie, un chiffre qui tombe à 8 % pour les femmes de couleur. Confronter la constante présomption d’incompétence, le sentiment d’illégitimité, les micro-agressions ou le harcèlement sexuel des managers ou des VC (venture capitalist ou capital-risqueur), le manque d’avancement, d’opportunités de carrière ou de financement, et les journées à rallonge couvrant trois continents, c’est épuisant. Elles se reconvertissent ou quittent le marché du travail. Dommage, car on manque justement de talents en Tech.\n\n\\> Quelles solutions ?\n\nPour s’attaquer aux problèmes structurels de sexisme, de racisme, de classes/castes, il faut des changements structurels. Les déclarations d’intention ne suffisent pas. La volonté politique (publique, privée) incarnée dans l’action est primordiale. Dans son discours de l’université de Rice en 1962, Kennedy déclare un objectif très ambitieux, défini et délimité dans le temps : « Nous avons choisi d’aller sur la Lune au cours de cette décennie \\[…\\], non pas parce que c’est facile, mais justement parce que c’est difficile. Parce que cet objectif \\[…\\] est le défi que nous sommes prêts à relever, celui que nous refusons de remettre à plus tard, celui que nous avons la ferme intention de remporter. » Kennedy n’a pas dit : « Nous aimerions aller sur la Lune au cours de cette décennie » ni « nous allons former un comité sur l’espace dans cette décennie ». L’objectif fut atteint en sept ans.\n\n\\> Organisations pionnières de la data : que pouvons-nous faire ?\n\nComme nous y invite Paloma Medina30, remplaçons « aller sur la lune » par « atteindre l’égalité des genres et des origines ethniques » et formulons ces mêmes objectifs ambitieux : atteignons l’objectif d’égalité dans notre processus de recrutement d’ici à trois ans ; accroissons de 50 % la représentation des femmes dans notre équipe exécutive en trois ans ; faisons un audit et atteignons l’égalité des salaires et des promotions en deux ans… Il faut tracer, mesurer, rectifier, avoir des incitations à la clé (bonus si objectifs atteints), comme pour tout autre indicateur business.\n\nAdoptons une proactivité systématique. Les RH et les hautes sphères de l’entreprise devront recadrer les comportements discriminants, former au repérage des biais et donner aux managers des outils pour les réduire. Alors que les minorités sont de plus en plus conscientes des biais cognitifs à leur égard, la majorité est très peu formée et donc assez ignorante sur les questions d’équité, d’inclusion et de diversité. Le décentrement est difficile à acquérir pour la majorité qui a été habituée à être le référant universel. La sensibilisation à l’intersectionnalité et l’utilisation du Design Thinking pour créer une expérience de travail adaptée à celle-ci garantiront une meilleure expérience pour tous. « Si vous travaillez dur et que vous demandez de l’aide, vous y arriverez » : on aimerait que cette promesse méritocratique de Maria Klawe à ses élèves du Harvey Mudd College soit étendue au monde de l’entreprise.\n\n\\> Faisons de la place",
          "Created At": "2025-09-06T13:22:34.125+02:00",
          "Updated At": "2025-09-06T13:22:34.126+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE134_1757157753085",
          "Status": "Ready For Refine",
          "Text": "Les groupes de salariés représentant les femmes et les différentes minorités au sein de l’entreprise sont utiles comme réseaux de soutien, d’inspiration, ou comme plateformes d’enseignement des codes nécessaires à adopter pour arpenter l’univers de la culture dominante, mais ils ne doivent pas être perçus comme l’outil de changement dans l’organisation. Le cœur du problème est la volonté d’inclure et de garantir l’égalité. Pensons et orchestrons le difficile partage de pouvoir. Appuyons-nous sur des objectifs individuels motivant le changement (promouvoir un homme qui prend un congé paternité, bonus si objectifs de diversité atteints, etc.), car ceux de l’entreprise ne suffisent pas.\n\nTels les Stoïciens, anticipons le pire et construisons des processus pour pallier les défaillances humaines et limiter les abus de pouvoir. Des plateformes tierces pourraient faciliter le signalement du harcèlement sexuel. Formons à la communication non violente, au respect, à la bienveillance, à la bientraitance, tout en gardant à l’esprit que certains groupes démographiques sont statistiquement plus exposés aux abus et aux sujets à l’état de stress post-traumatique. Ces pratiques font cruellement défaut dans nos entreprises aujourd’hui.\n\nEn tant que data pionnier, nous avons tous un rôle à jouer. Soyons des pionnières pour d’autres femmes, d’autres minorités. Embauchons-les, promouvons-les. Que les hommes soient ceux par qui le changement adviendra. Les femmes ne leur demandent pas une discrimination positive, mais juste de ne pas entraver leur carrière.\n\nEn tant qu’individus, utilisons notre pouvoir collectif pour transformer le pouvoir institutionnel par des lois, des choix d’investissement public, des expérimentations sociales, condition pour que notre travail d’équité et d’inclusion ait un impact à grande échelle. Soyons des acteurs délibérés du changement. Comme nous y invite Victoria Dimitrakopulos, catalyseur Social and Emotional Learning (SEL) dans l’éducation primaire et secondaire, « c’est à notre tour de combler les écarts, d’être les gardiens des valeurs humaines, de poursuivre nos aspirations pour le monde sans attendre d’encouragement, (et) de nous sauver ».",
          "Created At": "2025-09-06T13:22:34.126+02:00",
          "Updated At": "2025-09-06T13:22:34.126+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE135_1757157753085",
          "Status": "Ready For Refine",
          "Text": "# Odile Roujol",
          "Created At": "2025-09-06T13:22:34.127+02:00",
          "Updated At": "2025-09-06T13:22:34.127+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE136_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Fédérer la donnée entreprise : les premiers pas d’une Chief Data Officer (CDO)\n\nAprès quinze ans dans des grands groupes comme L’Oréal, Chanel, Saint Laurent, Odile a passé six ans en tant que Chief Data Officer chez Orange France. Elle a ensuite rejoint la Silicon Valley et dirige un fond de consumers brands créé pour des communautés social media et e-commerce engagées sur des problématiques data.\n\n\\> Quels éléments ont été clés pour votre position de CDO ?\n\nCe qui a primé pour moi, en tant que Chief Data Officer, c’est avant tout la connaissance métier. Je ne suis pas ingénieure et ma force (ou mon avantage), c’était la compréhension opérationnelle de ce que nous cherchions à mesurer.\n\nIl m’a fallu aussi un appui au plus haut niveau de l’entreprise, car la gouvernance de données dans un groupe mondial est une entreprise qui va nécessiter la fédération d’équipes variées. Je devais composer avec le marketing, le commercial, l’e-commerce, la production, etc, qui étaient des fonctions qui ne me reportaient pas. Ma position était rattachée au comité exécutif.\n\nJ’ai aussi veillé à entretenir une synergie durable avec ces équipes en combinant ma vision holistique à leur expertise terrain.\n\nEnfin, j’ai poussé l’agilité dans nos projets data. Certes, c’est un mot devenu « tarte à la crème », mais en l’occurrence, pour coller au terrain, comprendre, ajuster, corriger notre compréhension de marchés en constante évolution, je me suis appuyée sur des équipes compactes et rapides, les fameuses « équipes pizza » (pizza team), dont le nombre d’individus doit rester limité à celui d’un groupe qui aurait à se partager une pizza.\n\n\\> Quels défis avez-vous rencontrés ?\n\nJ’ai beaucoup travaillé sur la démocratisation du sujet. Aujourd’hui, toutes les entreprises réalisent que la data n’est pas que l’affaire d’une petite équipe de techniciens ou d’analystes. Tout le monde a un rôle à jouer tout au long de la chaîne analytique. Il y a dix ans, c’était moins commun. J’ai donc développé des programmes d’acculturation avec les ressources humaines, notamment par le jeu.\n\nJ’ai aussi contribué à donner un sens à notre gouvernance de données. Nous avons expliqué notre objectif : au delà d’être data-driven, c’était de mieux servir le client, de mieux le comprendre, de lui proposer de meilleurs produits et des solutions adaptées. Il ne s’agit pas de faire de la data pour de la data.\n\nL’autre grand défi a été celui de la transition de la donnée « froide » que nous utilisions traditionnellement à la donnée « chaude » plus proche du temps réel. Les questionnaires et les enquêtes nous donnaient auparavant des informations avec trois à six mois de décalage. En l’espace d’une décennie, nous sommes devenus capables d’avoir de la donnée très proche du temps réel. L’agilité évoquée plus haut est devenue critique pour constamment s’adapter aux nouvelles opportunités.\n\nEnfin, nous avons dès le départ ancré, si ce n’est sacralisé, le respect de la donnée privée.\n\n\\> Quelles qualités demandez-vous aux data pionniers ?\n\nJ’ai eu la chance de travailler avec D.J. Patel, ancien conseiller data de l’administration Obama et je le rejoins sur la qualité première de l’analyste : la curiosité.\n\nApprendre la data, ce n’est pas compliqué : par contre, avoir l’esprit inquisiteur, la vision critique est une qualité qui est plus personnelle et… plus rare.\n\nFormez-vous, préparez-vous, apprenez à reconnaître vos biais. Ne laissez pas l’IA entre les mains d’équipes seulement techniques et invitez la diversité dans vos groupes de travail.",
          "Created At": "2025-09-06T13:22:34.127+02:00",
          "Updated At": "2025-09-06T13:22:34.127+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE137_1757157753085",
          "Status": "Ready For Refine",
          "Text": "# Virginie Pez",
          "Created At": "2025-09-06T13:22:34.128+02:00",
          "Updated At": "2025-09-06T13:22:34.128+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE138_1757157753085",
          "Status": "Ready For Refine",
          "Text": "## Les nouvelles opportunités data dans la connaissance du client\n\nVirginie Pez est maître de conférences à l’université Paris II Panthéon-Assas et professeure chargée de cours à l’École polytechnique. Elle est titulaire d’un doctorat de l’université Paris-Dauphine (2010) et d’une habilitation à diriger des recherches de l’université Paris II Panthéon-Assas (2018). Celles-ci portent sur les stratégies clients et s’intéressent en particulier au comportement du consommateur et aux aspects psychologiques liés à la consommation. Elle accompagne régulièrement les entreprises sur ces problématiques (conférences de lancement d’ateliers, animation de groupes de travail, participation à des livres blancs, etc.). Elle a, entre autres, coordonné avec ses collègues l’écriture de l’ouvrage Stratégie clients augmentée (ISTE 2019). Ses travaux font l’objet de publications scientifiques dans des ouvrages, des revues académiques (Journal of Business Research, Recherche et Applications en Marketing, Journal of Marketing Management, Journal of Retailing and Consumer Services, Décisions Marketing, etc.), ou des revues professionnelles ou grand public. Virginie Pez est membre du laboratoire de recher­che LARGEPA (université Paris II Panthéon-Assas), du CCM (université Paris-Dauphine) et de i3-CRG (École polytechnique). Elle dirige le Master Marketing et Communication de Paris 2 (Master MC2) au sein de l’université Paris II Panthéon-Assas.\n\n\\> Qu’est-ce qui a changé pour les services marketing et ventes avec les nouvelles capacités d’acquisition et de traitement des data ?\n\nAu départ, rien ne change : les fondements restent les mêmes. Depuis la nuit des temps, les bons vendeurs savent combien il est important de développer un lien particulier avec les clients pour les fidéliser. Même si le fait d’être customer centric est devenu ces dernières décennies le mantra de bon nombre d‘entreprises, le fait de chercher à mieux connaître le client et à le mettre au centre des décisions n’est pas nouveau. Les archéologues ont par exemple retrouvé dans les fouilles de la Rome Antique des tablettes à écrire et des registres faisant état de manière très fine des transactions avec les clients, esquisse de nos bases de données modernes. Gandhi rappelait en son temps à quel point le client était la source de valeur essentielle des relations d’affaires et méritait d’être placé au centre de toutes les attentions : « Un client est le visiteur le plus important de nos locaux. Il ne dépend pas de nous. Nous sommes dépendants de lui. Il n’est pas une gêne dans notre travail. Il en est l’objectif. Il n’est pas étranger à nos affaires. Il en fait partie. Nous ne lui faisons pas une faveur en le servant. C’est lui qui nous fait cette faveur en nous en donnant l’occasion. ». Vous l’aurez donc compris, les fondamentaux restent les mêmes ! L’objectif est de générer une valeur mutuelle et de créer des relations saines, justes et équitables, seuls gages de fidélité. Mais, si cette philosophie reste inchangée, les avancées technologiques contemporaines nous offrent cependant des opportunités d’acquisition et d’exploitation des données clients inédites.\n\nIl y a désormais beaucoup d’informations disponibles, accessibles plus simplement, massivement et de manière plus économique\n\nOn peut mieux connaître son client et plus vite. Les entreprises profitent de chaque occasion de contact pour collecter des données sur leurs clients, et les enrichissent en continu via de multiples démarches.\n\nSur les canaux digitaux, chaque connexion peut être facilement tracée (sous réserve que l’utilisateur y ait consenti bien entendu). Les pages visitées, le parcours suivi, le temps passé par page, les produits visualisés, les informations lues, les produits mis au panier, les produits réellement achetés, etc. sont des informations accessibles et facilement collectées par l’intermédiaire des cookies installés sur les appareils. Il est également aisé désormais de collecter des informations liées à la localisation ou aux usages.\n\nEn magasin physique, la carte de fidélité ou le fichier client sont également des sources d’informations intarissables. Pour y adhérer, les consommateurs se voient demander leur identité, leur adresse ou code postal, leur adresse email, leurs coordonnées téléphoniques, leur date de naissance, voire la composition de leur foyer ou le prénom de leurs enfants. Chaque achat y est ensuite minutieusement enregistré : produits, date, heure, magasin. Ces données sont ensuite croisées pour dresser un profil le plus complet possible du client.\n\n\\> Quelles grandes approches ont changé la donne ?\n\nL’usage des réseaux sociaux : les partages d’informations sociales permettent de comprendre plus de nos clients.\n\nLes informations obtenues de façon directe par la collecte d’informations sur les canaux digitaux et/ou physiques peuvent être complétées par des données issues des réseaux sociaux, de manière plus ou moins automatisée en fonction de la maturité relationnelle des entreprises. Même si cette pratique n’est pas généralisée aujour­d’hui (seulement 40 % des entreprises intègrent des données issues des réseaux sociaux dans leur base de données clients), il est très probable que ces informations soient plus largement utilisées à l’avenir au fur et à mesure que les entreprises se dotent d’outils performants. Les informations issues des réseaux sociaux sont en effet très riches. Chaque individu laisse sur la toile des informations sur son mode de vie, ses goûts, ses inspirations, etc., et ces éléments sont une vraie mine d’or pour les entreprises. Lorsque ces données qualitatives sont rapprochées des données clients issues des autres canaux de distribution (magasins physiques, site web, SAV, etc.), les marques obtiennent une véritable vision à 360° de leur client. Cette mine d’or d’informations, parfois très intimes, est encore peu exploitée aujourd’hui, faute de savoir en industrialiser le traitement. Mais leur utilisation devrait s’intensifier dans les années à venir.\n\nL’application du Machine Learning nous permet de voir des signaux plus fins, et ce, plus rapidement\n\nLes entreprises n’ont pas tardé à s’appuyer sur des machines pour automatiser le traitement des données clients, potentiellement en temps réel, afin d’en tirer du sens efficacement et d’optimiser leur potentiel. Parmi ces techniques, le Machine Learning figure en première ligne. Cette méthode permet à l’ordinateur d’apprendre des données clients sans être programmé de manière explicite. Concrè­tement, il s’agit de faire émerger des modèles de nature à prédire les événements à partir des données. Prenons l’exemple d’un opérateur télécom qui souhaiterait prévenir le départ de ses clients (ce que l’on appelle dans le secteur des services le churn, ou la résiliation client). Il s’agit pour lui de réussir à anticiper le départ de ses clients pour les traiter de manière préventive. En s’appuyant sur le Machine Learning, l’opérateur en question pourrait alimenter l’ordinateur avec une base de données de ce qu’il s’est produit dans le passé, détaillant le profil des clients (en termes de caractéristiques socio-démographiques et de comportements de consommation) et leur fidélité (ici de manière binaire : sont-ils restés chez l’opérateur ou ont-ils résilié leur contrat ?). En analysant l’ensemble de ces données, l’ordinateur va trouver des modèles (ou patterns) prédictifs de la fidélité ou de l’infidélité. L’opérateur pourra ainsi identifier en amont les clients susceptibles de résilier leur contrat, et les traiter de manière adéquate (réduction de prix, offre spéciale, etc.). De cette façon, le Machine Learning permet d’identifier des signaux de manière très fine, si besoin en temps réel, pour aider les managers à la prise de décision.\n\nLa démocratisation de l’accès à l’information dans les réseaux marketing et commerciaux apporte une meilleure connaissance client à tous les niveaux pour un meilleur service\n\nLes données clients sont autant de pépites qui permettent d’offrir un meilleur service aux clients et de développer de la valeur. Mais, pour optimiser leur potentiel, encore faut-il qu’elles soient partagées entre les différentes parties prenantes de l’entreprise. Pour un hôtel par exemple, inutile de savoir que son client habitué préférera une chambre avec douche plutôt que baignoire et qu’il commandera toujours la même chose pour son petit déjeuner (qu’il préférera livré en chambre), si cette information n’est pas accessible et disponible pour le personnel au contact. Lui seul sera en effet en capacité de délivrer et de mettre en musique un tel service personnalisé. La question de l’organisation des données clients et de sa bonne diffusion dans l’entreprise est ainsi clé ! Et la réussite d’une stratégie de marque à l’appui des données clients est étroitement liée à la performance de ses systèmes d’information.\n\n\\> Quels sont les pièges à éviter ?\n\nSavoir naviguer l’éthique et les réglementations\n\nEn analysant et en exploitant les données collectées, les marques ont une responsabilité sociétale dont elles doivent prendre toute la mesure pour que leur approche des marchés fonctionne sur le long terme. Au-delà des réglementations qui s’appliquent (RGPD en tête en Europe), nombreuses sont celles qui se fixent un cadre déontologique et éthique de règles à suivre ou de limites à ne pas franchir. À laisser les algorithmes décider seuls, les questions éthiques qu’ils soulèvent ne tarderaient pas. Par exemple, l’algorithme sur lequel s’appuient les banquiers pour évaluer le risque qu’un prêt ne soit pas remboursé par son contractant aurait vite fait de considérer qu’un habitant de Seine-Saint-Denis a une probabilité plus forte que les autres de se retrouver en impayé… ce qui pourrait injustement stigmatiser l’ensemble des habitants du secteur. Autre exemple, il est fort probable qu’un algorithme de classification des clients qui aurait pour objectif d’identifier les clients les plus susceptibles d’être intéressés par un bon de réduction pour des sucreries recommande de les adresser à des consommateurs suivant un régime, car leur probabilité d’y succomber est forte (en raison de la frustration liée à leur cure, il leur serait difficile de résister, et ils pourraient saisir l’opportunité de ce bon de réduction inopiné pour légitimer un écart !). Les données ainsi collectées permettent de trouver les « failles » permettant de faire succomber les consommateurs.\n\nMais ces questions vont au-delà des enjeux réglementaires et éthiques. Ce n’est pas par philanthropie que les marques s’intéressent à ces dimensions. Un manque de transparence dans les pratiques pourrait en effet altérer la confiance des clients envers la marque (et on sait toute l’importance de ce levier dans la fidélité !). Il pourrait aussi contribuer au rejet des pratiques par les clients, via un mécanisme de réactance psychologique par lequel un consommateur qui a le sentiment de perdre le contrôle sur ses données personnelles va s’opposer aux marques. Une opacité trop importante des pratiques aurait ainsi l’effet délétère de susciter méfiance et défiance. L’éva­luation de la manière dont sont collectées les données clients conduit à la formation d’une attitude (favorable ou non) face à la communication des données demandées et influence positivement l’intention du consommateur de poursuivre la relation avec l’entreprise.\n\nNe pas confondre volume et qualité des données",
          "Created At": "2025-09-06T13:22:34.129+02:00",
          "Updated At": "2025-09-06T13:22:34.129+02:00"
        }
      },
      {
        "json": {
          "Page ID": "PAGE139_1757157753085",
          "Status": "Ready For Refine",
          "Text": "La grande nouveauté des stratégies clients modernes trouve sans doute sa source dans la capacité inédite des entreprises à organiser et à structurer leurs données clients. Mais, s’il est une chose de collecter des données clients, savoir les trier, les consolider et les rattacher aux « bons » clients en est une autre. Collecter les données clients tous azimuts peut être à double tranchant : plus le volume de données recueillies est important, plus il est complexe de repérer à l’intérieur l’information utile, qui permettra un meilleur service rendu aux clients et une plus grande création de valeur pour l’entreprise. Confondre volume et qualité des données est ainsi un écueil duquel il est essentiel de se préserver.\n\n\\> Votre recommandation personnelle pour les data pionniers ?\n\nLes entreprises investissent des sommes et des efforts considérables au service d’une meilleure connaissance client, permettant une création de valeur plus importante pour les utilisateurs. Mais, lorsque le consommateur ne se sent pas justement valorisé du fait de cette meilleure connaissance de son profil ou qu’il se sent mis sous pression par des sollicitations commerciales disproportionnées, celui-ci peut ressentir un malaise psychologique qui va à l’encontre d’un objectif de niveau supérieur à la vente transactionnelle simple et court-termiste : construire des relations harmonieuses entre les consommateurs et les entreprises qui les servent.\n\nAu final, c’est en (re)connaissant le consommateur sous toutes ses formes que des relations harmonieuses et durables pourront être construites de concert avec lui. Protéger les données des consommateurs, c’est préserver la croissance et poser le socle de relations saines, durables, justes et équitables. Agir de manière plus responsable en matière de connaissance client permettrait également de ne pas trop aiguiser l’intérêt du législateur sur ces questions. Sur­endettement, addiction aux jeux, pratiques de ventes forcées ou encore clauses abusives, le législateur s’est déjà saisi de nombreux sujets dans une optique de défense des utilisateurs. Face à l’intérêt grandissant du législateur sur ces questions, il est aussi dans l’intérêt des entreprises de faire preuve de responsabilité et de modération sur la question. Au final, en matière de connaissance client, il faut sans doute réussir à faire moins, mais mieux !\n\n------------------------------------------------------------------------\n\n24. Cette matrice est inspirée des travaux de Toscane Accom­pa­gnement, de Frédéric Laloux et de Ken Wilber.\n\n25. Black mirror est une série télévisée interrogeant les conséquences possibles des nouvelles technologies.\n\n26. Azhar A. Technology is too important to be left to technologists, SiliconRepublic, mars 2019.\n\n27. De l’anglais man, homme, et explaining, explication, ce concept désigne une situation où un homme explique à une femme quel­que chose qu’elle sait déjà, voire dont elle est experte, sur un ton condescendant.\n\n28. Madeleine Albright est diplomate, femme politique et femme d’affaires américaine, ambassadrice américaine auprès des Nations unies de 1993 à 1997, puis secrétaire d’État des États-Unis entre 1997 et 2001 dans l’administration du président Bill Clinton.\n\n29. Maynard P., Sommes-nous vraiment en train de combler l’écart entre les sexes dans la technologie ?, Forbes, mars 2021. Resetting Tech Culture 5 strategies to keep women in tech, Accenture – Girls Who Code.\n\n30. Paloma Median est une experte de l’équité et de l’inclusion, avec une expérience dans la Tech (Etsy, Digital Ocean, and Squarespace) et dans les ONG.\n\nRemerciements\n\nÀ Jan et Paul pour leurs illustrations.\n\nÀ Nolwenn pour la supervision des entretiens des contributeurs de cet ouvrage.\n\nAux éditions Mardaga pour avoir cru en ce projet.\n\nÀ Pierre Desproges pour l’inspiration humoristique de la photo récurrente.\n\nÀ propos de l’auteur\n\n« Quand on me demande pourquoi j’ai cette passion pour la data, je réponds qu’au-delà de l’inspiration de mes parents et de mes mentors, c’est l’attraction pour l’apprentissage : toujours apprendre et découvrir. C’est ce que la Berkeley Haas School of Business définit comme rester Student Always. Et c’est aussi ce que je retrouve dans mes autres passions : le surf, l’astrophotographie, la mixologie (et finalement toutes les disciplines que l’on pratique avec engagement) sont des domaines où chaque jour, chaque nuit me confrontent à de nouveaux challenges, à des erreurs et à des découvertes. Je vis pour cette vie riche et surprenante, même si elle me tire souvent de ma zone de confort. »\n\nGauthier Vasseur a commencé sa carrière en audit, finance d’entreprise et trésorerie, et a relevé de nombreux défis avec ses équipes : mise en conformité à de nouvelles normes internationales comptables, passage à l’an 2000, à l’euro, gestion de crise, agilité et performance de processus d’analyse et de reporting. Mais c’est la maîtrise de la data qui a créé un déclic, lorsque, au début des années 2000, il a compris que l’on pouvait travailler plus vite et surtout plus justement grâce à des techniques simples. Il se lance alors avec l’AFTE, l’Association française des trésoriers d’entreprises, dans le développement de cours et de conférences sur les systèmes d’information en Finance. En 2004, il bascule dans le monde de la technologie : directeur marketing produit pour les solutions de gestion de la performance chez Hyperion en Silicon Valley, racheté plus tard par Oracle ; en 2007, directeur de la gestion de la data financière chez Google ; puis vice-président des solutions data de TriNet, à la veille de son entrée en Bourse ; enfin Chief Operating Officer de la filiale américaine de Semarchy. Le dernier tournant de sa carrière est amorcé dès 2008 avec ses premiers cours data à Stanford. C’est ainsi qu’il s’engage définitivement dans la voie de la formation et crée en 2017 Data Wise Academy à Menlo Park en Californie. En 2018, il rejoint l’Université de Berkeley et prend la direction du Fisher Center for Business Analytics à la Haas School of Business. Il se concentre désormais à la recherche et à l’enseignement académique et professionnel : projets entrepreneuriaux avec Le Pont, un leader de la formation Data/IA en France, missions d’inclusion et d’accès à la formation, et création en 2018 de la Berkeley Alliance for Inclusive AI dont il est le coprésident avec Nolwenn Godard.\n\nCrédits photographiques\n\nPages 17, 18, 22, 29, 53, 65, 93, 119, 125, 172, 207, 222, 225, 231, 249, 275, 279 : Jean-Paul Laurens, L’Empereur Maximilien Ier du Mexique avant son exécution (1882), Wikimedia Commons, domaine public.\n\nPage 115 : Free material from [www.gapminder.org](http://www.gapminder.org)\n\nPage 130 : Peter Kaminski, Wikimedia Commons, CC BY 2.0.\n\nPage 325 : Jenny Freiermuth, Ford Photography.\n\n© 2021 Éditions Mardaga\n\nB-1160 Bruxelles (Belgique)\n\n[www.editionsmardaga.com](http://www.editionsmardaga.com)\n\nCoordination éditoriale : Véronique Dupas et Laura Vanham  \nMaquette intérieure : Carine Thurion\n\nDesign de couverture : Emmanuel Bonaffini\n\nDépôt légal : 2021/0024/85  \nISBN numérique : 9782804720339\n\nToute reproduction ou représentation intégrale ou partielle, par quelque procédé que ce soit, du présent ouvrage est strictement interdite.",
          "Created At": "2025-09-06T13:22:34.129+02:00",
          "Updated At": "2025-09-06T13:22:34.129+02:00"
        }
      }
    ],
    "Parse Markdown Pages": [
      {
        "json": {
          "pageId": "PAGE001_1757157753081",
          "text": "![](Vasseur_Data pionnier_media/Images/1.png)\n\nDevenez un data pionnier !\n\nGauthier Vasseur\n\nDevenez un data pionnier !\n\nComprendre et exploiter les données en entreprise\n\nAdapté des classes de l’auteur\n\n« Become a Data CEO » du programme Continuing Studies Program de Stanford University\n\n« Seven keys to Master Data » de L’Alliance for Inclusive AI à L’Université de Berkeley en Californie\n\nÀ Marie\n\nAvant-propos\n\nCe livre, j’aurais dû l’écrire il y a vingt ans, quand mon parcours professionnel a croisé celui de Philippe, de Salah, de Marie, et un peu plus tard de Nazhin et de Burton.\n\nJ’avais embrassé une carrière en finance, un peu par passion, certainement beaucoup par convention. J’aimais le côté technique du sujet, ses aspects processus et optimisation. J’y rencontrais toutefois beaucoup de frustrations : je passais beaucoup trop de mon temps à faire des choses que je n’aimais pas, telles que de la collecte manuelle de données, des traitements sous tableurs à rallonge. J’avais aussi le sentiment de ne jamais vraiment être en contrôle : toujours à la limite des échéances de reporting, à la merci d’une erreur ou souvent bien trop loin de la réalité opérationnelle, tout enfoui que j’étais dans mes fichiers. À coups d’effort, de week-ends écourtés et de nuits blanches, j’arrivais à continuer à progresser, à faire de nouvelles choses et à casser la routine et la monotonie d’un hamster dans sa roue. Mais la lumière au bout du tunnel ressemblait plus à une faible ampoule, vue par mes yeux fatigués, qu’à un éclat d’espoir.\n\nParfois, il faut un peu de chance, que j’ai eue et que j’ai combinée avec l’énergie (du désespoir) et une insatiable curiosité. En quelques minutes, j’ai réalisé que je pouvais bouleverser ce statu quo, lorsque deux consultants et experts en analytique sont arrivés dans mon bureau : Philippe et Salah. En recherche de projets, ils avaient été orientés jusqu’à moi par mon directeur informatique, soit par pitié pour moi soit par intérêt de se débarrasser d’eux. À leur rencontre, j’ai été bien inspiré de me défaire des certitudes que m’avaient inculquées mes études. En leur ouvrant mes fichiers Excel (mon cœur) et en partageant mes peines, j’ai lu dans leurs yeux la compréhension profonde de mes souffrances et, surtout, cette étincelle de confiance qui semblait me dire : « Tu connais ton métier, on connaît la data, on est tous prêts à fournir les efforts… C’est déjà gagné ! » Trois ans, une quinzaine d’articles et autant de conférences plus tard, mon rôle s’est transformé. Mieux encore, j’ai vu autour de moi grandir et apprendre une équipe qui avait trouvé cette même passion pour l’analytique et la data. Rien ne nous résistait : le passage à l’an 2000, à l’euro, les normes comptables, le multidevise, le juridique, les dérivés, la trésorerie, la dette, le cash, les budgets : nous transposions tous ces challenges en des processus finement huilés qui nous laissaient le luxe de la réflexion et du temps libre. Marie était comme moi novice en la matière et, malgré plus de trente ans de maison, elle fut la première à me rejoindre dans cette aventure. Nous avons appris ensemble, en combinant ma précipitation à son organisation, mon sens de la vision globale à son attention au détail, mon engouement parfois débridé à sa patience. Marie nous a quittés une veille de noël il y a quelques années. Elle restera pour moi l’une des meilleurs analystes avec lesquels j’ai travaillé, mais surtout la représentation avant l’heure de ce que les profils data sont en train de devenir : une harmonie de compétences pour servir des questions et des challenges métier.\n\nEn 2004, décidément à court de vision long terme, je perdais un peu de ma passion dans une routine qui finissait par s’installer, clôture après clôture, budget après budget. Ma route a croisé Nazhin et Burton, cadres exécutifs d’une société en logiciel décisionnel, Hyperion (rachetée par Oracle en 2006). C’est à une conférence qu’ils m’ont demandé pourquoi je ne ferai pas mon métier de cette passion pour la data. Ils avaient mis le doigt sur ce que j’avais toujours eu en face de moi, mais que j’avais refusé de voir : la data et la technologie étaient mes vraies passions. Depuis mon premier PC en 1983, mes premiers prix en programmation, mon premier jeu publié, mes premiers devoirs rendus sous traitement de texte (qui m’avait valu une convocation chez le proviseur à l’époque), mes nuits à faire des petits boulots, à câbler des PC dans des salles de marché bancaires (pour m’offrir un disque dur ou de la mémoire), la technologie avait toujours été autour de moi. Et je dois cela à mon papa, ingénieur, qui m’avait dès le plus jeune âge associé à ses projets.\n\nC’est alors que j’ai « basculé ». En quelques mois, j’ai abandonné douze ans de carrière en finance pour rejoindre la Silicon Valley, sans garantie, sans réelle visibilité, mais avec la certitude de réaliser un rêve caché en moi depuis si longtemps.\n\nQuinze ans et cinq aventures plus tard entre Hyperion (Business Intelligence), Oracle, Google, TriNet (services RH en ligne), Semarchy (solution de Master Data Management) et un bel échec dans une start-up européenne, je comprenais que la richesse de tout ce que j’avais vécu et l’inspiration de professionnels exceptionnels que j’avais croisés devaient être partagées. De mes premiers cours donnés à l’université de Stanford et à l’Association des financiers professionnels (l’AFP), puis le mentorat avec le Google Launchpad jusqu’à aujour­d’hui, je n’ai cessé de me passionner pour l’enseignement. J’avais vu au Fisher Center for Business Analytics de l’Université de Berkeley tant de mes collègues ou employés grandir et se révéler grâce à la maîtrise de l’analytique : je voulais maintenant partager tout ce que j’avais appris et qui m’avait tant apporté. Et je dois cela à ma maman, institutrice qui m’a certainement donné l’engouement pédagogique.\n\nIntroduction\n\nDans un monde volatile, la capacité des entreprises à se réinventer, à agir vite et à s’adapter constamment est une des clés de survie. Les leviers de cette agilité sont le temps et la visibilité. Le temps permet de se poser les bonnes questions, de réfléchir et d’agir. La visibilité permet de se situer, de savoir d’où l’on vient et où l’on doit se diriger. La transformation digitale n’est donc pas un but en soi. Elle est, par la maîtrise des flux d’information et des gains de temps qu’elle procure, un formidable atout pour réussir (chapitre 1).\n\nPionnier de la data, ou Data Pioneer, n’est pas une terminologie officielle, universelle ou approuvée par tous. C’est une expression que j’ai choisie parce qu’elle représente toutes les facettes de l’analyste d’aujourd’hui.\n\nCe chemin du data pionnier est devenu clé dans un parcours professionnel. Dès lors que les organisations vont acquérir une culture data plus forte, les analystes d’hier devront élever leur niveau de jeu pour répondre à des populations plus au fait et en attente des promesses du digital. Ils devront devenir des pionniers, moteur de progrès, d’amélioration, de découverte. Les analyses jusqu’à présent concentrées au niveau des équipes data vont naturellement être reprises par les salariés devenus Data Citizen, laissant aux analystes des responsabilités nouvelles sur des domaines plus en pointe.\n\nComment pouvons-nous aborder ce tournant et nous équiper des compétences clés pour réussir dans ce rôle émergeant de Pionnier ?\n\nCet ouvrage rassemble tout ce qui m’a permis de bâtir ma connaissance en data et en analytique. Il se concentre sur les fondements impératifs à connaître, il intègre les derniers paradigmes et livre des cas réels, vécus. Il ne s’attardera pas sur les modes et les gadgets non nécessaires au quotidien des professionnels. Il passera également sur les querelles de terminologie et de clocher qui ne sont d’aucune utilité pour ce qui nous rassemblera au cours de ces pages, à savoir résoudre les questions et les challenges du quotidien de manière pragmatique, efficace, éthique et durable.\n\nCe livre a aussi pour but de secouer nos schémas de pensée par des prises de position parfois volontairement paradoxales. En nous étirant de part et d’autre d’une pensée molle et convenue de la data, nous allons pouvoir recalibrer nos expériences vers des positions plus claires et plus saines.\n\nLa principale erreur en analytique est de ne pas savoir ce que l’on cherche. Maîtriser son analytique ne sert à rien si on l’applique à la mauvaise question ou à une question mal posée. Cet exercice délicat s’appuie sur l’expérience, la compétence, la curiosité et une dose de courage tout en maîtrisant ses biais cognitifs et de raisonnement (chapitre 2).\n\nAu cœur de l’analytique est la donnée (chapitre 3). C’est la sève, le sang de tout process. La définir avec précision, en connaître sa nature et les conditions de sa fluidité est la condition sine qua non pour une analytique efficace et pertinente.\n\nL’analytique peut se passer de technologie (chapitre 4), mais les volumes et la complexité de la donnée aujourd’hui nécessitent des outils dédiés. Comprendre les solutions qui nous entourent, choisir et mettre en place celles sur lesquelles nous allons nous appuyer sera nécessaire pour développer des processus performants.\n\nLa donnée est une matière première. Sa capture, son organisation, sa valorisation et son analyse font appel à des techniques à la portée de chacun. 80 % de l’analytique se gagne dans ces phases de préparation de données (chapitre 5).\n\nL’élément le plus complexe de la chaîne analytique, c’est l’humain (chapitre 6). La gestion du changement, l’adaptation de notre leadership, la connaissance de nos biais font partie des outils essentiels de tout projet data.\n\nLorsque technologie, données et humains sont prêts, mettre en harmonie leurs interactions est la clé des processus légers, efficaces et durables (chapitre 7). Les processus Creeper paralysent nos entreprises. Des processus lean font gagner des heures chaque semaine pour nous permettre de nous concentrer sur les missions à valeur ajoutée.\n\nVous avez tous les outils en main. Il faut maintenant commencer. Le dernier pas, c’est avec votre cœur que vous le prendrez. Il vous faudra du cœur pour franchir les obstacles du quotidien de la data, pour faire preuve d’humilité, de curiosité et de collaboration, et pour commencer ce cheminement d’apprentissage. Il vous faudra du cœur pour grandir en architecte et chef d’orchestre de votre donnée et pour faire de votre maîtrise du temps et de l’insight un formidable tremplin pour l’innovation.\n\nLe succès en data repose sur la compréhension de ces fondamentaux. Il s’appuie aussi sur la pratique. L’excellence ne viendra que par la répétition continue de ces gestes et de ces réflexes. Enfin, le progrès que nous ferons sera une fonction de notre engouement, de notre appétit d’apprendre, de notre curiosité et de notre courage devant les défis et les nouveautés que nous rencontrerons.\n\nSi vous avez ce livre, vous démontrez déjà ces qualités. Vous vous démarquez déjà d’une majorité de professionnels qui se contentent de lectures rapides et de mots à la mode. Vous faites la différence entre le dire et le faire. Vous vous présentez aux côtés de ceux qui bâtissent ce que sera la data de demain dans nos entreprises, mais également dans nos sociétés.\n\nAlors, pour votre engagement, pour votre passion, bravo et bienvenue dans un monde de découverte et d’apprentissage sans fin. Bienvenue chez les Pionniers de la data.\n\nCHAPITRE 1  \nLes racines de la transformation digitale\n\nLa direction générale annonce son nouveau projet digital."
        }
      },
      {
        "json": {
          "pageId": "PAGE002_1757157753082",
          "text": "Ne sommes-nous pas souvent frustrés quand nous voyons quelqu’un justifier une décision, une action ou un comportement… parce qu’elle est « importante », parce qu’elle est « bonne », parce que « tout le monde le fait », parce que « tout le monde en parle » ? La data, l’analytique, la transformation digitale sont des domaines riches en ce type de dogmes. Ces affirmations ne sont pas mauvaises et ont le mérite d’être en ligne avec une tendance observée, validée et pertinente.\n\nNéanmoins, ces raccourcis et simplifications peuvent s’avérer contre-productifs dans une dynamique de changement et d’adhésion au changement. Si, dans le meilleur des cas, nous avons la profondeur de compréhension du sujet et n’éprouvons plus le besoin de nous répéter les détails, la perception de nos propos par notre entourage, elle, va s’appuyer sur ces seuls termes simplistes. Cela peut être suffisant pour commencer une transition, mais cela n’aura plus guère de poids lorsque les premières difficultés apparaîtront.\n\nSi, tout au long de notre courbe d’apprentissage, notre seule motivation est guidée par « parce que c’est important », nous risquons rapidement de lâcher prise. Le « c’est important » doit avoir une substance que nous pouvons comprendre, que nous pouvons appliquer à ce que nous faisons, et surtout un ancrage dans des éléments stratégiques ou tactiques pertinents pour justifier nos efforts. L’objectif de ce chapitre est de fonder l’impératif de la data dans une logique claire et d’apporter des arguments et des exemples pour l’illustrer, afin que nous soyons les meilleurs ambassadeurs de la data.\n\nDepuis mes premiers pas en data, j’ai constamment remis en cause la raison d’être de ma passion. Je voulais être sûr que je ne lui succombais pas en perdant mon pragmatisme et mes obligations professionnelles pour suivre une mode. Je ne suis jamais parvenu à changer d’opinion et j’ai d’ailleurs pris le risque de m’engager dans ce domaine en laissant beaucoup derrière moi, en quittant ce pour quoi j’avais été formé pendant cinq années d’études, puis en me tournant complètement vers l’éducation, abandonnant ma carrière d’entrepreneur. Je ne faisais pas de la data parce que c’était dans l’air du temps, mais parce que j’avais la conviction que c’était un sujet clé, bien avant qu’il devienne Big (Data).\n\nPour comprendre l’importance de la maîtrise de la donnée et de l’analytique aujourd’hui, il faut prendre quelques pas de recul."
        }
      },
      {
        "json": {
          "pageId": "PAGE003_1757157753082",
          "text": "# Les défis d’un monde en constante évolution\n\nL’annonce du cygne noir.\n\nQuels sont les éléments de succès d’une organisation ? Nous pouvons penser à ses profits, à son cash flow, à la satisfaction client ou à sa domination mondiale. Mais il y a un élément plus fondamental : sa survie ou son développement durable. Qui en effet lancerait une entreprise sans objectif de durer, ou au moins d’avoir la maîtrise de sa durée de vie ?\n\nCe point évident a pendant longtemps été couplé avec la relative assurance que, une fois sur des rails et une taille critique atteinte, une entreprise pouvait se considérer hors de danger. C’était le fameux too big to fail (trop gros pour échouer) qui donnait aux grands groupes mondiaux l’assurance qu’ils vivraient toujours.\n\nC’était sans compter sur l’accélération des économies, sur l’émergence de catastrophes climatiques, politiques, terroristes, boursières, sanitaires plus fréquentes, sur la révolution digitale et ses opportunités exponentielles, et sur l’émergence de nouveaux modèles insufflés par des start-up audacieuses. En quelques décennies, l’environnement de l’entreprise s’est considérablement transformé et s’est rempli d’incertitudes."
        }
      },
      {
        "json": {
          "pageId": "PAGE004_1757157753082",
          "text": "# Le darwinisme digital\n\nLe darwinisme digital est un concept développé par Bryan Solis. Il transpose la théorie de l’évolution de Charles Darwin au monde de l’entreprise et postule que cette dernière est comme un être vivant : elle doit s’adapter, évoluer, se transformer, si elle veut survivre. Il ajoute également que cette transformation digitale ne doit pas être limitée à la technologie ou à la data, mais qu’elle doit également toucher les effectifs de l’entreprise, le leadership et la culture.\n\nLa fin d’un règne\n\nUne histoire naturelle qui illustre bien l’aspect caduc d’une domination par la taille et la puissance est celle du Mégalodon. Ce requin, aujourd’hui disparu, mesurait plus de 20 mètres de long. C’est certainement le plus grand prédateur que les océans aient connu. Et pourtant, après vingt à trente millions d’années de règne, il s’est éteint. Ce n’est pas la collision de notre planète avec le funeste astéroïde il y a 66 millions d’années qui a mis un terme à son existence. Les paléontologues estiment que c’est sa physionomie et sa physiologie qui sont en cause. Il était trop lent pour capturer suffisamment de nourriture pour sa taille. Les requins de taille plus réduite ainsi que d’autres poissons ou mammifères marins plus agiles ont probablement contribué à limiter son accès à des sources de nourriture. Enfin, sa mobilité plus réduite l’a peut-être empêché de migrer vers des eaux à température viable lors de grands épisodes climatiques de notre Terre.\n\n→ Mais où sont-ils ?\n\nÀ la fin de mes études, me destinant à une carrière financière, je contemplais de nombreuses entreprises alors au pinacle du secteur pour bon nombre de jeunes professionnels. Arthur Andersen et Lehmann Brothers étaient parmi les sociétés les plus en vue. Je m’amuse de voir certains de mes étudiants aujourd’hui ne même pas se souvenir de ces noms. Une de mes statistiques préférées est l’évolution du nombre d’entreprises du Fortune 500 (500 plus grandes entreprises) de 1955 jusqu’à aujourd’hui. Il ne reste que 71 de ces groupes de l’époque dans le classement actuel. Les autres ont été soit dépassés par des évolutions trop rapides ou les aléas de notre monde, soit réduits à néant par la compétition."
        }
      },
      {
        "json": {
          "pageId": "PAGE005_1757157753082",
          "text": "# Les clés de la survie\n\nDe quelles armes disposent les entreprises pour assurer leur survie ? Même si la tentation est grande de dire « la data », si nous remontons à un niveau plus stratégique, nous nous rendons compte que ce ne sont pas les analyses qui vont sauver les organisations (sauf si l’analytique est leur produit ou service final). Ce qui va compter, qui semble évident et qui pourtant est régulièrement occulté par les mirages de la technologie, c’est :\n\n– l’innovation ;\n\n– l’agilité ;\n\n– la résilience ;\n\n– l’efficacité ;\n\n– la qualité des produits et des services.\n\nNous pouvons facilement nous accorder sur ces points, même si souvent nous demandons à des cabinets de consulting de nous les rappeler à grands frais. Le problème est que, si cette constatation est évidente, voire triviale, engager une organisation sur ces sujets ne se décrète pas. On peut certes apporter des éléments de confort ou de support propices à les encourager ou rédiger de grandes chartes, voire investir dans des toboggans et des baby-foot, mais c‘est rarement suffisant pour déclencher des engagements durables. Travailler la culture de l’entreprise va également être un facteur clé pour engager ces énergies, mais cela reste un travail de longue haleine souvent délicat.\n\nIl existe deux leviers immédiats pour déclencher ces comportements innovants, audacieux et collaboratifs :\n\n– garantir une visibilité de tous les instants sur notre situation ;\n\n– gagner le temps pour réfléchir, échanger et agir.\n\nSi nous savons où nous sommes, d’où nous venons et où nous pouvons aller, si nous avons le temps de nous poser les bonnes questions, de mettre en place une équipe et surtout de prédisposer notre esprit à une réflexion calme et posée, alors nous allons naturellement commencer à entrevoir des solutions nouvelles et certainement meilleures.\n\nMême en n’identifiant des axes de progrès ou des solutions que marginalement supérieurs, la récurrence de ce processus va nous donner un vrai avantage. Les succès sont rarement bâtis sur des coups de génie : ils cachent souvent un travail d’équipe, de multiples échecs et des réajustements de stratégie douloureux. Ce travail itératif est une des clés de la réussite dans toutes les start-up que j’ai connues. Cette même logique s’applique aux équipes, aux départements ou aux divisions d’organisations : nous ne gérons pas avec des coups de poker, mais nous progressons constamment en nous adaptant à ce que nous découvrons et en apprenant de nos échecs.\n\nSi le concept est simple et plutôt intuitif, la question est de savoir comment on déclenche ces comportements et ces dynamiques. Il est évident que l’approche par diktat peut fonctionner à court terme : forcer les équipes dans des réunions de brainstorming, récompenser les nouvelles idées ou encore sanctionner le manque de résultat est un processus simple à mettre en œuvre. Mais qu’en est-il de la substance créée ? La vraie bonne idée ne vient pas d’un cerveau fatigué et sous pression. Elle vient d’une vague d’itérations rapides de pensées d’un groupe collaboratif et se raffine à l’épreuve du temps. C’est toute la valeur qu’apporte une donnée maîtrisée que nous avons vue plus haut : une visibilité claire et pragmatique et des gains de temps significatifs.\n\nVoilà pourquoi grandir nos compétences data et analytiques est clé. En tant qu’individu, le temps et la visibilité nous permettent de valoriser notre expérience. En tant qu’organisation, nous pouvons nous reposer sur des équipes vraiment engagées dans les défis stratégiques, parce que libérées du pensum du data crunching (bricolage manuel de la donnée).\n\n→ Des gains de 90 %\n\nDès mes premiers pas en analytique, je me suis intéressé aux gains de temps produits par le design de processus optimisés (grâce aux principes partagés dans cet ouvrage). Depuis vingt ans, je retrouve les mêmes mesures. Même des processus manuels considérés comme courts (souvent sur tableur), perçus comme efficaces et ne nécessitant que « quelques minutes », finissaient souvent par prendre au moins trente minutes de labeur. Leur version optimisée était opérée sous la barre des trois minutes sans efforts et surtout sans risque de mauvaise manipulation. 90 % de temps de réduction est devenu une norme à laquelle je me suis habitué et que j’ai systématique réinvesti pour continuer à apprendre."
        }
      },
      {
        "json": {
          "pageId": "PAGE006_1757157753082",
          "text": "# La data est-elle vraiment le nouvel or noir ?\n\nLa data, ce n’est pas toujours le nouvel « or noir ».\n\nCette métaphore est doublement intéressante pour notre réflexion.\n\nBien que séduisante, elle induit une conception erronée, voire contre-productive de la data. Celle-ci en tant que telle ne vaut rien. Nous sommes entourés d’une infinité de données et la majorité ne sont d’aucune utilité à un instant t. Cette illusion de l’Eldorado de la data a conduit de nombreuses entreprises à se lancer dans des courses frénétiques de centralisation d’informations sans but particulier, si ce n’est d’apporter un rassurant : « Nous l’aurons au cas où. » Après des années de collecte et des millions d’euros ou de dollars engloutis, ces lacs de données sont devenus des marais inextricables. Les données jamais contrôlées ou alignées ont perdu de leur pertinence.\n\nAvec le temps, l’intérêt statistique perd de sa valeur et ce ne sont pas toujours les plus grands échantillons qui font la pertinence d’un résultat statistique, car ils introduisent souvent des biais historiques et des problèmes de consistance dans le temps.\n\nEn réalité, la data, comme le pétrole brut au sortir du puits, n’est d’un intérêt que très limité. Si ce dernier n’est pas utilisé, sa valeur devient même négative car son stockage entraîne des coûts. Ce qui va donner sa valeur à l’or noir comme à la data, c’est notre capacité :\n\n– à la transformer et à la raffiner efficacement pour la rendre utilisable par les différents acteurs de la chaîne de décision. Le pétrole brut n’a intrinsèquement que peu d’applications. L’embaumage de momie ou l’étanchéisation de coque de bateau de bois n’étant plus très porteurs, c’est grâce aux techniques de raffinage que l’or noir a pu livrer tant de dérivés utiles à l’industrie et à notre quotidien. La data n’est pas différente. Seule une data préparée, nettoyée, arrangée, agrégée ou connectée peut servir l’analytique. Sans capacité de transformer de manière efficace, durable et précise la data, une organisation ne pourra en tirer qu’une valeur réduite, et ce, au prix d’efforts opérationnels coûteux ;\n\n– à trouver des applications concrètes. Le pétrole brut et les hydrocarbures sont incontournables, car ils sont devenus des sources d’énergie, des briques pour la chimie de composants complexes ou des produits dérivés. On les retrouve ainsi omniprésents dans des applications de notre quotidien. De la même façon, la donnée ne crée de la valeur qu’à partir du moment où elle peut être appliquée à une problématique. Si elle n’est pas attachée à un objectif, elle ne peut être rattachée à une valeur potentielle. Elle devient rapidement un centre de perte.\n\nLe voyageur et le guide\n\nCette fable illustre les paradoxes de certaines courses à la data.  \nPour qui aime voyager, découvrir de nouveaux pays, vivre l’expérience de nouvelles cultures, préparer son voyage est une phase importante. Ce que chacun va chercher dépend de ses centres d’intérêt, de la saison de la visite, du budget et du temps sur place. Un voyage de quinze jours va prendre des mois de préparation et de collecte d’informations.  \nImaginons maintenant une approche similaire à celle que certaines organisations ont choisie à grands frais.  \n« Dès aujourd’hui, je vais récupérer toutes les informations de tous les pays à toutes les saisons, parce que, sait-on jamais, je pourrais un jour être amené à les visiter. Autrement dit, je vais chaque année acheter tous les guides touristiques de la planète que je vais ranger consciencieusement dans ma bibliothèque achetée à cet effet. Je vais même développer un système de gestion de ces ouvrages tant il y a de pays, de régions et d’éditeurs à mettre à jour chaque millésime.  \nLes années passent et je me décide enfin à partir quelques semaines visiter une partie de l’Afrique occidentale. Avec fierté, et non sans en parler à tous les dîners entre amis et collègues, je me mets à compulser les nombreuses années de guides et les multiples éditeurs différents sur le sujet. Une belle masse d’informations qui, sans nul doute, me permettra de faire le voyage parfait.  \nMais la réalité est différente. Les vieux guides sont périmés, certains éditeurs se concentrent sur des thèmes qui ne sont dans mes centres d’intérêt et puis je n’ai pas le temps de faire une analyse en profondeur de dizaines de milliers de pages. Je comprends que mon retour sur investissement temps est nettement plus intéressant si je me concentre sur une sélection récente de guides et que j’emprunte ou achète quelques ouvrages vraiment spécialisés sur ce que je veux voir : la faune, l’histoire, l’art, etc.  \nUn grand sentiment de frustration m’envahit alors : tout cet argent dépensé pour ces milliers de livres, tant de papier gâché. L’in­for­mation pertinente dont j’ai besoin tient finalement sur ma table de nuit et ne m’aura coûté que quelques dizaines d’euros. J’essaie de me rassurer en me disant qu’avec une décennie de guides j’ai accès aux tendances, aux évolutions du pays : mais est-ce là une donnée vraiment pertinente pour ma décision de voyage ?  \nN’est-ce pas une donnée que je peux récupérer en ligne sur des services spécialisés à payer quand j’en ai besoin ? Cette donnée va-t-elle vraiment influencer mes choix de visites ? Finalement, n’est-ce pas ma passion de visiter ce qui me plaît, indépendamment de toute donnée socio-économique ou autre ?  \nDe façon pragmatique, la décision et l’organisation de mon voyage dépendront de facteurs beaucoup plus terre à terre, voire d’éléments d’imprévus qui font partie de toute expédition. Les statistiques et les simulations pointues tomberont rapidement devant la réalité de terrain (cela me fait penser aux plans de bataille à l’armée !). Le succès de mon voyage dépendra principalement d’une information fraîche, ciblée et facilement accessible pour être adaptée aux situations changeantes.  \nAlors, sans rien dire à personne, je vais garder près de moi ces quel­ques guides et préparer mon voyage. Je ne manquerai pas néanmoins de faire visiter ma collection massive de guides “tous les pays depuis quinze ans” dans ma bibliothèque en noyer massif équipée de son système informatique dernier cri d’indexation et de recherche, et de régulièrement publier photos et commentaires pour assurer au monde que tout cela était une bonne idée ! »"
        }
      },
      {
        "json": {
          "pageId": "PAGE007_1757157753082",
          "text": "# Trois illustrations non conventionnelles des bénéfices de la maîtrise de la data\n\nOn ramène souvent l’exercice de la data à celui de l’analyse financière, de la Data Science ou de la mesure d’une performance. La maîtrise de la chaîne analytique a une portée beaucoup plus large illustrée par les trois exemples ci-après."
        }
      },
      {
        "json": {
          "pageId": "PAGE008_1757157753082",
          "text": "## Protéger ses dirigeants de la prison (Keep your execs out of jail)\n\nPendant plusieurs années, c’était l’objectif numéro 1 des équipes financières d’un grand groupe de la Silicon Valley qui n’avait clairement pas à se soucier de problème de rentabilité. Cette mission l’emportait sur la performance des processus budgétaires (objectif numéro 2) ou la rapidité des clôtures comptables (objectif numéro 3). La maîtrise de la donnée devait se concentrer sur la transparence des processus, l’application des normes et surtout la remontée des informations régulières permettant d’identifier les expositions aux risques pénaux."
        }
      },
      {
        "json": {
          "pageId": "PAGE009_1757157753082",
          "text": "## Préparer sa survie aux cygnes noirs\n\nLes cygnes noirs sont des événements aléatoires, hautement improbables, qui jalonnent notre vie : ils ont un impact énorme et sont presque impossibles à prévoir1.\n\nLa préparation aux cygnes noirs est par définition impossible. Il convient donc, pour mieux les appréhender, de bâtir des processus super agiles et des analytiques rapides. Par exemple, pour les compagnies aériennes, le 11 septembre aura été un cygne noir, compte tenu des disruptions qu’il a entraînées en termes de baisse du trafic, d’annulations de vols, de hausse des prix des carburants, etc. Comment la data peut-elle jouer un rôle dans ce contexte ? Pour une compagnie aérienne américaine, Southwest Airlines, c’est l’agilité avec laquelle elle a su modéliser ses modèles économiques et simuler ses choix tactiques qui lui ont permis de conserver une activité bénéficiaire. Même lors de la pandémie de Covid-19, la société, qui n’a cette fois pas pu éviter des pertes, s’est appuyée sur son agilité pour s’approprier des itinéraires délaissés par des compagnies en souffrance."
        }
      },
      {
        "json": {
          "pageId": "PAGE010_1757157753082",
          "text": "## Faire de la data un non-sujet\n\nCette approche a priori paradoxale illustre parfaitement le concept détaillé plus haut : la data n’a intrinsèquement pas de valeur si elle n’est pas appliquée à un problème. Dans le cas de ce manufacturier d’armes du Royaume-Uni, le point de sa directrice financière était de dire que son activité ne présentait pas vraiment d’aléas économiques ou de recouvrement. Par conséquent, les reportings mensuels, les analyses budgétaires et les beaux tableaux PowerPoint n’étaient pas son obsession. Son risque, comme celui de l’entreprise, était la sécurité de la donnée : le vol d’un fichier clients aurait des conséquences graves, de sécurité de portée nationale. Avec son équipe, elle avait fait de la data un « non-problème ». Elle avait réduit le processus à des automatisations parfaitement huilées. Ses équipes passaient un temps minimal à « faire de la data » pour se concentrer sur ce qui comptait : l’assurance que les données de l’entreprise restaient confidentielles et protégées tout au long des transactions.\n\nCes trois exemples illustrent à quel point les enjeux de la data doivent être considérés au-delà de la simple maîtrise de l’exercice analytique. Ce qui nous amène à devenir d’excellents analystes doit rester la résolution des challenges de nos organisations de manière durable."
        }
      },
      {
        "json": {
          "pageId": "PAGE011_1757157753082",
          "text": "# Comprendre la data dans sa globalité\n\nL’ancrage de la donnée dans une logique métier implique une définition plus large de la data. Réduire ce monde à la composante information est trop réducteur et ne reflète pas sa richesse.\n\nLa data est un monde à cinq facettes indissociables :\n\n– la donnée : les informations qui nous entourent ;\n\n– la technologie qui va nous permettre de traiter les données de manière efficace et durable ;\n\n– les humains qui les bâtissent et leur donnent une raison d’être ;\n\n– les processus qui les orchestrent et les supportent ;\n\n– l’objectif métier ou personnel qui les unit au sein d’un projet.\n\nCette définition simple possède une autre vertu : elle nous rappelle que le succès de nos initiatives digitales repose sur l’équilibre et l’harmonie de ces cinq piliers. Ces derniers nous évitent les visions tunnel qui nous enferment dans des logiques trop simplistes."
        }
      },
      {
        "json": {
          "pageId": "PAGE012_1757157753082",
          "text": "# Sommes-nous prêts ?\n\nNous pouvons commencer notre cheminement. Nous avons compris que les enjeux de cette quête digitale sont des enjeux pour les sociétés et la société. Les problématiques économiques et humaines seront de plus en plus complexes à résoudre et notre contribution devra être active et éclairée.\n\nSi vous lisez ce livre, vous êtes certainement parmi ceux qui ont compris ces défis. Votre rôle à ce stade est de devenir un ambassadeur de ces idées.\n\nJ’ai longtemps négligé cette mission en me jetant tête baissée dans des problématiques analytiques. Je viens de partager avec vous les recettes qui m’ont aidé à créer l’engouement, l’engagement et surtout la confiance. À vous de jouer !\n\n------------------------------------------------------------------------\n\n1. Taleb N.N., Le cygne noir, Les Belles Lettres, 2011.\n\nCHAPITRE 2  \nCommencer sur de bonnes bases\n\nLa pression pour trouver la bonne question.\n\nAu démarrage, dans la data, nous pouvons nous sentir un peu désarmés devant l’étendue du sujet et la perspective d’un apprentissage sans fin. Néanmoins, même si la destination se dessine au fur et à mesure et que la ligne d’horizon semble s’éloigner sans cesse, la bonne nouvelle est que le point de départ, lui, reste constant dans sa définition.\n\nToute initiative analytique doit commencer par la formulation d’une question ou d’une problématique précise. En apparence simple, ce principe est en fait souvent négligé. Nous allons voir son importance, les raisons qui nous poussent à court-circuiter cette étape et enfin les outils qui permettent de mieux l’aborder."
        }
      },
      {
        "json": {
          "pageId": "PAGE013_1757157753082",
          "text": "# Pourquoi est-ce capital de poser une question pour commencer ?\n\nIl ne nous viendrait pas souvent à l’idée d’entreprendre une recette de cuisine ou un bricolage sans but précis. Même si un projet peut évoluer dans sa mise en œuvre du fait de contraintes ou d’opportunités, savoir ce pour quoi nous commençons à travailler semble assez naturel. Et pourtant, en pratique, cette règle est régulièrement mise de côté dans les projets analytiques : « Donnez-nous la data, on trouvera bien quelque chose », ou « On va tout stocker pour avoir plus de chance de trouver quelque chose », ou enfin « On va mettre de l’intelligence artificielle et elle nous dira ce qui est important ». Si commencer tout de suite pour rendre des rapports tous azimuts est tentant, car cela démontre une certaine maestria et expose vite des « résultats », cette approche aboutit rarement à une vraie création de valeur. Nous produisons certes des mesures, des tableaux et des graphiques intéressants, mais, comme ils ne répondent pas à des besoins concrets, leur valeur est limitée. On devine rapidement le retour sur investissement négatif de ces projets malgré les apparences."
        }
      },
      {
        "json": {
          "pageId": "PAGE014_1757157753082",
          "text": "# Pourquoi est-ce difficile de poser une question ?\n\nDe nombreux facteurs de différente nature rendent cet exercice délicat."
        }
      },
      {
        "json": {
          "pageId": "PAGE015_1757157753082",
          "text": "## L’obstacle des biais cognitifs\n\n« La plupart d’entre nous posons systématiquement les mêmes types de questions, ce qui ne représente que 15 % des possibilités, et ceci afin d’obtenir des réponses qui nous conviennent, ou auxquelles nous sommes habitués », écrit Fabrice Anguenot2.\n\nLes principaux ennemis de la bonne question sont notre habitude, notre routine et notre champ d’expérience.\n\nLes biais cognitifs sont des mécanismes de pensée qui sont à l’origine d’une altération du jugement en nous faisant perdre notre rationnel et notre raisonnement. Lorsque ces biais se produisent, nous ne capturons plus l’information de manière impartiale ou nous ne la traitons plus de manière logique. Ils sont souvent liés à une compréhension limitée d’une situation par manque de temps, d’informations ou de capacités, ou à des facteurs plus personnels tels que des croyances, des habitudes ou de la complaisance avec soi-même3.\n\nNous retrouvons souvent les biais suivants en analytique.\n\n\\> L’aversion à l’ambiguïté ou à l’incertitude\n\nCe biais va nous conduire à favoriser les options que nous connaissons avec leurs risques associés, sans envisager des alternatives à risques inconnus, qui pourraient être moins importants.\n\nCe biais est illustré par le paradoxe d’Ellsberg. Supposons qu’il y ait deux sacs contenant chacun un mélange de 100 boules rouges et noires. Dans le premier sac, nous savons qu’il y a le même nombre de boules de chaque couleur. Dans le second sac, la proportion rouges/noires est inconnue. Nous sommes invités à tirer une boulle dans l’un des deux sacs avec pour objectif de tirer la couleur rouge. Notre aversion à l’ambiguïté et à l’incertitude va nous conduire à préférer tirer la boule du premier sac avec le mélange connu.\n\n\\> L’ancrage\n\nL’ancrage apparaît lorsque nous sommes initialement exposés à une référence qui va ensuite influencer notre jugement.\n\nSi nous sommes dans des discussions budgétaires et que des ordres de grandeur de certains projets marketing que nous analysons sont de l’ordre du million, alors nous serons tentés d’aligner les estimations des autres projets sur des bases similaires.\n\n\\> L’argumentum ad novitatem ou ad antiquitatem\n\nIl s’agit typiquement des raisonnements fallacieux qui nous conduisent à penser qu’une idée est meilleure parce qu’elle est nouvelle et moderne ou l’inverse. Nous allons avoir tendance à sur­évaluer le potentiel ou la qualité de ce qui est récent et à dénigrer les statu quo existants depuis plus longtemps dans le cas du ad novitatem ou de nous retrancher derrière ces mêmes traditions et habitudes dans le cas du ad antiquitatem.\n\nLes organisations qui se veulent tournées vers l’avenir sont des proies faciles pour l’argumentum ad novitatem. Elles vont souvent venir battre en brèche des sagesses validées au profit d’hypothèses futuristes et séduisantes.\n\nL’importance de considérer les biais cognitifs en analytique  \n\nJean Doridot est ingénieur et docteur en psychologie. Il partage son temps professionnel entre son activité de psychologue libéral, ses interventions en entreprise et le développement de l’application de méditation Zenfie, qu’il a créée en 2015. Auteur de plusieurs ouvrages de référence, il enseigne la psychologie en école de commerce et intervient régulièrement dans les médias.  \nQu’est-ce qu’un biais cognitif ?  \nNous avons dans notre crâne un extraordinaire ordinateur. Cette machine formidable calcule incroyablement vite. Elle est capable de mener de front des milliards d’opérations simultanées. Néanmoins, elle est faillible. Sa plus grande faiblesse, c’est également, parfois, sa plus grande force : l’intuition. Cette espèce de faculté à comprendre, et même à prévoir, ce qui se passe et ce qui va se passer.  \nLe problème est qu’il arrive que cette intuition soit complètement fausse. Sur quoi repose le « il est sûr et certain que ce projet va marcher » ? Sur l’analyse complète et exhaustive de données observables, mesurables et vérifiables ? Ou alors sur le désir aveugle d’un entrepreneur plein d’enthousiasme aux convictions communicatives ?  \nIl nous paraît évident que c’est la première option qui doit toujours l’emporter sur la seconde. Et il n’est pas nécessaire d’être un expert pour avoir la conviction que, dès lors que nous entrons dans le monde des affaires, les bons indicateurs, les KPI’s (key performance indicator), indicateurs clé de performance tels que l’EBITDA, devraient prendre le pas sur nos intuitions.  \nVoici une histoire récente qui nous rappelle à une réalité différente.  \nUn jeune homme, sportif, sympa, a un jour la vision de comment le travail de demain va s’organiser. Il envisage des travailleurs nomades et connectés, avec laptop et smartphone comme tout bagage. Bref, il invente les espaces de coworking et crée WeWork. Adam Neumann réussit à lever des sommes colossales, avant que tout ne s’écroule lors de son entrée en Bourse4. Les investisseurs qui ont engagé des sommes folles dans WeWork étaient certainement tous intelligents et rationnels. Ils se fondaient sur des chiffres observables, mesurables et vérifiables. Ils ont sans doute multiplié leurs analyses avant de prendre leurs décisions.  \nCette histoire n’est pas unique. Que s’est-il passé pour les investisseurs de WeWork ? La réponse tient en quelques mots : leurs raisonnements étaient biaisés. L’objectif de cet encart est de détailler un peu plus l’art, o combien délicat, de détecter les biais, et idéalement, de ne pas en faire l’objet  \nLes trois erreurs fondamentales à éviter ou la sainte trinité du data pionnier vigilant  \nBiais 1 : je crois ce que je vois  \nC’est notre plus grande faiblesse. Pour comprendre l’importance de ce biais, imaginez la situation suivante : un étudiant fait les cent pas dehors, juste devant son université. Il vient de terminer son grand oral et attend que les membres du jury l’invitent à entrer pour con­naître la décision : reçu ou recalé. Fébrile, il regarde sa montre, puis allume une cigarette, sur laquelle il tire une profonde et longue bouffée. Imaginons maintenant que nous nous demandions pourquoi ce jeune homme allume une cigarette à ce moment-là ? D’expérience, par ordre d’importance, nous pourrions dire que c’est parce qu’il est stressé, ou qu’il trouve le temps long, ou encore qu’il ne sait pas quoi faire d’autre.  \nÉvidemment, aucune de ces réponses n’est juste. La seule raison qui explique vraiment pourquoi ce jeune allume une cigarette, est la suivante : c’est parce qu’il s’agit d’un fumeur. Car ce même jeune homme, non-fumeur, son jumeau, dans la même situation, par définition, ne fume pas.  \nEt si nous travaillons sur cet exemple, c’est parce que les tabacologues ont prouvé depuis longtemps que le tabac est une plante qui crée du stress, et non le contraire…  \nQue s’est-il passé ? Nous avons vu une personne, en situation de stress, allumer une cigarette. À ce moment-là, notre cerveau, qui cherche toujours des explications à tout, a déduit que c’est ce stress qui a fait allumer une cigarette à ce pauvre impétrant. Alors qu’en réalité le jeune homme en question fume aussi quand il est heureux et qu’il fait la fête avec des amis. Il fume quand il réfléchit sur un problème compliqué. Il fume quand il a du chagrin à cause d’une peine de cœur. En fait, il fume tout le temps, justement parce qu’il s’agit d’un fumeur.  \nNous noterons que, même après cet exemple, nous aurons peut-être du mal à nous convaincre qu’il n’y a aucun rapport entre le stress et la consommation d’une cigarette. C’est une autre caractéristique des biais cognitifs : même quand nous comprenons l’erreur, il est difficile de s’en défaire.  \nExplications  \nLe biais du « Je crois ce que je vois » est très utilisé en marketing et en publicité. Encore aujourd’hui, les marques paient très cher des sportifs, des stars et des influenceurs pour qu’ils s’affichent simplement avec leurs produits. Pourtant, tout le monde sait qu’une paire de baskets ne devient pas meilleure le jour où un grand footballeur signe un accord avec son nouveau sponsor. Mais, ça marche quand même.  \nDans notre métier de data pionnier, la situation sans doute la plus typique des pièges du « Je crois ce que je vois » est la corrélation illusoire. Rien de pire que deux courbes qui nous regardent dans le blanc des yeux, et qui nous prouvent noir sur blanc qu’on ne peut rien en déduire, car, nous le savons certainement, une corrélation ne prouve pas une relation de causalité.  \nAutre exemple, en France, plus les enfants passent du temps devant les écrans, plus leur QI est bas. Cela a été observé maintes et maintes fois. C’est qu’il doit bien y avoir quelque chose, non ? Sans doute, bientôt, l’effet toxique des écrans sur le développement cognitif des enfants sera prouvé, c’est une simple question de temps. Mais ajoutons juste une petite précision avant la suite : à Singapour, c’est le contraire qui est prouvé.  \nSolutions  \nNous l’aurons compris, il est très difficile de lutter contre un biais cognitif. Pour ce qui est du « Je crois ce que je vois », souvenons-nous justement qu’une corrélation n’est en rien une preuve de quoi que ce soit. C’est en ayant conscience que ce biais est bel et bien là que nous développerons la vigilance nécessaire. Bref, souvenons-nous que, si la sagesse populaire s’échine depuis toujours à nous dire que l’habit ne fait pas le moine, c’est parce que, justement, pour notre cerveau, l’habit fait le moine.  \nBiais 2 : l’erreur fondamentale d’attribution  \nC’est sans doute le biais le plus important après le précédent. Retournons pour cet exemple dans les années 1960, lorsqu’un psychologue demande à des psychiatres quelle serait la proportion d’individus, dans la toute la population, capables d’agir en tortionnaires.  \nLes psychiatres de l’époque ont répondu unanimement qu’il ne pouvait s’agir que de personnes pathologiquement perverses et sadi­ques, donc, Dieu merci, moins de 1 %. Le psychologue en question a alors monté une expérimentation dans laquelle plus de 6 personnes sur 10 infligeaient des sévices terribles à de parfaits inconnus pendant une heure, en échange de quelques dollars. Ce psychologue s’appelait Stanley Milgram, et son expérience a fait date. Deux petites précisions supplémentaires :  \n– Stanley Milgram avait demandé à des psychiatres d’évaluer la personnalité de chacun des participants à son expérience. Ils furent déclarés parfaitement équilibrés, ni pervers ni sadiques ;  \n– l’expérimentation en question a été répliquée de nombreuses fois. Récemment, en Pologne, la proportion de sujets allant jus­qu’au bout (une décharge de 450 volts administrée au participant malchanceux) était de 90 %.  \nNous avons tendance à attribuer les actes des uns et des autres à leur personnalité. Pourtant, bien souvent, comme le démontre l’expérience de Milgram, c’est la situation qui explique le mieux ce qui se passe. Ce biais d’attribution peut se décliner de différentes façons :  \n– le biais du champion : il suffit qu’un manager obtienne un résultat exceptionnel pour qu’il soit rapidement débauché par la concurrence. L’année suivante, le succès n’est pas au rendez-vous. La personne recrutée est pourtant la même que l’année précédente. Seul le contexte est différent ;  \n– le produit qu’il vous faut : certaines entreprises font le choix de se doter de solutions qui ne leur conviennent pas, simplement parce que le produit en question est leader sur son marché.  \nSolutions  \nCherchons toujours ce qui ne se voit pas. Lorsque nous analysons des données, amusons-nous à imaginer des paramètres absents, qui pourraient avoir une influence sur les tableaux que nous observons. Notamment, souvenons-nous que la façon la plus sûre d’analyser des données est de recueillir les chiffres de façon expérimentale, avec au moins deux conditions différentes : une condition qui teste notre hypothèse et une condition qui la contrôle."
        }
      },
      {
        "json": {
          "pageId": "PAGE016_1757157753082",
          "text": "Dans la vraie vie, il n’y a jamais de groupe contrôle. C’est pour cela qu’il est si difficile de devenir un bon data pionnier. Lorsque nous analysons des données, c’est le contexte qui explique souvent le mieux ce qui se passe : succès ou échec d’un produit, taux d’absentéisme, croissance, etc. Cherchons des contextes à peu près similaires et faisons des comparaisons. Même si toutes choses ne sont pas égales par ailleurs, nous aurons quand même un semblant de groupe con­trôle. C’est mieux que rien !  \nBiais 3 : l’influence majoritaire  \nL’influence sociale est un « gros morceau » de la psychologie. Com­ment et pourquoi un individu change d’avis fascine les psys depuis longtemps. Parmi les différentes formes que peut prendre cette influence, la plus simple, et sans doute aussi la plus efficace, reste l’influence majoritaire.  \n  \nDans les années 1950, Solomon Hasch a monté une expérimentation très astucieuse. Il montrait à des groupes de sept étudiants trois segments de droite de tailles différentes, à côté d’un quatrième qui avait, clairement et indiscutablement, la même taille qu’un des trois autres. Plusieurs planches étaient projetées, et chaque sujet devait dire à voix haute quel segment, A, B ou C, était de même taille que le segment anonyme. Au début, tout allait bien, mais, après quelques essais, les six premiers sujets de l’expérimentation donnaient une réponse clairement erronée. Ils étaient en réalité des com­pères de l’expérimentateur, et seul le septième sujet était un sujet naïf. Bien sûr, chaque compère donnait la même mauvaise réponse, comme si cela crevait les yeux. Contre toute attente, trois sujets sur quatre se sont soumis au moins une fois à l’influence majoritaire. Dans plus d’un tiers des cas, les réponses données par les sujets naïfs étaient conformes aux réponses, fausses, données par la majorité.  \nExplications  \nL’homme est un animal, et c’est un animal social. L’une de ses plus grandes angoisses est d’être exclu du groupe, mis à l’écart. Aussi, les participants à l’expérience de Hasch ne sont pas devenus subitement atteints de cécité. Ils ont simplement constaté que quelque chose ne tournait pas rond et que, s’ils ne voulaient pas se mettre à l’écart du groupe, ils avaient intérêt à dire la même chose que tout le monde.  \nDans le monde de l’entreprise, ce biais est souvent désigné par le problème de la « pensée de groupe ». À la problématique du nombre viennent s’ajouter les difficultés des rapports hiérarchiques, et des enjeux économiques et politiques qui rythment la vie de toute entreprise.  \nSolutions  \nAcceptons d’être seuls contre tous. Cultivons en permanence notre esprit critique. Et, surtout, trouvons des alliés. Les expérimentations ont observé que l’influence majoritaire disparaît complètement dès lors qu’il n’y a plus un sujet naïf mais deux. Si nous sommes curieux, penchons-nous sur le sujet de l’influence minoritaire. Nous comprendrons qu’il est souvent plus facile d’influencer une réunion de 50 personnes à deux que de lutter seul contre l’avis de trois autres.  \nMes recommandations pour mitiger nos biais  \nNous devons absolument retenir que les biais cognitifs sont comme les effets d’optique. Même lorsque nous sommes au courant, l’illusion est toujours là. Personne n’en est exempt.  \nÉtablissons des check-lists, avant toute prise de décision. Lorsque nous tentons d’expliquer des résultats, recherchons systématiquement trois histoires cohérentes et différentes. Cela forcera notre esprit à élargir sa vision et à aller chercher ce qui ne se voit pas. N’hésitons pas à aller chercher des résultats fiables, et contradictoires les uns par rapport aux autres. Intéressons-nous aux succès des grandes entreprises, et aussi à leurs échecs les plus cuisants. Dans les deux cas, faisons l’inventaire des biais qui ont été à l’œuvre.  \nMes conseils aux data pionniers  \nUne erreur terrible est de croire que celui qui peut expliquer le passé peut faire des prévisions sûres et certaines sur l’avenir. Il n’y a rien de plus faux. Tout le monde aujourd’hui peut se dire qu’il n’est pas si compliqué que ça d’inventer la roue. Il a pourtant fallu à nos ancêtres plus de 60 000 ans pour y parvenir… et 1 000 ans supplémentaires pour penser à faire une brouette.  \nDans le quotidien du data pionnier, nous allons devoir souvent expliquer le passé. Nous allons devoir aussi prévoir l’avenir, le prédire. Souvenons-nous par conséquent que les mêmes causes produisent toujours les mêmes effets. Mais restons vigilants, et ne cherchons pas à déterminer trop vite les causes.  \nJe terminerai par un conte oriental. À la tombée de la nuit, un homme scrute le sol très attentivement, à quatre pattes sous un lampadaire.  \nUn mendiant qui passe par là lui demande ce qu’il fait. Il lui répond : « J’ai perdu les clés de chez moi, alors je les cherche pour entrer dans ma maison. Aide-moi donc ! » Tandis que les deux hommes sont maintenant en train de regarder partout autour du lampadaire, le mendiant demande encore : « Tu es sûr que tu les as perdues là tes clés ? » « Pas du tout », répond l’autre homme. « Mais alors pourquoi tu les cherches ici ? » Et l’homme de répondre : « C’est parce que sous le lampadaire est le seul endroit où il y a de la lumière. »"
        }
      },
      {
        "json": {
          "pageId": "PAGE017_1757157753082",
          "text": "## La perception des données disponibles\n\nParce que certaines questions requièrent des données nouvelles ou inhabituelles, nous sommes tentés de ne pas les poser. Ou de manière plus sournoise, nous posons la question en nous efforçant d’y faire correspondre une mauvaise réponse.\n\n→ Les mauvaises données pour une bonne question\n\nDans un groupe international de grande distribution et de restauration, le contrôle de gestion produit voulait vérifier les opérations de lancement pour un nouveau produit. Alors que le responsable avait commencé par une question ouverte pour lancer la réflexion, « comment allons-nous mesurer l’exécution opérationnelle de ce lancement ? », il fut interrompu par les deux jeunes analystes dédiés à la marque qui claquèrent une réponse au ton définitif : « Il suffit de mesurer le chiffre d’affaires et la marge générée, c’est simple. » Une analyse rapide de cette évidence montrait qu’elle ne correspondait pas à la problématique posée. Ce n’était pas les ventes qui intéressaient le responsable, même si un lancement réussi a un impact favorable sur les métriques financières. Il voulait savoir si le lancement était correctement mené. Son objectif était donc de mesurer si les moyens mis en œuvre étaient suffisants pour atteindre les cibles définies par le marketing en termes de démographie et de géographie. La mesure des ventes ne serait connue que dans plusieurs semaines, voire des mois, trop tard pour corriger une campagne mal gérée.\n\nLes analystes insistèrent d’un ton un peu plus vindicatif : « On ne va pas y passer la journée, on mesure les ventes et voilà ; de toute façon, nous n’avons que cette donnée-là facilement accessible. » Le vice-président de la marque se tourna alors vers ses deux collaborateurs et leur demanda froidement : « Attendez, vous êtes en train de me dire que vous allez répondre à une question par la mauvaise réponse, parce que les éléments qui pourraient donner la bonne ne sont pas directement accessibles ? » Un grand blanc s’installa dans la salle. À la suite de quelques échanges destinés à ne faire perdre la face à personne, le groupe arriva finalement à trouver les bons indicateurs.\n\n» Qu’auriez-vous proposé ? Auriez-vous choisi d’aller contre un statu quo imposé si rapidement dans la réunion ?\n\nVoici quelques-uns des indicateurs qui ont été finalement arrêtés pour ce lancement de produit de grande distribution et de restauration/café/club :\n\n– nombre de commerciaux sur le terrain et leur répartition géographique pendant la campagne ;\n\n– nombre de commerces et de distributeurs démarchés pour la présentation du nouveau produit ;\n\n– type de commerces visités ;\n\n– taille des zones de chalandise touchées ;\n\n– nombre d’échantillons distribués ;\n\n– nombre de promotions laissées sur le lieu de vente.\n\nCes données ont semblé a priori impossibles à collecter du fait de leur originalité ou de l’absence de prise en charge dans les systèmes marketing. Avec une simple compréhension des fonctionnalités de base des outils analytiques et de la nature de ces data, le problème de collecte, de consolidation et de reporting (voir chapitre 5) a été résolu dans l’après-midi qui a suivi, sans nécessiter d’investissement."
        }
      },
      {
        "json": {
          "pageId": "PAGE018_1757157753082",
          "text": "## La compréhension des mécaniques analytiques\n\nLorsque l’analytique est considérée comme une boîte noire, il est délicat d’en connaître ses limites et ses opportunités. Le cas précédent souffrait un peu de cette lacune de connaissance de la part des protagonistes. Ces limitations créent d’autres entraves à la formulation de questions pertinentes. Voici une liste non exhaustive de schémas de pensée autolimitants qui sont pourtant résolus avec des dispositifs d’analyse simples et accessibles à tous.\n\nTableau 1. Quelques schémas de pensée autolimitants faciles à corriger\n\nBien sûr, dans ces exemples, la réponse d’un analyste formé mériterait d’être argumentée et pourrait ne pas couvrir certaines exceptions. C’est néanmoins le niveau de confiance et celui de compétences auxquels nous arriverons par la mise en pratique des éléments de cet ouvrage. Ce bagage de compétences et de connaissance va libérer notre questionnement. Les seuls challenges impossibles seront ceux pour lesquels la donnée est trop chère à acquérir ou interdite d’accès, l’équipement trop onéreux, la maturité de l’équipe insuffisante ou le temps imparti trop court (voir chapitres 4, 5, 6 et 7)."
        }
      },
      {
        "json": {
          "pageId": "PAGE019_1757157753082",
          "text": "## La pression de l’habitude\n\nPartiellement illustré dans un exemple précédent, cet obstacle pernicieux nous empêche de formaliser des questionnements nouveaux de peur de choquer ou de déstabiliser une organisation adepte de la routine. Pourquoi bousculer les habitudes et éveiller les susceptibilités quand des générations d’analystes se sont contentées, avec un certain succès, de reproduire les schémas passés ?\n\n» Avez-vous déjà passé en revue les reportings de votre équipe, minutieusement publiés mois après mois, année après année ? Avez-vous déjà essayé de les faire évoluer ?"
        }
      },
      {
        "json": {
          "pageId": "PAGE020_1757157753082",
          "text": "## Le courage (ou son manque)\n\nPoser une nouvelle question ou surtout une question pertinente évitée jusqu’à présent revêt deux risques :\n\n– elle va déranger, être considérée comme impertinente par ceux que le statu quo arrangeait ;\n\n– elle nous expose en nous mettant dans l’obligation d’y répondre au risque de mettre au grand jour des conclusions non désirées.\n\n→ Des conclusions parfois dangereuses\n\nUn contrôleur financier faillit perdre son emploi pour avoir osé suggérer la mesure de la rentabilité d’instruments financiers sur le long terme, alors que ces derniers avaient apporté gloire et bonus à leur initiateur. Sa motivation était pure : son rôle était de suivre la valorisation des actifs du groupe. Dès le premier jour de mise en place des analytiques de suivi, il sentit une pression croissante sur ces rapports (de nos jours, on appellerait cela du harcèlement). Il n’eut pas le droit de les publier en dehors de l’équipe, et pour cause : trois mois après, les bénéfices des instruments avaient disparu et les appels de marge se mesuraient en millions. La révélation de ces chiffres aurait pu lui coûter son emploi : tout cela pour une question peut-être courageuse, et certainement fondée."
        }
      },
      {
        "json": {
          "pageId": "PAGE021_1757157753082",
          "text": "## La tentation du voisin et de la bonne pratique\n\nLes projets analytiques ont besoin d’une reconnaissance rapide. Parfois nous manquons d’imagination, de temps et de courage pour poser la bonne question (voir supra, Le courage (ou son manque)). Pour pallier cela, nous pouvons céder à la tentation de produire des analyses et des indicateurs en masse au détriment de leur pertinence. Il suffit d’aller voir des confrères dans l’industrie et de copier les analyses standards du métier. Cela procure rapidement du travail pour les architectes de la donnée et donne l’illusion de produire de la valeur. Il y a bien sûr des métriques classiques que nous devrions toujours avoir, mais cette approche comporte aussi des risques : leur production peut créer une vision tunnel et un sentiment de travail accompli. Elle risque de nous limiter dans une zone de confort et de ne pas développer des analyses qui comptent.\n\nLa vision tunnel (tunel vision)\n\nLa vision tunnel décrit la manière avec laquelle nous pouvons nous engager dans des décisions en renonçant à aller au-delà de notre perspective individuelle.\n\nLes analyses à valeur ajoutée seront celles qui répondent vraiment à une problématique de notre entreprise, pas à celle de notre voisin.\n\n→ Nos besoins ne sont pas ceux du voisin\n\nDes équipes d’analystes avaient développé des analyses en pointe dans leur domaine. Le temps gagné grâce à l’efficacité de leurs processus était réinvesti en partie dans l’investigation de nouveaux problèmes. Leurs modèles de données étaient extrêmement raffinés et précis. À de nombreuses reprises, des confrères et des consultants vinrent leur demander de partager leurs KPI et leur structure de données. L’entreprise avait volontiers partagé certaines de ces informations (sans les données dedans bien évidemment), tout en sachant que cela risquait fort d’être contre-productif pour leur acquéreur. Sa situation économique (très précaire à l’époque), sa stratégie, ses contraintes humaines, data, techniques et ses processus l’avaient conduite à développer une analytique très spécifique, avec des biais volontaires en termes de choix de métrique et d’organisation de données. Elle était focalisée sur la gestion de la dette avec ses marchés principaux en Asie, et des processus étaient très en pointe sur la partie bancaire. C’était là une situation singulière, absolument pas applicable aux confrères du domaine.\n\nL’entreprise acquéreur était en excédent de trésorerie, en plein développement aux États-Unis, très manuelle, en grand manque de compétences. Cette copie d’analytique n’a jamais pris, faute de pertinence, mais elle a occupé tout le monde pendant dix-huit mois !\n\nPoser la bonne question nécessite l’exercice de nombreuses qualités professionnelles et personnelles. Se connaître soi-même, maîtriser sa data et sa chaîne analytique et une petite dose de courage sont des fondements que nous devons acquérir pour challenger les statu quo.\n\n→ Commencer par chercher ce que l’on recherche\n\nLe centre que je dirige à Berkeley (Fisher Center for Business Analytics) collabore à de nombreux projets d’étude ou de recher­che avec des entreprises partenaires. Le point d’achoppement est toujours celui de la question. Même dans le contexte de recherche, il est impératif de savoir ce que l’on recherche. Des professeurs de premier plan, qui pourraient sans doute faire de la magie en analytique, s’obstinent avec raison à obtenir cette fameuse question avant de démarrer tout projet. Bien souvent, ce qui paraît un exercice évident est beaucoup plus complexe. Il n’est pas rare de refuser des projets ou d’aboutir à de faux problèmes en raison de l’absence de questions pertinentes."
        }
      },
      {
        "json": {
          "pageId": "PAGE022_1757157753082",
          "text": "# Comment poser la bonne question\n\nLa nature et le contexte des questions changeant constamment, les techniques de questiologie, autrement dit l’art de poser la bonne question, pour optimiser la formulation des questions peuvent nous aider.\n\nLe pouvoir des questions  \n\nSophie Chamayou Degoix est Executive coach certifiée HEC, spécialisée en intelligence émotionnelle, questiologie et programmation neuro-linguistique (PNL). Après un parcours de plus de vingt ans en négociation, marketing et communication, elle décide de mettre ses compétences au service des dirigeants et managers et cofonde Intuitik, cabinet de coaching, formation et conseils. Son accompagnement est basé sur la conviction que chaque être humain peut exprimer son plein potentiel et générer un impact positif sur son environnement et sur le monde.  \nComment pouvez-vous aider votre interlocuteur à grandir, à s’enrichir, à changer de posture ? Comment pouvez-vous modifier le comportement et l’engagement de vos collaborateurs, de vos clients ? Comment pouvez-vous obtenir des résultats plus impactants ?  \nL’une des clés est de formuler des questions suffisamment puis­santes et confrontantes pour provoquer chez votre interlocuteur un « bug cérébral » qui va lui permettre de reconsidérer ses certitudes et d’explorer d’autres chemins.  \nSocrate, avec la maïeutique (l’art d’accoucher les âmes), ne cessait de questionner ses disciples afin de les inciter à remettre en question leurs idées habituelles. Ses questions démontaient les concepts qui sous-entendaient les argumentations pour pousser encore plus loin les réflexions et développer la pensée critique. D’ailleurs, Socrate offrait ses questions comme des cadeaux à ses disciples, ce qui les rendait plus importantes que les réponses elles-mêmes.  \nEinstein, également, portait une attention particulière aux questionnements : « Si je disposais d’une heure pour résoudre un problème et que ma vie en dépende, je consacrerais les 55 premières minutes à définir la question appropriée à poser, car, une fois cela fait, je pourrais résoudre le problème en 5 minutes. »  \nIl a, ainsi, découvert la théorie de la relativité en sortant des schémas de pensée habituels et en se posant une question qui a complètement modifié notre compréhension de l’univers et de la gravité : « À quoi l’univers ressemblerait-il si je me déplaçais à la vitesse de la lumière ? »  \nUne question pertinemment posée peut donc bousculer notre mode de pensée et être à l’origine d’une découverte révolutionnaire.  \nComment distinguer une question classique d’une question puissante ?  \nTout d’abord, une question puissante est une question ouverte, c’est-à-dire qu’elle donne la possibilité d’avoir une infinité de réponses. Avec de puissantes questions ouvertes, ni le demandeur ni le répondant ne connaissent la réponse. Elles permettent, donc, si elles sont aidantes, de faire émerger une idée inédite en activant la créativité de l’interlocuteur. Elles impliquent les personnes et développent la coopération et l’engagement.  \nMais, force est de constater que, dans notre quotidien, 80 % de nos questions sont fermées. Elles commencent soit par un verbe, soit par « Est-ce que » ou « Y a-t-il », ou bien encore par une assertion négative « Ne pensez-vous pas que ». Ce sont des questions qui attendent une réponse par « oui » ou « non » et qui illustrent, la plupart du temps, l’expression de nos propres pensées.  \nEn posant des questions fermées, votre intention est de valider votre mode de pensée (« Ne pensez-vous pas que le télétravail est source de responsabilisation ? »), de prendre le contrôle (« Pouvez-vous relancer vos clients ? ») ou de fixer un cadre (« Êtes-vous d’accord ? »). Ce type de questionnement ne vous permet pas d’élargir votre compréhension du monde, et ne développe pas non plus le niveau de réflexion de votre interlocuteur, voire il le déresponsabilise et le désengage.  \nEn transformant vos questions fermées en questions ouvertes, vous transformez également la posture de votre interlocuteur. Par exemple, « Pouvez-vous relancer vos clients ? » induit une réponse déresponsabilisante avec un « oui » ou un « non ». Par contre, si on la modifie en « Qu’allez-vous mettre en place comme actions pour ne plus avoir à relancer vos clients ? », alors elle prend une autre dimension et redonne du pouvoir à l’interlocuteur. De simple exécutante, la personne qui répond va modifier son état d’esprit et utiliser ses capacités de réflexion et de créativité pour trouver sa propre solution au problème.  \nUne question puissante va ainsi vous permettre de sortir de vos schémas de pensée habituels et d’envisager d’autres champs des possibles. Si vous posez toujours les mêmes questions, vous obtiendrez toujours les mêmes réponses et vous serez limité dans vos possibilités. Alors que, en questionnant vos certitudes, vous bannissez vos réponses définitives, celles qui vous confortent dans vos croyances, dans votre vision du monde et vous limitent dans votre développement. Vous découvrez qu’il n’y a plus une seule vérité mais plusieurs, à partir desquelles en jaillissent d’autres. La bonne question devient un catalyseur de développement d’idées, stimulatrice de créativité.  \nIl est d’ailleurs frappant de constater que les leaders les plus créatifs, ceux qui contribuent aux changements du monde, sont ceux qui (se) posent le plus de questions. Elon Musk, par exemple, privilégie le questionnement aux réponses : pour aborder l’innovation, il commence par formuler la question qu’il souhaite adresser. Il développe ensuite des hypothèses, qu’il reconsidère une nouvelle fois par une autre question, jusqu’à ce que la réponse soit « probablement vraie ». Il remet donc sans cesse en question ses idées, ses réponses, ses conclusions, sans tenir compte des évidences, des croyances et des habitudes.  \nUn questionnement donnant-donnant  \nUn bon questionnement va déterminer également la qualité de vos relations. Questionner va vous permettre de rentrer en contact avec quelqu’un, d’appréhender son monde et d’enrichir le vôtre. Cela vous permet de comprendre qu’il n’y a pas une seule vérité, que chacun a des perceptions différentes de la réalité, et vous accueillez celles des autres comme point de départ d’une nouvelle réflexion. En privilégiant la connexion et les échanges constructifs, vous comprenez mieux les besoins et les attentes de l’autre personne et vous pouvez donc mieux y répondre. Vous développez votre empathie et votre tolérance.  \nUn autre objectif du questionnement puissant est de développer votre capacité à impliquer votre interlocuteur et à le mettre en mouvement. Les questions posées vont l’aider à trouver son chemin et à faire son premier pas. Par exemple, avec des questions confrontantes, vous pouvez faire bouger les référentiels : « Quelles nouvelles habitudes allez-vous mettre en place pour développer un état d’esprit positif ? » ou « Comment votre singularité va contribuer à rendre ce projet exceptionnel ? ».  \nD’autre part, pour éviter les justifications, vous devez limiter les questions commençant par « Pourquoi », sauf si vous demandez à votre interlocuteur d’y répondre avec « pour » (et non pas « parce que »). En faisant cela, vous allez chercher sa motivation, son sens et non pas une justification qu’il se sent obligé de donner et qui le place en position défensive. Par exemple, à la question « Pourquoi faites-vous comme ça ? », préférez « Quel est le bénéfice de faire comme ça ? ».  \nPour développer sa motivation et aller chercher ses valeurs, vous pouvez poser une autre question puissante comme « Qu’est-ce qui est important pour vous ? » ou « Quand vous aurez atteint votre objectif, qu’est-ce que cela va vous apporter de plus d’important ? ». Vous obtiendrez ainsi la valeur essentielle pour la personne, celle à laquelle elle va se raccrocher et se mobiliser pour atteindre son objectif.  \nDe l’importance du temps du verbe dans la question  \nLe temps utilisé dans les verbes des questions est également très important et peut renforcer leur impact. Bannissez le conditionnel qui ne donnera que des réponses conditionnelles et donc pas engageantes ni responsabilisantes. Privilégiez le futur qui permet de se projeter dans un état désiré. Au lieu de demander « Que feriez-vous si vous n’aviez plus peur ? », préférez « Que ferez-vous quand vous n’aurez plus peur ? ». Projeter quelqu’un dans un futur de réussite avec une visualisation est extrêmement puissant pour l’inciter à l’action et développer sa motivation : « Comment saurez-vous que vous avez réussi ? Que ressentirez-vous ? Qu’observerez-vous ? Que vous direz-vous ? Avec qui serez-vous ? »  \nEn posant des questions au passé, vous permettez à votre interlocuteur d’identifier et de comprendre les causes de son problème.  \nEnfin, les questions au présent vont inciter votre interlocuteur à mettre en place les actions nécessaires pour concrétiser son objectif.  \nDonc, si vous souhaitez débloquer une situation et trouver des pistes d’actions concrètes, commencez par poser des questions au passé (les causes), puis projetez la personne dans la réussite de son objectif avec des questions posées au futur (la visualisation) et, enfin, ramenez-la au présent en statuant sur les actions à mener.  \nLa Questiologie  \nUne autre technique pour poser des questions puissantes est la méthode de la Questiologie© créée par Frédéric Falisse qui, après avoir modélisé 3 800 questions intéressantes, en a extrait une logique et des critères communs. Cette méthodologie propose de développer une stratégie de questionnement en vous adaptant, dans un premier temps, au profil de votre interlocuteur (les « quatre locus ») :  \n– « observateur » : celui qui va observer, analyser ;  \n– « méta » : celui qui va réfléchir, modéliser, théoriser ;  \n– « introspectif » : celui qui va ressentir des émotions ;  \n– « acteur » : celui qui va agir.  \nGénéralement, nous avons chacun un ou deux locus de prédilection.  \nEn vous adaptant au profil de votre interlocuteur, vous développez une meilleure connexion avec lui, vous débloquez des situations et vous lui permettez de trouver des idées novatrices. Vous développez ainsi votre agilité relationnelle.  \nSi, par exemple, un collaborateur vient vous voir, très en colère, en vous disant qu’il ne supporte plus la relation avec son manager, l’objectif, dans un premier temps, sera de diminuer sa charge émotionnelle en lui faisant prendre de la distance par rapport à son problème et de lui permettre de mieux réfléchir. Puisqu’il est dans un locus « introspectif » (il ressent une émotion), vous allez l’emmener vers un locus « observateur » (analyser la situation factuellement) : « Pour vous, c’est quoi une bonne collaboration ? » Puis, pour l’aider à prendre encore plus de hauteur et à développer sa réflexion, vous allez lui poser une question du locus « méta » : « À quoi pensez-vous quand vous imaginez cette collaboration ? » L’interlocuteur, qui a pu ainsi prendre de la distance par rapport à son émotion, peut maintenant envisager des actions concrètes et réfléchies. Vous pouvez alors décider de lui poser une question « d’acteur » qui va l’engager : « Que pouvez-vous faire pour obtenir une bonne collaboration ? » Et, enfin, pour ancrer sa décision et évaluer sa charge émotionnelle, vous pouvez finir par une question de type « introspectif » : « Comment vous sentez-vous avec ça ? » Avec cette stratégie, vous incitez votre interlocuteur à modifier ses schémas de pensée habituels et à trouver de nouvelles pistes de réflexion."
        }
      },
      {
        "json": {
          "pageId": "PAGE023_1757157753082",
          "text": "Vous pouvez aussi adapter votre stratégie de questionnement en fonction des cinq étapes de raisonnement de votre interlocuteur. Comme ce ne sont pas les mêmes connexions neuronales qui sont sollicitées, cela permet de développer différentes capacités réflexives. En posant des questions sur chacune de ces étapes de raisonnement, appelées « gestes mentaux », vous allez « bousculer » son cerveau pour qu’une nouvelle connexion se mette en place et génère une nouvelle réponse :  \n– description des choses ;  \n– perception, appropriation des choses ;  \n– évaluation des options ;  \n– sélection des options, choix ;  \n– intégration des solutions dans une perspective plus large.  \nEn questionnant sur le même sujet mais de manière différente, en adaptant vos questions au profil de l’interlocuteur, vous multipliez votre propension à poser des questions puissantes et vous démultipliez donc votre capacité à obtenir des réponses différentes, plus réfléchies, plus créatives, plus impliquantes.  \nPour résumer  \nSavoir poser des questions puissantes va donc vous permettre de :  \n– créer du lien ;  \n– d’enrichir la perception du monde de chacun ;  \n– de clarifier un objectif ;  \n– de motiver, d’engager ;  \n– de donner du sens ;  \n– de mettre en mouvement ;  \n– de stimuler une réflexion profonde ;  \n– d’explorer les options ;  \n– de développer l’intelligence collective.  \nIl s’agit donc d’un véritable enjeu de communication, de leadership et de développement humain à la fois individuel et collectif.  \nL’art de poser des questions, et donc d’obtenir des réponses plus impactantes, est ce qui fait la différence dans un monde de plus en plus incertain où l’agilité, l’engagement, la créativité, l’empathie et la coopération sont des compétences comportementales (soft skills) très recherchées.\n\nIl est également important de privilégier les dynamiques de réflexion qui s’adaptent rapidement. Pour poser la bonne question, il s’agit de :\n\n– rester humble. La première étape peut être de se mettre en situation de ne pas savoir. Cela va éviter de s’enferrer dans un biais dès le départ et forcer l’ouverture à des suggestions extérieures ;\n\n– poser les cinq pourquoi. Cette pratique empruntée à l’industrie automobile part d’un problème observé pour remonter vers la cause initiale (root cause) par une succession de « pourquoi » ;\n\n– aller Gemba (qui signifie « le lieu où les choses se passent » en japonais) ! Ce concept lui aussi emprunté à l’industrie automobile consiste à se rapprocher au plus près du terrain et, dans notre contexte, à affiner la question auprès de ceux qui font et qui savent ;\n\n– s’appuyer sur la diversité. S’entourer d’une grande diversité de profils minimise les biais et enrichit la réflexion avec le maximum d’idées ;\n\n– viser le cygne noir. Confronter la pertinence de la question à l’interrogation « Qu’est-ce qui pourrait nous tuer (figurativement) demain ? » permet de se concentrer a priori sur ces risques fatals. Si nous constatons que la problématique que nous cherchons à résoudre est complètement déconnectée des risques qui pourraient précipiter la faillite, est-ce bien cette problématique que nous devons résoudre en premier ?\n\n– cartographier notre savoir. Le champ des possibles est contenu dans cette matrice que Donald Rumsfeld, secrétaire d’État sous George Bush Jr, avait rendue célèbre.\n\nFigure 1. Matrice de Rumsfeld\n\nCe que nous savons que nous connaissons est notre zone de confort et fait partie de notre routine. Nous devons éviter de nous y complaire. Nous nous contenterons d’en contrôler régulièrement la validité par des reportings automatisés pour mieux nous consacrer à ce que nous savons que nous ne connaissons pas. Ce sont des questions qui nous attendent et sur lesquelles nous n’avons pas encore su ou eu le temps de nous pencher. Enfin, il y a ce que nous ne savons pas, que nous en soyons conscients ou non. Nous pouvons le découvrir en faisant appel à des processus analytiques empiriques, mais surtout par des échanges, des collaborations avec nos confrères, nos mentors, par nos lectures en dehors de nos domaines de pré­dilections. Ce temps, gagné grâce à l’efficacité de nos processus analytiques, est notre meilleur moteur de créativité dans tous les domaines, car il nous permettra de nous poser et de créer ces moments de réflexion pour aller découvrir ce que nous ne savons pas que nous ne savons pas."
        }
      },
      {
        "json": {
          "pageId": "PAGE024_1757157753082",
          "text": "# Mettre la question au service de l’action\n\nJustifier un investissement en temps, voire technologique, dans le domaine décisionnel est toujours délicat. Une solution, une question ou un challenge opérationnel peut être valorisé en fonction d’un gain ou d’une économie souvent quantifiable. S’il n’a pas de valeur, le projet n’a pas de raison d’être. Si un projet analytique résout un problème concret, une valeur peut être estimée et il sera alors parfaitement possible de faire le calcul du retour sur investissement à l’envers et d’en déduire l’investissement maximal. S’il résout un problème à 1 000 000 d’euros, un projet à 200 000 euros sera plus facilement acceptable. Si le processus manuel ou sur tableur peut être réduit de cinq heures par semaine, alors un projet d’une quarantaine d’heures pour l’optimiser peut sembler acceptable. Ces logiques ne sont pas encore souvent mises en œuvre et l’histoire est jonchée d’échecs de projets data cuisants et coûteux, car trop centrés sur la technologie ou la data."
        }
      },
      {
        "json": {
          "pageId": "PAGE025_1757157753082",
          "text": "# Pour conclure\n\nNous voilà équipés pour commencer nos projets analytiques sur le bon pied. Parce que cet exercice de questionnement est délicat et qu’il ne peut pas être résolu à coups d’achat de logiciels ou d’accumulation de données, nous pouvons être tentés de le négliger. De plus, le temps passé à réfléchir ne se matérialise pas aux yeux d’un management impatient, au contraire d’un ballet de feuilles de tableur et de graphiques qui, même sans pertinence, font toujours leur effet lors des réunions d’équipe.\n\nCette première étape conditionne pourtant les résultats futurs parce qu’il ne sert à rien de trouver la bonne réponse à la mauvaise question.\n\n------------------------------------------------------------------------\n\n2. Anguenot F., La questiologie ou l’art de se poser des questions, [www.lettreducadre.fr](http://www.lettreducadre.fr), 21 janvier 2015.\n\n3. Voir aussi : Meyer O, Daniel Kahneman et les biais cognitifs, RSE Magazine, 21/02/2020.\n\n4. Un quotidien français, Libération, a alors publié un papier titré « Le baratineur qui valait 47 milliards » le 16 novembre 2019.\n\nCHAPITRE 3  \nComprendre la donnée dans l’entreprise\n\nL’annonce de la perte de toutes les données de l’entreprise.\n\nQuelle est donc cette fameuse donnée ? On nous parle de Big Data, de données clients, de données privées, mais savons-nous bien visualiser ce qu’elle est vraiment ? Ce chapitre va clarifier le concept, présenter sa structure quasi immuable, les différentes méthodes de base pour la traiter, et enfin comment respecter les normes et l’éthique autour de la data."
        }
      },
      {
        "json": {
          "pageId": "PAGE026_1757157753082",
          "text": "# Qu’est-ce que la donnée pour l’analytique en entreprise ?\n\nLa réponse à cette question a bien évolué depuis un quart de siècle. Les noms ou expressions qui vont suivre ont tous été utilisés pour définir la data, sans jamais vraiment le faire correctement. L’évolution des technologies et les biais des mentalités dans notre domaine ont régulièrement empêché de considérer la data pour ce qu’elle est.\n\nDans les années 1980, la data en analytique était souvent considérée comme l’information contenue dans les logiciels. De nature financière ou opérationnelle, son utilisation était principalement limitée au monde de l’entreprise ou de la science. Elle restait principalement un domaine de techniciens. Les mondes financiers, marketing, commerciaux, voire même parfois opérationnels, regardaient avec dédain ceux qui y mettaient les mains. Les premières solutions d’aide à la décision par l’analytique avaient une adoption limitée.\n\nLe début des années 2000 a mis la data sur le devant de la scène en lui affublant un qualificatif aussi imposant qu’effrayant : le Big Data. De grandes entreprises telles que Google, Yahoo et autres acteurs de l’Internet avaient commencé à collecter et à analyser toutes les données du monde, quelles qu’elles soient, pour ensuite en bâtir des propositions de valeur pour le grand public. En l’espace de quelques années, la data prit un caractère plus universel. Elle pouvait être beaucoup de choses, et le terme de « données non struc­turées » permettait de savamment faire référence aussi bien aux photos, aux voix et aux textes.\n\nLa mode du Big Data s’est ensuite estompée. Aujourd’hui, l’intelligence artificielle et sa cohorte de termes comme Machine Learning, Deep Learning (voir chapitre 5, Les domaines du Big Data) sont venus supplanter la notion de Data dans les esprits. La data est moins ce qu’elle est, mais plus ce que l’on en fait.\n\nTout au long de cette évolution, la plupart du temps, seuls les techniciens avaient une compréhension pragmatique de ce qu’est la data. Les métiers se contentaient souvent de mentionner ces termes plus sexy, sans jamais vraiment apprendre leur nature. Dans les paragraphes qui vont suivre, nous allons combler ces manques et démontrer que non seulement cette compréhension est accessible à tous, mais qu’elle est aussi nécessaire pour l’analytique."
        }
      },
      {
        "json": {
          "pageId": "PAGE027_1757157753082",
          "text": "# Comment appréhender la data"
        }
      },
      {
        "json": {
          "pageId": "PAGE028_1757157753082",
          "text": "## La data, c’est « tout »\n\nJ’aime à me remémorer cette magnifique chanson des Pink Floyd, extrait de l’album Dark Side of the Moon, dont les paroles donnent une définition de la data très juste (sans le faire exprès). Il y a une infinité de datas, où que nous soyons.\n\nLa data doit être considérée comme pouvant être tout ce qu’il y a de visible et d’invisible dans notre univers. En évitant toute limitation a priori, nous évitons la réduction du champ des possibles dans la sélection de nos éléments de réponse. Nous évitons ainsi de ne retenir que les réponses pour lesquelles nous avons de la donnée. À la question « combien y a-t-il de data dans une pièce, dans une entreprise, dans un pays ? », la réponse doit être par principe « une infinité ».\n\n→ Les données qui nous entourent\n\nPar exemple dans un bureau nous trouverons :\n\n– le nombre de prises électriques ;\n\n– la température de l’air ;\n\n– le volume de papier ;\n\n– la couleur des livres sur le bureau ;\n\n– le compte des caractères du brouillon du dernier e-mail ;\n\n– l’âge moyen des collaborateurs passés nous voir dans la journée, etc.\n\nLa liste est sans fin.\n\n» Quelle est votre liste de données ? Prenez le temps de considérer les données les plus simples ou les plus farfelues autour de vous."
        }
      },
      {
        "json": {
          "pageId": "PAGE029_1757157753082",
          "text": "## Identifier quelle data est importante\n\nLa clé est dans le point de départ en analytique : la question.\n\nOui, il y aura toujours une infinité de données autour de nous, mais ce champ se réduira fortement dès que notre question sera posée. Ensuite, le temps et le retour sur investissement réduiront encore un peu plus le champ des possibles, car certaines données seront trop coûteuses à extraire ou à raffiner. Cependant, grâce à notre connaissance et à notre pratique du sujet, nous ne nous autolimiterons pas à notre petite zone de confort et nous pourrons aller capturer des données au-delà des limites précédentes.\n\nReprenons l’exemple des données dans un bureau, choisissons-en quelques-unes et regardons a posteriori quelle question pourrait les rendre pertinentes et si leur collecte était économiquement valide. La position retenue sera bien sûr très personnelle et pas forcément pertinente pour tous.\n\nTableau 2. Comment choisir la collecte des données dans un bureau ?\n\nPour chacun de ces exemples, nous avons pu rapidement nous concentrer sur une partie palpable de l’univers de data autour de nous, grâce à la question posée.\n\n→ Comment expliquer l’échouage de cétacés ?\n\nIl y a quelques années, les côtes océaniques européennes avaient connu une recrudescence d’échouage de cétacés qui mouraient par dizaines sans raison apparente. Un journaliste demanda à un scientifique s’il allait faire du Big Data et de l’IA pour tenter d’expliquer ces événements. La réponse fut un exemple de pragmatisme : « je connais mon métier, on connaît les grands facteurs qui désorientent les cétacés, on va déjà mesurer ces éléments. Pas la peine de faire bouillir l’océan. »\n\nAucune donnée ne saurait être écartée de nos champs d’analyse si elle est pertinente. Le problème est que toutes les datas ne sont pas facilement et économiquement accessibles, voire pas du tout, du fait de leur nature (ce que nous avons en tête par exemple) ou de leur complexité (les interactions entre les masses d’air, les océans dans les études climatiques, par exemple). Pour être valorisée, une donnée devra être captée dans un format utilisable."
        }
      },
      {
        "json": {
          "pageId": "PAGE030_1757157753082",
          "text": "## Collecter une donnée pour une utilisation efficace, précise et durable\n\nLa diversité des formats est quasi infinie, par exemple du plus classique au moins conventionnel :\n\n– le rapport papier qui peut contenir textes et images ;\n\n– le format électronique avec ses différents types de fichiers (texte, images, vidéo, etc.) ;\n\n– le support spécifique : le film, la voix, le son, le mouvement, la pierre, l’argile et tout ce qui peut être imprimé ;\n\n– la donnée cérébrale : nos pensées (que l’on pourrait ramener à des impulsions électriques ?).\n\nEn 2015, les principes FAIR ont établi les caractéristiques requises pour qu’une donnée puisse être traitée, échangée ou analysée avec un minimum de friction. L’acronyme FAIR représente quatre termes :\n\n– Findable (trouvable) : les données doivent pouvoir être identifiées sans ambiguïté, correctement décrites, simples à chercher ;\n\n– Accessible : les données peuvent être extraites avec des protocoles standardisés, (idéalement) universels, qui permettent si nécessaire l’application de protocoles d’identification ;\n\n– Interopérable : les données utilisent un langage formel, accessible, partagé et largement applicable pour la représentation des connaissances ;\n\n– Réutilisable : les données sont publiées avec une licence d’utilisation des données claire et accessible. Les données sont associées à une provenance détaillée et répondent à des normes communautaires pertinentes pour le domaine.\n\nCette caractéristique FAIR commence par une première nécessité : la donnée doit être stockée dans une forme de base de données, soit-elle basique.\n\nPour pouvoir être traitée et analysée, la donnée doit présenter un format que l’individu ou la machine peut comprendre. Pour être transportée, échangée, mise à jour, modifiée, elle doit également rester fluide. Dans une vision pragmatique, applicable à nos environnements standards, le seul conteneur qui assure cette capacité d’analyse et cette fluidité est celui de la base de données."
        }
      },
      {
        "json": {
          "pageId": "PAGE031_1757157753082",
          "text": "## Qu’est-ce qu’une base de données ?\n\nUne base de données n’est ni plus ni moins qu’un endroit où nous pouvons entreposer de la donnée. Cette solution assure, selon sa forme, des services de contrôle d’accès, de sauvegarde, de formatage, d’organisation, d’indexation, voire de prétraitement.\n\nLa base de données : une histoire antique\n\nLe concept de base de données est assez naturel. Depuis l’antiquité, de grandes bibliothèques (comme celles d’Alexandrie ou d’Assurbanipal à Ninive) centralisent, sécurisent et donnent accès à des savoirs, à des témoignages, à des relevés. Ces bibliothèques maintenaient en permanence le classement, la mise à jour et l’ajout de nouveaux ouvrages, comme les bases de données actuelles.\n\nLe concept naturel de dépôt central de données a toutefois dû être adapté aux contraintes de volume, de partage, de rapidité et de fluidité exigées par l’entreprise. Nous avons assisté, depuis trois décennies, à la numérisation progressive de ces « bibliothèques » qui sont devenues tout ou partie digitales. Cette transformation a permis de libérer l’information et de la rendre progressivement accessible à tous, partout et en masse.\n\nAujourd’hui, la digitalisation d’une grande partie de notre vie a accéléré cette mise en base de données et, par la même occasion, a permis d’analyser encore plus de données, plus facilement. Nos transactions, nos messages, nos photos, nos réactions (sur les réseaux sociaux), notre voix (sur les assistants vocaux), notre vie sociale (sur nos smartphones) sont nativement dans un format numérique et par conséquent beaucoup plus propices à l’analyse.\n\nCette numérisation de la donnée et son stockage en base de données sont devenus la norme en analytique. Et pourtant, de nombreuses organisations continuent à stocker de la donnée sur du papier, dans des tableurs ou autres documents électroniques bureautiques où les caractéristiques FAIR disparaissent.\n\nPapier contre silicone\n\nPourquoi certaines données sont-elles toujours conservées dans la mémoire commune ou sur des médias personnels tels que des calepins ou des cahiers ? Certes, le support physique confère à la donnée une indépendance à la technologie et à l’électronique, mais il introduit une friction quand il s’agit de partager, de consolider et d’analyser cette information autrement que manuellement.\n\nMis à part certaines technologies avancées, la représentation logique des bases de données que nous allons rencontrer le plus souvent est une collection de tables de données. Ces tables, tout à fait similaires à une liste de courses, à un tableau de points au tarot ou à un menu de restaurant, sont composées de lignes et de colonnes. Bien souvent, les noms dans la première ligne apportent des indi­cations sur le type ou la nature des données de chaque colonne. Chaque ligne est un enregistrement dont les caractéristiques et les valeurs apparaîtront dans les colonnes.\n\n» Combien trouvez-vous de ces structures tabulaires autour de vous ? Vous constatez à quel point cette structure est naturelle pour collecter de l’information.\n\nFigure 2. Structure d’une base de données\n\nL’approche semble évidente et, pourtant, bon nombre de nos données sont conservées dans des tableaux complexes, alambiqués, délicats à retravailler. Pour des finalités de communication, il est normal d’ajouter des totaux, de la couleur et des graphiques, mais, pour les processus intermédiaires d’échange et de transformation de la donnée, ces formats « cosmétisés » sont hautement inefficaces (voir chapitre 7, Chasser les processus Creeper).\n\nIl ne faut pas confondre les objectifs de stockage/préparation et d’échange de données brutes avec les besoins d’analyse Ces deux étapes se suivent, mais ne peuvent être mélangées sous peine de faire perdre à la data sa fluidité et son aspect FAIR, et à l’analyse son adaptabilité et son agilité."
        }
      },
      {
        "json": {
          "pageId": "PAGE032_1757157753082",
          "text": "## Les fichiers plats : le plus simple des formats FAIR\n\nSi la logique tabulaire est assez simple, une question vient rapidement à l’esprit. Comment utiliser des tables si :\n\n– nous ne sommes pas équipés de bases de données à proprement parler ;\n\n– nous n’avons de budget pour investir dans ces technologies ;\n\n– les services informatiques n’ont pas de temps à nous consacrer.\n\nCe qui pourrait constituer un obstacle incontournable ou une excuse pour ne pas avancer a en fait une solution universelle, accessible à tous sans aucun investissement. Nous avons tous accès à des tables de données capables de stocker des millions de lignes, d’être analysées avec performance, d’être partagées sans difficulté et de nourrir des algorithmes de dernière génération. Nous pouvons tous les mettre en œuvre indépendamment du système d’exploitation de nos ordinateurs, et ce, sans aucun investissement. Ce médium universel, ce couteau suisse de la data n’est autre que le fichier plat, aussi connu sous le nom de fichier texte, ou fichier CSV.\n\nSa mécanique est la plus simple qu’il soit pour définir une table :\n\n– les enregistrements sont représentés par des lignes avec un retour à la ligne à la fin de chacune (retour chariot/entrée) ;\n\n– les colonnes sont indiquées par un délimiteur que toute solution data et analytique saura interpréter comme un changement de colonne. Ce délimiteur est souvent une virgule (voir illustration 1) pour les fichiers CSV (Comma Separated Value), une tabulation pour les fichiers TSV (Tab Separated Value) ou tout autre séparateur qui puisse être identifié sans ambiguïté.\n\nDans ce dernier cas, on parle par extension de CSV, même si le délimiteur est un point-virgule, un pipe (\\|), voire parfois un espace.\n\nIllustration 1. Exemple de fichier plat CSV : extrait du manifeste d’un bateau au destin funeste\n\nUn CSV ou un fichier texte ne ressemble à rien quand il est lu de manière brute. Notre première impression est souvent celle du rejet : « C’est moche, ça à l’air complexe et mon patron ne comprendra jamais. »\n\nMais, si l’on garde en tête les exigences de fluidité de la donnée et la séparation des processus de préparation et de stockage avec celle de l’analyse, on comprend que ces tables sont la partie immergée de l’iceberg sur laquelle reposera la partie émergée de l’analyse et du reporting. Et comme la partie immergée d’un bloc de glace, elle représente la majeure partie du travail et de la valeur du processus analytique. La capture de la donnée, sa préparation, sa modélisation et sa mise à disposition représentent 80 % du travail.\n\nIllustration 2. Exemple de fichier issu de l’interprétation par une solution analytique du fichier CSV de l’illustration 1\n\nConserver les données en tables et a minima en fichiers plats, c’est garantir plusieurs choses.\n\n\\> Un média quasiment sans limite en termes de volume\n\nLe fichier CSV n’a pas de limitation autre que l’espace disque disponible sur notre lecteur ou une limitation de la taille du fichier de notre système d’exploitation. Les formats tableurs présentent rapidement des contraintes de volume et de performance.\n\nTableau 3. Contraintes de volume des tableurs\n\n\\> Un format plus performant au chargement et qui se compresse mieux\n\n\\* La solution 1 est locale. \\*\\* La solution 2 est sur le Cloud.\n\nNote : exemple pour des fichiers de 950 671 lignes × 18 colonnes, soit 17 112 078 cellules (2 150 604 vides, 14 961 474 remplies) traité en format fichier plat et XLSX.\n\nTableau 4. Mesures de volumétrie et de performance pour des fichiers Excel et texte sur un ordinateur portable de travail\n\n\\> Un format d’échange universel\n\nLa majorité des solutions savent aujourd’hui importer et exporter des fichiers plats sans effort. On ne retrouve pas de problème de version de logiciel. Le seul point d’attention est le séparateur qui doit être correctement indiqué dans les phases d’intégration. En effet, si des champs contiennent le séparateur, alors la machine ne pourra pas appliquer sa logique pour retrouver les colonnes.\n\nUn piège classique est celui des CSV dans des contextes internationaux où les nombres peuvent s’écrire avec des virgules ou des points comme marqueur de décimales et dont créer des problèmes suivant l’origine de l’export. Il est préférable dans ce cas d’utiliser des tabulations, voire des pipes (\\|).\n\n\\> Un format sur lequel on peut travailler directement\n\nCes fichiers plats peuvent être immédiatement utilisés pour des analyses, des visualisations avancées, des algorithmes de Machine Learning."
        }
      },
      {
        "json": {
          "pageId": "PAGE033_1757157753082",
          "text": "# Les trois règles cardinales en analytique\n\nTout ce qui précède nous amène à poser une première fondation dans notre pratique de la donnée :\n\n1\\. récupérons, échangeons et stockons la donnée sous forme de tables ;\n\n2\\. pour des raisons techniques, pratiques ou budgétaires, nous pouvons nous contenter d’un fichier plat ;\n\n3\\. procédons toujours en trois étapes : capturer, préparer, puis analyser la donnée.\n\nLes tableurs sont de formidables outils d’analyse de données. Utilisés selon les règles que nous venons d’énoncer, ils peuvent offrir un niveau acceptable de fluidité, de partage ou d’agilité. Ils ouvrent néanmoins la porte à la tentation de mélanger préparation et analyse, parce que c’est facile et pratique. Ils vont être limités en performance sur des gros volumes de data. Ils vont être moins faciles à partager, car ils nécessiteront toujours une phase de compréhension de leur mécanisme. Enfin, s’ils contiennent des fonctions avancées de transformation, de calcul et de visualisation, ils n’atteindront pas la performance d’une combinaison : base de données + solution d’analytique + bibliothèque de Machine Learning. Les tableurs sont d’excellents outils de prototypage, même si, avec l’expérience, on devient beaucoup plus agile avec des outils analytiques dédiés. Ils apportent également des possibilités d’automatisation de processus intéressantes avec les macros (bien qu’elles aboutissent souvent à des usines à gaz), même si les approches en Python ou en développement de procédures stockées au niveau de la base de données (voir chapitre 5, Optimiser le choix du moteur de calcul) sont beaucoup plus rapides.\n\nCHAPITRE 4  \nMettre les systèmes au service du data pionnier\n\nOn a encore « bidouillé » dans le transactionnel.\n\nAfin de mettre les technologies à notre service, nous allons dans ce chapitre :\n\n– comprendre pourquoi et quand la technologie est pertinente dans un projet analytique ;\n\n– reconnaître et qualifier les deux grandes familles de logiciels ;\n\n– détailler les solutions dédiées à l’analytique ;\n\n– lister les pratiques d’implémentation nécessaires pour des mises en œuvre réussies."
        }
      },
      {
        "json": {
          "pageId": "PAGE034_1757157753082",
          "text": "# Quelle est l’utilité de la technologie en data ?\n\nAvant de nous précipiter sur la technologie comme remède à tous les problèmes data, répondons à une première question : peut-on faire de l’analytique sans système ? Soyons encore plus extrêmes : peut-on faire de la data sans électricité ? Qu’en pensez-vous ?\n\nLa réponse est sans appel : on peut collecter, stocker, analyser et reporter de la donnée sans aucune technologie particulière. L’humanité fait cela depuis des millénaires soit par la voix et la mémoire, soit par l’écrit sur les supports les plus variés : pierre, tablette d’argile, papyrus, bambou, parchemin ou encore papier. Il est donc possible de gérer une chaîne analytique sans aucun outil informatique.\n\nLa data antique\n\nIl y a plus de trois mille ans, les souverains administraient leur empire avec des relevés réguliers de population, de réserve de métaux ou de grain… Un recensement civil avait pour objectif spécifique d’établir quel rôle les hommes de chaque localité joueraient en matière de mobilisation. Le premier recensement connu a été effectué en 1776 (avant notre ère), sous le règne de Yasmah-Addu, et un autre en 1770, sous le règne de Zimri-Lim. À ces deux occasions, ils se sont produits à la fin d’une période de guerre… Il était évidemment nécessaire de contrôler les effectifs disponibles pour recruter des troupes pour lutter contre Élam5.\n\nSi nous pouvons faire des analyses sans systèmes, pourquoi ces derniers semblent-ils aujourd’hui s’imposer dans tous les projets ? Pourquoi avons-nous besoin de technologies dans nos processus data ? La réponse nous donne la raison d’être des solutions que nous mettons en place. Elle tient en cinq points :\n\n– aller plus vite ;\n\n– voir plus loin et plus en détail ;\n\n– traiter plus d’informations ;\n\n– collaborer avec agilité ;\n\n– bâtir des processus durables.\n\nCela paraît évident, et pourtant ces préceptes ne sont pas toujours partagés et appliqués en entreprise où l’implémentation de technologie est parfois une raison en elle-même.\n\n→ Quand le manuel fait mieux que le 2.0\n\nDans les milieux de la finance, les modes, le qu’en-dira-t-on et l’influence des confrères pèsent lourd dans la balance des investissements logiciels. La mise en place de solutions digitales peut devenir non seulement d’une utilité relative, mais également chronophage par rapport au processus qu’elles sont censées supporter. Voici un exemple vécu à la fin des années 1990.\n\nLa solution en question visait à faire passer dans le 2.0 la confirmation de certaines transactions. Manuelle à l’époque, elle consistait en une série de tâches simples :\n\n– extraire les fichiers de confirmation ;\n\n– imprimer le lot ;\n\n– les séparer par institution financière ;\n\n– les faxer (il y a deux décennies, cette technologie était la seule reconnue comme valeur contractuelle) en chargeant liasse par liasse les documents et en laissant ensuite la machine les envoyer.\n\nUne collaboratrice se chargeait chaque matin de cette tâche. Malgré son obsession pour l’efficacité et la rapidité, le responsable avait choisi de ne pas remettre en cause ce processus qui présentait beaucoup d’avantages :\n\n– les tâches manuelles ne prenaient pas plus de cinq à sept minutes ;\n\n– elles comportaient peu de risques d’erreur, si ce n’est aucun ;\n\n– le fax pouvait prendre un quart d’heure, mais cette tâche n’avait pas besoin de supervision humaine, sauf pour une comparaison pages chargées/pages envoyées à la fin de l’envoi ;\n\n– la collaboratrice profitait de la durée d’envoi des documents pour se préparer un café et fumer une cigarette, ce qui constituait une pause quotidienne en milieu de matinée ;\n\n– et surtout, elle venait rejoindre son responsable avec une tasse du café qu’elle avait fait couler, l’esprit détendu par sa pause, pour faire la revue des opérations de la veille et des points d’attention sur les reportings quotidiens des filiales. Ce moment était précieux, car il présentait un temps de réflexion apaisé et calme au milieu de journées chargées. C’était la possibilité de traiter de vrais problèmes, de réfléchir à des points stratégiques.\n\nLorsque le processus passa en 2.0 par le truchement de la confir­mation électronique, la collaboratrice perdit du jour au lendemain sa pause-café-cigarette et le responsable un moment précieux d’échange. Il n’avait pas été consulté sur le projet, une migration contre-productive qui faisait s’évaporer beaucoup de valeur. Le processus était devenu le suivant :\n\n– extraire un à un les fichiers par banque ;\n\n– les stocker sur une clé USB ;\n\n– les recopier sur un poste de communication dédié ;\n\n– imprimer ces fichiers pour revue et validation interne ;\n\n– les assigner à chacune des banques ;\n\n– lancer le transfert des fichiers ;\n\n– relancer les transferts non passés la première fois.\n\nPour la collaboratrice, le processus prenait maintenant une bonne trentaine de minutes et elle devait enchaîner des manipulations informatiques non triviales. Elle ne pouvait prendre ces minutes de pause, de préparation de son café et encore moins profiter d’une cigarette. Elle n’avait de surcroît plus qu’une dizaine de minutes à consacrer aux sessions de travail avec son manager.\n\nCerise sur le gâteau, cette avancée vers le 2.0 avait coûté l’équivalent de 200 000 euros pour perdre 10 minutes par jour et faire disparaître du calendrier une séance de travail à forte valeur ajoutée.\n\nLa technologie n’est pas l’arme absolue du digital. Dans l’exemple ci-dessus, les choix avaient été dictés par la mode du 2.0, la pression d’un éditeur et la volonté d’épater les confrères, au détriment de toute efficacité… et de l’humain. Le projet avait été dirigé par des professionnels sans réelles connaissances techniques. Il a été possible de modifier le processus, d’adapter les solutions tout au long de la chaîne et finalement de livrer un niveau de performance et d’automatisation optimal… qui faisait gagner quatre minutes sur le traitement initial. Cet exemple illustre à quel point il est facile d’oublier la raison pour laquelle nous utilisons des outils. Le monde de l’analytique est très prompt à mettre la charrue avant les bœufs.\n\nNous allons voir maintenant les familles de systèmes que nous retrouvons toujours dans nos organisations pour ensuite nous concentrer sur les technologies qui supportent l’analytique.\n\nL’objectif est de comprendre le rôle et le positionnement de chacune des familles de solutions dans la chaîne analytique. C’est ainsi que les personnels du métier peuvent devenir des interlocuteurs éclairés des services techniques et les équipes informatiques, des éducateurs pédagogues vis-à-vis de leurs collègues du terrain, voire des architectes de solutions à leur niveau.\n\n« Chaque science, chaque étude, a son jargon inintelligible, qui semble n’être inventé que pour en défendre les approches. » Voltaire\n\nNous pouvons en général regrouper l’ensemble des systèmes de l’entreprise en deux catégories : les systèmes transactionnels et les systèmes décisionnels, chacun avec sa raison d’être. Leur nature et leurs fonctions diffèrent et ne pas savoir les distinguer ou les utiliser l’un pour l’autre peut conduire à des erreurs coûteuses."
        }
      },
      {
        "json": {
          "pageId": "PAGE035_1757157753082",
          "text": "# Les solutions transactionnelles\n\nLes solutions transactionnelles sont celles qui enregistrent les… transactions. C’est simple, non ?\n\nComment peut-on les définir un peu mieux ? Elles ont les objectifs suivants :\n\n– automatiser les opérations commerciales de base telles que les prises de commandes, les rentrées et les sorties en stock, les livraisons, la facturation, les règlements et les encaissements, etc. Elles remplacent les multiples saisies manuelles et les chaînes administratives papier tout au long du cycle ;\n\n– monitorer les étapes du cycle d’exploitation de la commande fournisseur au règlement client en passant par la mise en stock ou la production :\n\n• pour aider les employés à faire leur travail plus efficacement en supprimant les barrières entre les unités commerciales ;\n\n• pour donner une vue en temps réel du cycle afin de permettre aux entreprises de répondre aux problèmes de manière pro­active et d’optimiser en permanence leurs opérations ;\n\n– améliorer la conformité avec les normes réglementaires en encadrant, voire en forçant, la capture d’éléments obligatoires et en assurant une gestion relativement stricte de leur administration.\n\nLes systèmes transactionnels font d’ailleurs souvent l’objet d’audits d’intérim par les équipes qui veillent à ce que les transactions réelles y soient bien enregistrées, puis correctement portées d’une étape à une autre. Avec leur extension à tous les domaines de l’entreprise, les systèmes transactionnels gèrent souvent la majorité des flux de l’activité. Certaines de ces solutions, souvent appelées ERP (Enterprise Ressource Planning), se retrouvent par exemple dans les services :\n\n– achats ;\n\n– marketing en ligne ;\n\n– support clients ;\n\n– ressources humaines ;\n\n– ventes.\n\nLes données qu’ils collectent sont fondamentales pour l’établissement des comptes financiers et des déclarations fiscales.\n\n» Sauriez-vous identifier les systèmes transactionnels ou ERP de votre entreprise ou de votre département ? Sauriez-vous repérer celui qui se cache dans ces photos (attention, il y a un piège sur la troisième) ?\n\nLa caisse enregistreuse, ici dans sa forme la plus moderne, est le système transactionnel qui capture les transactions d’achats et leurs encaissements.\n\nLe petit boîtier à la ceinture de ce livreur scanne les colis livrés et enregistre l’acte de dépôt chez le récipiendaire.\n\nIci, pas de technologie digitale, mais le tableau de score enregistre les coups et la performance de la golfeuse. Ce dernier collecte les « transactions » du parcours. On peut imaginer que ces données seront consolidées avec d’au­tres pour suivre à la fois la progression de la golfeuse, mais également les performances globales des membres du club. Considérer que cette étape est très manuelle et relativement peu efficace pourrait amener à adopter une solution de capture plus intégrée pour gagner en temps, en précision, voire en richesse de données captées (température, heure exacte, biométrie, etc.), à moins que le retour sur investissement ne soit trop élevé ou que les challenges métier soient suffisamment couverts par cette approche « papier »."
        }
      },
      {
        "json": {
          "pageId": "PAGE036_1757157753082",
          "text": "## Une brève histoire de l’ERP\n\nLe terme ERP a été inventé en 1990 par la firme d’analystes Gartner, mais ses origines remontent aux années 1960. Le concept s’appliquait alors à la gestion et au contrôle des stocks dans le secteur manufacturier. Les ingénieurs logiciels ont créé des programmes pour surveiller les stocks, rapprocher les soldes, etc. Dans les années 1970, cela avait évolué vers des systèmes de planification des besoins en matériaux (MRP) pour les processus de production.\n\nDans les années 1980, le MRP s’est développé pour englober davantage de processus de fabrication, ce qui a incité beaucoup de gens à l’appeler MRP-II ou Manufacturing Resource Planning. En 1990, ces systèmes se sont étendus, au-delà du contrôle des stocks et d’autres processus opérationnels, à d’autres fonctions de back-office, comme la comptabilité et les ressources humaines, ouvrant la voie à l’ERP.\n\nAujourd’hui, l’ERP gère également les fonctions de front office telles que l’automatisation des forces de vente (SFA), l’automatisation du marketing et le commerce électronique. Devant ces avancées de produits et leurs réussites, un large éventail d’entreprises, de la distribution en gros au commerce électronique, utilisent désormais des solutions ERP6."
        }
      },
      {
        "json": {
          "pageId": "PAGE037_1757157753082",
          "text": "## Les forces des ERP sont leurs faiblesses pour l’analytique\n\nLorsque l’on comprend la finalité des systèmes transactionnels, on saisit mieux certaines de leurs caractéristiques intrinsèques :\n\n– ils sont limités à un groupe de tâches défini. Chaque module est développé avec les spécificités du processus qu’il couvre ;\n\n– ils capturent la donnée au plus haut niveau de granularité. Chaque transaction, chaque virement, chaque pièce produite, chaque envoi, chaque détail de facture, etc. nécessaire à l’exécution de la transaction est enregistré, sans exception ;\n\n– l’information est brute dans le détail le plus fin ;\n\n– leur agilité n’est pas innée. Les processus transactionnels s’appuient tous sur une base standard et commune à tous les acteurs économiques, au moins au niveau de chaque industrie. Ils doivent bien sûr s’adapter aux spécificités de chaque entreprise, mais, en dehors de ces customisations, une facture reste une facture, un bordereau de livraison reste un bordereau de livraison, etc. ;\n\n– ils affichent un certain manque de transparence et un côté « boîte noire ». Ces logiciels ne sont pas faits pour être désossés. Ils contiennent par ailleurs la propriété intellectuelle de leurs éditeurs. Par conséquent, s’aventurer à customiser les rouages sans contrôle est risqué. Il est souvent préférable de ne pas trop interférer avec leur mécanisme ;\n\n– ils doivent garantir de hauts niveaux de disponibilité et de performance. Le niveau de service peut être de 24 heures sur 24, 365 jours sur 365 et les besoins de réponse en temps réel ;\n\n– ils intègrent de fortes contraintes d’intégrité pour garantir des transactions justes, opposables à des tiers ou respectant des contraintes légales. On ne saurait être tenté de venir manuellement ajouter de la donnée d’un logiciel de facturation ou de gommer une liste de livraisons reçues.\n\n→ Oui, mais chez nous c’est différent\n\nLa tentation de customiser les outils transactionnels est parfois grande. Combien de fois avons-nous pu entendre « nos processus sont différents et nous devons fortement adapter les solutions du marché » ? S’il faut respecter l’expérience et l’expertise de chacun dans l’émission de ces propos, il peut être pertinent de réfléchir également à des approches moins extrêmes où les processus et les outils peuvent être adaptés de manière synergétique et moins coûteuse. Cela peut permettre d’éviter :\n\n– des coûts importants de transformation de l’outil, car les compétences sur ce genre d’intervention sont souvent onéreuses ;\n\n– une complexification de la solution avec les problèmes de maintenance et de performance qui peuvent en résulter ;\n\n– de tomber dans une logique pernicieuse autant en processus transactionnels qu’en analytique qui conduit à bâtir des usines à gaz : « Nous savons mieux que les autres et, si le reste du monde fait différemment, ils ont probablement tort. »\n\nSi le retour sur investissement de l’insertion de ces customisations est clair et qu’il prend bien en compte les éléments ci-dessus, il n’y a pas de discussion. Dans les autres cas, il est peut-être plus sage de concentrer son énergie sur des éléments de gestion qui apporteront de la valeur et de la pérennité."
        }
      },
      {
        "json": {
          "pageId": "PAGE038_1757157753082",
          "text": "## L’analytique transactionnelle n’est pas l’analytique décisionnelle\n\nLa plupart des solutions ERP intègrent leurs analytiques et leurs tableaux de bord. Elles sont dédiées, spécifiques et en prise directe sur l’activité. Elles sont au plus proche du temps réel, ce qui est nécessaire pour les équipes chargées de l’exploitation qui ont besoin d’identifier les problèmes rapidement pour éviter des conséquences graves sur l’activité. Par exemple, la mesure en continu de la qualité de pièces produites, de conditions climatiques, de risque de fraude est nécessaire, car tout délai dans l’arrêt d’une machine, dans la protection d’infrastructures ou dans le blocage d’un compte pourrait être dommageable.\n\nLorsque les questions métier deviennent plus larges, nous allons avoir besoin de données plus diverses et de leur appliquer des traitements qui varieront en fonction des réponses recherchées et qui restitueront les résultats sous des formes diverses.\n\nLes solutions transactionnelles trouvent rapidement leurs limites pour centraliser ces données, pour proposer des analyses spécifiques et des rapports partageables avec des audiences larges.\n\nPersister à traiter ces questions décisionnelles dans le transactionnel entraîne quatre grands types de conséquences.\n\n\\> La naissance de monstres\n\nLa customisation du transactionnel aboutit à la naissance d’un système de plus en plus complexe et de moins en moins gérable. Ces adaptations sont de surcroît coûteuses, car elles requièrent des intervenants éditeurs ultraqualifiés.\n\n→ La progéniture des docteurs Frankenstein\n\nIl y a quelques années, un fabricant de composants réseau de la Silicon Valley s’est retrouvé confronté de manière brutale à une situation inextricable avec son ERP global. Pendant près d’une décennie de customisations à tout va, les équipes informatiques avaient évité les mises à jour de leur solution afin de ne pas avoir à redévelopper leur modification. Mais l’inéluctable est arrivé, lorsque l’éditeur a annoncé la fin de la maintenance de la version ancienne de son outil, il fallait mettre à jour un système qui ne ressemblait plus du tout à une installation standard. Une analyse des coûts aboutit rapidement à la conclusion que repartir de zéro était la seule option viable. Le groupe décida d’entreprendre une nouvelle implémentation et choisit cette fois-ci l’éditeur concurrent. L’implémentation dura deux ans et fut extrêmement douloureuse pour les équipes et pour l’activité qui dut être arrêtée pendant plus d’un mois pendant la migration. Rétrospectivement, ces problèmes auraient pu être évités si les équipes en charge avaient respecté leur ERP.\n\n\\> La mutation en escargot\n\nL’utilisation du transactionnel pour des analyses régulières a un impact sur la performance. Ces solutions sont calibrées pour un volume d’activités transactionnelles et l’ajout d’une charge analytique supplémentaire peut considérablement les ralentir, surtout lors de phases de charge intenses en production.\n\n→ La fièvre du vendredi soir\n\nLa scène prend place dans un beau magasin de vêtements féminins et de décoration, d’une chaîne haut de gamme, un vendredi en fin d’après-midi, dans le très select Stanford Shopping Center. Dans la douce odeur des parfums artificiels du magasin, la musique parfaitement choisie et un éclairage digne d’un studio de cinéma, une cliente se rend à la caisse avec un peu plus de 2 000 dollars d’articles. Il est 17 heures 10, elle sera à la maison pour 18 heures, juste à temps pour commencer une belle soirée. L’agent de caisse l’attend avec le sourire et commence à scanner les articles. Un premier beep rassurant, puis un deuxième… puis plus rien… : les articles ne passent plus. Le caissier tente une saisie manuelle du code-barres. La caisse enregistreuse ne répond pas. Les autres caissiers sont dans la même situation. Il est maintenant 17 heures 20 et la file aux caisses s’est allongée. Les clientes du vendredi soir s’impatientent. Elles regar­dent nerveusement les trois caissiers qui, malgré l’embarras de la situation, semblent habitués au problème. Certaines commencent à partir, laissant par terre leurs sacs remplis de vêtements et de bibelots. D’autres interpellent le personnel du magasin avec des menaces à peine voilées de plainte auprès du directeur. Mais toutes ont leur téléphone à la main, sur leurs réseaux sociaux préférés, critiquant à tour de bras le service de cette enseigne de renom.\n\nQue se passe-t-il pour que tous les vendredis à 17 heures les systèmes du magasin se mettent à ralentir au point de causer de tels retards aux caisses ?\n\nLa solution se trouve à 4 700 km de là, sur la côte est. Il est 20 heures là-bas et c’est la fin de semaine. Un analyste prépare activement les rapports du vendredi en commettant deux erreurs cardinales. Il attaque massivement le système transactionnel de facturation par des requêtes de données et fait tourner de gros calculs d’agrégation sur ceux-ci. Ses extractions sont par ailleurs mal programmées et mal ajustées à ses besoins : elles extraient beaucoup trop de détails. Parce qu’il est en prise direct sur le transactionnel, l’impact est immédiat : les systèmes commencent à ralentir notablement, entraînant les problèmes aux caisses.\n\nFaudrait-il investir dans des serveurs plus puissants ? Faudrait-il changer d’ERP ? Avec des ressources et du temps, toutes ces solutions seraient possibles. Mais l’approche la plus raisonnable ne serait-elle pas d’éviter de mettre le transactionnel à genoux sur des périodes de fonctionnement de magasin ? Une compréhension des technologies, des mécaniques de la donnée et des processus pourrait permettre de résoudre le problème simplement et sans grand surcoût.\n\n» Que feriez-vous pour alléger la charge du vendredi soir ?\n\nNous pourrions par exemple :\n\n– lisser la capture de données tout au long de la journée et de la semaine ;\n\n– valider le périmètre de la capture pour éviter une surextraction d’informations ;\n\n– effectuer les calculs sur un autre serveur que celui du transac­tionnel ;\n\n– extraire moins de détails pour le reporting du vendredi soir : les grandes lignes peuvent suffire dans un premier temps. Qui prendrait une décision d’urgence à cet instant-là ?\n\n– extraire toute la data une fois que tous les magasins sont fermés : les analyses détaillées ne pourront être faites que lundi matin de toute façon ;\n\n– ou, de manière plus élégante, n’extraire qu’un échantillon représentatif et inférer une tendance dès la mi-journée, pour permettre, sur la journée du vendredi, un vrai travail de réflexion et d’analyse.\n\n\\> La création de cocktails explosifs\n\nLes solutions transactionnelles ne sont pas organisées pour faciliter l’accès de l’utilisateur à la donnée. Les modèles de données visent la performance et la granularité. Les ouvrir à des analystes généralistes, non formés aux arcanes des ERP, est la garantie de gros problèmes, tant dans la qualité de la donnée extraite que dans la stabilité du système. En effet, le nombre élevé de tables ajouté à la complexité des conditions à définir pour une extraction précise peut donner des requêtes complètement fausses.\n\n\\> Le péché cardinal de la requête sans fin\n\nLes systèmes transactionnels ont des structures extrêmement complexes. Non seulement les données sont organisées de manière non naturelle pour un utilisateur métier, mais elles apparaissent en plusieurs exemplaires, sous des en-têtes incompréhensibles et requièrent des requêtes très complexes pour être extraites. Pourquoi faire compliqué ? L’objectif est ici la performance et l’intégrité du traitement des opérations : faciliter le travail de l’analyste n’est pas dans le cahier des charges.\n\nTous ces risques contribuent à augmenter la charge sur le transactionnel et à diminuer sa performance opérationnelle.\n\n» Cherchez autour de vous des exemples de dérogation à cette règle et constatez les conséquences de ces choix.\n\nRespecter l’intégrité du transactionnel est une règle quasi cardinale et nous pouvons comprendre les résistances justifiées des équipes informatiques, d’autant qu’il existe des solutions pour traiter exactement ces problèmes dans les règles de l’art."
        }
      },
      {
        "json": {
          "pageId": "PAGE039_1757157753082",
          "text": "# Les nouveaux besoins analytiques\n\nNous avons vu que, jusqu’à présent, les solutions transactionnelles contribuent à notre excellence opérationnelle. Mais, aujourd’hui, cet avantage concurrentiel peut ne plus être suffisant. Nous devons aussi être équipés pour l’excellence décisionnelle, celle qui va nous permettre de prendre des décisions rapides et pragmatiques sur des pans entiers de l’organisation, dans un monde en constante évolution."
        }
      },
      {
        "json": {
          "pageId": "PAGE040_1757157753082",
          "text": "## Le cycle de management\n\nÀ la maîtrise des cycles d’exploitation est venue s’ajouter celle du cycle de management. Il fallait apporter intégration et automa­tisation des processus de planification stratégique, de budget, de mesure de performance, d’analyse d’écart, et d’investigation de problème et de reporting.\n\nFigure 3. Cycle du management de la plupart des organisations\n\nLe cycle « stratégie-planification-action-mesure-ajustement » s’appuie sur l’ensemble des modules transactionnels tout autour du cercle. Les solutions décisionnelles au centre viennent unifier la donnée, pour une vision globale de l’entreprise.\n\nL’une des difficultés venait de ce que l’exercice devait être réalisé au niveau du périmètre entier des activités de l’entreprise. Il n’était plus question de chiffres détaillés au centime près et de mesure de chaque transaction, mais d’une approche plus globale en tendance et en agrégation. Il n’était plus question de se concentrer sur la production, la gestion de la paie ou les stocks, mais de faire la synthèse de tous ces éléments. Les solutions transactionnelles n’avaient pas été bâties pour cet exercice. Lorsque les métiers ont dû centraliser, préparer, analyser et diffuser leurs informations, ils se sont tournés vers la solution qu’ils maîtrisaient : les tableurs en général et Excel en particulier.\n\nLe besoin de suivre des données de plus en plus diverses, de plus en plus vite, a conduit à l’essor de l’analytique sous Excel et, dans bien des cas, à des sous et surexploitations de l’outil mal maîtrisées. Des jungles de feuilles de tableur ont ainsi proliféré, apportant à court terme des solutions rapides mais créant à moyen terme des processus creeper (voir chapitre 7, Chasser les processus Creeper). C’est ce qui a été communément appelé le Spreadsheet Hell (l’enfer des tableurs)."
        }
      },
      {
        "json": {
          "pageId": "PAGE041_1757157753082",
          "text": "## La naissance de la Business Intelligence\n\nLes outils de BI (Business Intelligence) sont venus combler le manque de solutions durables au-dessus des ERP et éviter ces proliférations anarchiques d’outils tableurs. Le terme apparaît dès les années 1950. On le repère dans un papier technique du IBM Journal of Research and Development de 1958.\n\nLa Business Intelligence… il y a soixante ans\n\nUn système automatique est en cours de développement pour diffuser des informations aux différentes sections de toute organisation industrielle, scientifique ou gouvernementale. Ce système de renseignement utilisera des machines de traitement des données pour réaliser des synthèses de documents et pour créer des profils d’intérêt pour chacun des « points d’action » d’une organisation. Les documents entrants et générés en interne sont automatiquement extraits, caractérisés par un modèle de mot, et envoyés automatiquement aux points d’action appropriés. Cet article montre la flexibilité d’un tel système pour identifier les informations connues, pour trouver qui a besoin de les connaître et pour les diffuser efficacement sous forme résumée ou en tant que document complet7.\n\nCe n’était pas très vendeur, mais le concept était là.\n\nEn 1989, Howard Dresner a proposé Business Intelligence comme terme générique pour décrire « les concepts et les méthodes pour améliorer la prise de décision commerciale en utilisant des systèmes de soutien basés sur des faits ».\n\nDans les années 1990, les solutions décisionnelles sont apparues avec des outils pour combler les manques des solutions transactionnelles. Après la vague ERP, le monde était prêt pour celle du décisionnel.\n\nMais là où les solutions transactionnelles avaient concurrencé des tâches manuelles et papier sans difficulté, l’adoption des solutions décisionnelles était confrontée à un existant bureautique tenace et à un manque de compréhension « data » de ses utilisateurs cibles. La population d’analystes était d’une grande diversité de profil, de niveau et d’appétence pour les chiffres.\n\nLes solutions décisionnelles apportent des fonctions jusqu’à présent gérées manuellement et devenues critiques :\n\n– centraliser des données de différents domaines pour avoir une vue d’ensemble ;\n\n– analyser de manière agile pour répondre à des besoins changeants ;\n\n– gérer efficacement des cycles budgétaires ou prévisionnels ;\n\n– diffuser l’information aux acteurs de l’entreprise ;\n\n– et, plus récemment, appliquer des mathématiques, des statistiques et des apprentissages par la machine (Machine Learning) pour analyser ou prévoir avec plus de précision. Ce qui est souvent rangé sous les vocables : Data Science, intelligence artificielle ou Machine Learning.\n\nPour y répondre, les solutions BI se sont organisées autour de quatre fonctions :\n\n– la capture et la préparation de données ;\n\n– le stockage orienté métier des informations ;\n\n– l’analytique ;\n\n– le reporting.\n\nFigure 4. Pile décisionnelle\n\nNous allons détailler les technologies à travers ce prisme.\n\nAttention terrain miné\n\nCher lecteur, vous entrez dans un domaine sensible pour beaucoup de personnes. Pendant des décennies, il a été précieusement gardé afin d’éviter la démocratisation de ses concepts. Des barricades de jargon technico-marketing ont été régulièrement érigées pour noyer les simples mortels comme nous. Une vingtaine d’années d’expérience dans le secteur et une quinzaine passée dans l’enseignement supérieur et en mentorat m’ont appris que ces concepts peuvent être présentés de manière simple, sans pour autant être simpliste.  \nNous ne ferons pas l’économie de quatre termes barbares et un tantinet rebutants, car il faut à un moment donné appeler les choses par leur nom. Une fois que vous aurez lu les quelques lignes qui les décrivent et surtout leur raison d’être, vous ne pourrez vous empêcher de lâcher un profond soupir et de vous dire : « Ah, c’est juste ça dont il est question ! » Nos quatre vilains mots (ou acronymes) sont :  \n– Middleware ;  \n– ETL (Extract, Transform, Load) ;  \n– MDM (Master Data Management) ;  \n– Base de données ou Database."
        }
      },
      {
        "json": {
          "pageId": "PAGE042_1757157753082",
          "text": "# Comprendre les solutions BI ou décisionnelles\n\nAfin de mieux comprendre les différents composants décisionnels et leurs interactions, remettons-les dans leur contexte. Le but du décisionnel est de répondre rapidement et pragmatiquement à des questions ou à des challenges métier grâce à une chaîne de production d’analytique efficace.\n\nQu’il y ait ou non des solutions logicielles dans ce processus n’est pas le point important. Ce qui compte est de comprendre que cette supply chain de la donnée doit constamment transformer de la donnée brute en information pertinente pour répondre aux problématiques métier. Elle doit le faire avec :\n\n– rapidité pour répondre sans délai aux problèmes ;\n\n– agilité pour s’ajuster à un environnement changeant ;\n\n– fiabilité pour éviter des erreurs grossières ;\n\n– durabilité pour qu’elle devienne un vrai instrument de gestion.\n\nCela ne vous rappelle-t-il pas les critères de fluidité évoqués plus haut (voir chapitre 3, Qu’est-ce que la donnée pour l’analytique en entreprise ?) ? Cette chaîne a été pensée pour répondre à ces exigences. Ce n’est pas forcément une solution logicielle, c’est avant tout un enchaînement logique de tâches que soit l’homme soit la machine devra réaliser."
        }
      },
      {
        "json": {
          "pageId": "PAGE043_1757157753082",
          "text": "## Le Middleware\n\nUne fois la question métier posée, tout processus analytique commence par de l’acquisition de données. Sans cette phase, rien ne peut commencer. Si la donnée est déjà à disposition comme c’est le cas sur des processus récurrents, c’est que cette tâche a déjà eu lieu en amont. Un copier-coller, une récupération de documents papier, un questionnaire, une saisie manuelle : toutes ces étapes peuvent servir à collecter une donnée que nous allons analyser.\n\nLe groupe de solutions en charge de cette étape est appelé dans le jargon le Middleware ou intergiciel en français.\n\nCe terme, néologisme aux sonorités barbares, est en fait plus simple qu’il n’en a l’air. En un mot, c’est comme Tupperware, mais on a mis « Middle » à la place ; et au lieu de boîtes en plastique, il s’agit de solutions techniques que l’on a placées pour faire le lien, dans le cas du décisionnel, entre les solutions transactionnelles et les sources données nécessaires à l’analyse. Ce sont bien les solutions qui sont au milieu (Middle) de deux technologies.\n\nCes solutions sont fondamentales parce que l’acquisition et la préparation des données sont les maillons les plus importants de la chaîne analytique. Tout analyste ou data scientist vous le dira : la préparation de données, c’est 80 % du travail. D’où ma recommandation de considérer cette phase critique comme une phase préliminaire à part entière et de ne pas la mélanger complètement avec la phase d’analyse. On peut certes faire des préparations de données de dernière minute, mais, si l’on veut des processus efficaces et durables, ce n’est pas la meilleure option à terme.\n\nÀ ce niveau, nous pouvons considérer qu’il y a trois grandes fonctions à réaliser :\n\n– centraliser l’information ;\n\n– aligner et normaliser les données pour qu’elles soient cohérentes entre elles ;\n\n– nettoyer et préparer pour pouvoir travailler sur une donnée un minimum propre.\n\n\\> Récupérer et préparer la donnée\n\nLe premier groupe de solutions récupère de l’information. Il va permettre d’accélérer, d’automatiser et de mettre à l’échelle cette phase, souvent couverte par des importations de fichiers ou des copier-coller. C’est le monde des ETL.\n\nETL est un acronyme qui veut simplement dire extraire, transformer, load (charger).\n\nEn clair, ces solutions vont chercher de l’information, la préparer pour qu’elle soit adaptée à l’analyse et la charger dans un endroit accessible (la base de données dans la majorité des cas). Et voilà, vous avez compris un premier mot de jargon absolument clé en analytique.\n\nDétaillons maintenant ces trois étapes. Nous allons voir qu’elles vont apporter beaucoup à notre chaîne analytique en termes de fluidité de données (voir chapitre 7).\n\nExtraire la donnée\n\nCombien de temps passons-nous à exporter de l’information de systèmes transactionnels ? Il nous faut penser à faire les extractions à temps, savoir où aller, passer par des menus à rallonge, se souvenir d’interfaces, penser à changer les filtres d’extraction, attendre, récupérer des extractions dans différents endroits, etc.\n\nCette tâche est totalement automatisée par les ETL. La fonction d’extraction automatique de l’ETL est quasi magique quand on se remémore les heures passées à centraliser manuellement des informations pendant des années. Elle va :\n\n– aller se connecter à toutes les sources de données nécessaires à l’analyse ;\n\n– sélectionner l’exact périmètre de données à extraire avec tous les bons filtres ;\n\n– choisir l’heure d’extraction pour éviter les impacts sur les systèmes transactionnels.\n\nTransformer la donnée\n\nCette possibilité peut être mise à profit pour réaliser bon nombre de calculs, de corrections ou d’agrégations. En les lançant pendant la phase d’ETL, nous améliorons notre processus analytique à plusieurs niveaux :\n\n– concentration sur l’analyse : nous n’avons plus à passer du temps à faire ces calculs ;\n\n– temps de traitement : les calculs sont réalisés sur de puissants serveurs en amont, pas sur notre machine de bureau ;\n\n– qualité de données : les transformations (voir chapitre 5, Opti­miser les analyses par les données calculées/préparées) et les calculs ont été développés en collaboration avec des ingénieurs sur des technologies spécialisées et, une fois les algorithmes validés, le risque d’erreur est nul (sauf si la donnée est mauvaise).\n\nCharger la donnée\n\nÀ ce stade, notre donnée a été extraite et raffinée et l’ETL nous offre la liberté de la déposer où nous le souhaitons. Finis les copier-coller, les importations qui durent des heures ou encore la recherche désespérée du fichier créé par l’informatique. Ce confort se cumule avec un gain de temps à chaque mise à jour de reporting, sans compter l’esprit en paix de savoir que toutes les données sont bien arrivées et à leur place.\n\n→ Commencer sa journée Data Ready\n\nDe tous les grands moments dont se souvient un responsable du contrôle de gestion, celui qui l’a toujours frappé était les réunions quotidiennes à 10 heures du matin avec son équipe. C’était autour d’un café qu’il discutait ouvertement des problèmes, réfléchissait aux solutions, mettait en évidence les défis commerciaux à résoudre et prenait en compte les informations qu’il devait produire pour le directeur financier, la direction et les partenaires commerciaux.\n\nCe qui avait lieu au cours de ces réunions de vingt minutes résumait ce qu’il appréciait le plus dans son travail en finance : des discussions impartiales et calmes fondées sur les informations dont chacun disposait et les analyses que chacun avait ou pouvait facilement produire. Le niveau de confiance et de confort autour de l’équipe était fort, tout comme la confiance dans les données et les rapports. Cette équipe était-elle composée de super-héros ? De personnes des plus grandes universités ? Pas du tout. Étaient-ils des bourreaux de travail sacrifiant leurs nuits et leurs week-ends pour arranger les choses ? Absolument pas. Étaient-ils terrorisés par leur direction ? Non plus. Ce qui faisait la différence, c’est que l’équipe se réveillait chaque jour avec de nouvelles données fraîches qui avaient été saisies, traitées, organisées, connectées pendant la nuit par des systèmes d’ETL efficaces.\n\nLeur première responsabilité n’était plus de trouver les données, ni le nombre de macros qu’il leur faudrait exécuter ou déboguer pour avoir un ensemble de données propres. Il s’agissait de lire les informations comme s’il s’agissait d’un livre ouvert et d’exploiter leur intelligence, leur expérience d’entreprise et leur créativité pour trouver de nouvelles façons de sauver la situation. Ce processus automatisé de collecte de données sans faille peut sembler un petit détail dans la séquence globale de gestion des données, mais il faisait toute la différence. Cela a eu un impact sur les gens, leur comportement, leur niveau de confort et de confiance. La disponibilité d’une solution offrant des données propres et à jour, accessibles de manière transparente par l’équipe, a amélioré de façon exponentielle leur capacité à servir leur organisation.\n\nUn élément essentiel de la vague d’analyse des données que nous traversons tous est la capacité de capturer et de centraliser les points de données clés indépendamment du nombre de systèmes, du volume et de la complexité des sources sous-jacentes. C’est la base d’une analyse précise qui, surtout, permet aux équipes de concentrer leur expérience, leur puissance de travail et leur collaboration sur le problème réel, d’enquêter au lieu de croquer les données.\n\nC’est l’art de l’ETL que nous devons tous apprendre à maîtriser ou du moins à contrôler. Qu’elles soient autonomes ou intégrées, le choix de ces solutions qui nous permettent de préparer les données chaque jour est une des clés du succès.\n\n» Avez-vous souvent l’occasion de profiter de réunions productives, celles où des équipes entières peuvent non seulement servir leur organisation et leurs partenaires commerciaux, mais aussi faire pleinement l’expérience d’effectuer un travail qu’elles apprécient vraiment ?\n\nLe positionnement du T (Transformation), au milieu ou à la fin de l’acronyme, indique à quel niveau se font les calculs. Vont-ils être réalisés sur le serveur source ou sur le serveur cible ? Même sans être technicien, nous pouvons avoir un avis. Il s’agit ici de savoir quel endroit va être le plus performant, causer le moins d’interruptions et dans lequel il sera plus facile de programmer les transformations. Ces réponses ne peuvent venir que d’une discussion constructive et donneront certainement lieu à des arbitrages et à des concessions entre les métiers et les différents groupes de l’informatique en charge des systèmes concernés.\n\nPourquoi évoque-t-on rarement l’ETL dans les processus analytiques ?\n\nTout d’abord, il s’agit d’une problématique peu sexy en comparaison des projets flamboyants de visualisation avancée ou d’analytique prédictive. C’est aussi probablement la discipline qui demande le plus de rigueur, d’organisation et de technique. Ce réseau de tuyaux qui vont acheminer l’information nécessite une bonne connaissance de la gestion de données qui nous fait souvent défaut (voir chapitre 5, Organiser la donnée pour une meilleure analytique). Elle nous met aussi en prise avec les services plus techniques, avec lesquels nous entretenons parfois des relations délicates.\n\n» Identifiez dans votre entourage les solutions d’ETL qui sont utilisées. Sont-elles appelées par le nom de leur éditeur (Talend, Stambia, Oracle, etc.). Sont-elles identifiées par le nom du produit (ODI, SSIS, etc.) ? Sont-elles nommées en fonction d’un projet ou d’un processus ? Ou bien sont-elles effectuées manuellement ?\n\n\\> Les solutions de Master Data Management\n\nLes données que nous avons préparées ne sont pas toutes prêtes à l’emploi. Elles peuvent arriver de plusieurs systèmes qui ont souvent chacun des référentiels différents. Afin de pouvoir par la suite joindre ces informations pour des analyses d’ensemble, il va falloir créer un référentiel commun pour que chaque client, chaque fournisseur, produit ou entité puisse être défini par des références uniques et validées, telles que l’adresse, le nom pour un client, une taille, un poids, une référence pour un produit (voir chapitre 5, Gérer les données maîtres : le Master Data Management (MDM)).\n\n\\> Les solutions de gestion de la qualité des données\n\nLa gestion de la qualité des données est un process complexe qui allie autant l’humain et les processus que la technologie et qui se situe tout au long de la chaîne. Je l’ai positionnée dans le Middleware pour la mettre au côté du Master Data Management dont elle bénéficie et pour montrer son importance en amont de la chaîne si nous souhaitons éviter de multiples corrections une fois les données mises à disposition et diffusées (voir chapitre 5, Établir une qualité holistique de la donnée).\n\n\\> Pour conclure\n\nVous comprenez désormais ce rôle absolument clé du Middleware en décisionnel. Vous en connaissez les termes principaux et vous allez pouvoir engager la discussion avec vos homologues de l’informatique qui se réjouiront d’avoir enfin des métiers moins ignorants sur le sujet.\n\nLe dernier mètre de la préparation de la data\n\nDepuis le début des années 2000, nous observons l’émergence d’outils de préparation de données orientés utilisateurs. Ces derniers permettent de couvrir le dernier mètre de la chaîne de préparation avec des solutions agiles, simples et souvent graphiques. Elles n’ont pas vocation à se substituer aux ETL qui gèrent les transferts massifs de data : elles sourcent le plus souvent leurs informations parmi celles raffinées par ces derniers et produisent des tables finales pour simplifier les analyses à suivre.  \n  \nFigure 5. Exemple de processus couvrant le dernier mètre de la préparation de données par un ETL géré par les analystes"
        }
      },
      {
        "json": {
          "pageId": "PAGE044_1757157753082",
          "text": "Ces solutions peuvent être des logiciels dédiés comme Alteryx ou être intégrées dans les solutions d’analyse comme avec Tableau Prep dans Tableau, PowerBI ou le Modeler de Pyramid Analytics.  \n  \nIllustration 3. Exemple de processus de préparation de données avec injection d’une table propre dans une base de données et dans un fichier texte  \nLe processus d’ETL sur Pyramid Analytics permet d’automatiser sur des grands volumes de nombreuses tâches manuelles que nous faisons traditionnellement sous tableur. Les tâches s’enchaînent de manière automatique et s’appliquent aussi bien à dix lignes qu’à plusieurs millions de lignes.\n\nL’émergence du Python pour la transformation de données\n\nLe Python est un langage créé dans les années 1980 par Guido van Rossum et popularisé au début des années 2000. Le nom a été inspiré par Monty Python’s Flying Circus. Si le data pionnier n’a pas vocation à devenir un développeur, il peut néanmoins s’aventurer dans cette discipline pour couvrir des besoins de transformation de données massives. Grâce à sa librairie de traitement de données « Pandas » (comme l’animal), il donne ainsi accès à une grande variété de fonctions capables de manipuler des tables de dizaines voire de centaines de millions de lignes sans effort. Utilisé de manière ciblée, pour éviter le redéveloppement d’usines à gaz telles que celles bâties par nos macros sous tableur, cette option peut s’avérer très utile pour intégrer une donnée raffinée à nos analyses et à nos visualisations."
        }
      },
      {
        "json": {
          "pageId": "PAGE045_1757157753082",
          "text": "## La base de données\n\nLa présence de la base de données à la suite de l’ETL est à la fois logique et nécessaire : où mettre cette information si bien préparée ? La base de données en décisionnel peut prendre des noms variés en fonction de sa taille ou de sa fonction. Il n’y a pas de règle formelle selon moi, mais on retrouve souvent :\n\n– Data Mart quand elles sont plus dédiées à un domaine donné ou de petite taille ;\n\n– Data Warehouse quand elles couvrent l’ensemble de l’entreprise ;\n\n– Entrepôt de données quand on veut rester franco-français ;\n\n– Data Lake quand on veut faire plus « digital », quand les volumes de data entreposés sont grands, structurés ou non structurés, avec un raffinage limité. Les Data Lake sont parfois complétés par des Data Mart pour entreposer la donnée une fois préparée pour les métiers ;\n\n– Oracle, AS400 ou encore Hana ou BW pour nommer ces entrepôts par la technologie qu’ils utilisent.\n\nCe qui est important, c’est de regarder au-delà des terminologies et de retenir la nature et le rôle de ces bases de données. En décisionnel, ce sont juste des endroits où l’information est rangée pour une analyse ultérieure. Cette clarification va d’ailleurs permettre de tourner les débats vers ce qui compte réellement pour les analyses qui vont suivre.\n\n\\> L’émergence difficile des bases vraiment décisionnelles\n\nNous avons quitté le monde du transactionnel et les objectifs sont maintenant de livrer des analyses pour répondre aux questions et aux challenges de la vie et de la stratégie de l’entreprise.\n\nOr, une tendance régulièrement observée est de reproduire dans ces bases décisionnelles l’exacte copie des bases transactionnelles. Cela part d’une bonne intention qui vise à soulager le transactionnel d’éventuelles requêtes ultradétaillées. Néanmoins, si l’on s’éloigne des analyses vraiment transactionnelles, peu d’analystes vont vraiment travailler au niveau ultragranulaire.\n\nLa plupart du temps, cette option conduit les utilisateurs à récupérer ces tables géantes pour les réduire à grands frais de macros et de copier-coller afin de recréer des bases sous tableur, chacun de son côté.\n\nUne autre tendance est de laisser le contrôle total de ces bases de données à des équipes techniques qui décident de ce qui y sera chargé et sous quelle forme. L’objectif de gouvernance et de contrôle centralisé est louable, mais bride complètement les utilisateurs finaux dans leur capacité d’organiser leur information en fonction de leurs besoins. Le besoin d’analyse d’une entreprise sera différent de celui d’une entreprise concurrente (voir chapitre 2, La tentation du voisin et de la bonne pratique). Si nous devons travailler à partir de la même base standard préparée par une tierce partie sans connexion avec le quotidien métier, nous devrons à chaque fois réajuster en local notre niveau de granularité et notre sélection de métriques, voire les données jointes à notre modèle. Nous sommes repartis pour des manipulations sans fin.\n\nEnfin, aujourd’hui, le monde de l’analytique tourne encore souvent sur tableur. Outre un déficit de culture data, des parcours de formation pas toujours adaptés et un management qui peine à se séparer de ses rapports Excel, une des raisons de cet enracinement des feuilles de calcul dans le monde décisionnel peut s’expliquer par le manque total d’autonomie laissé aux analystes par les solutions de stockage actuelles.\n\nCe n’est pas tant un manque de fonctionnalités des bases de données, mais un manque total de contrôle des chaînes décisionnelles de la part des métiers qui poussent au retranchement dans des solutions que nous maîtrisons et sur lesquelles nous avons toute liberté : le tableur.\n\n\\> Les qualités intrinsèques d’une bonne base décisionnelle\n\nNous devons nous concentrer sur les besoins métier pour comprendre ce qui va être important dans le choix et la gouvernance de ces bases de données.\n\nAnalystes, data scientists et informaticiens doivent travailler de concert dans l’organisation de ces entrepôts de données. On laissera le choix de la technologie aux techniciens, le budget à la finance et le nom de cette architecture aux pinailleurs.\n\nQuels sont les éléments primordiaux et sur quoi il faudra être inflexible, sous peine de retomber sur des proliférations de feuilles Excel et de temps perdu en aval de la base de données ?\n\nLa capacité de la base de données à stocker le bon volume et à traiter des calculs de préparation de données\n\nSouvenez-vous, l’ETL, ou plutôt l’ELT, peut amener à faire porter à la base de données la charge de nombreux traitements de préparation lors de la récupération de données. Aujourd’hui, le coût de stockage est insignifiant et, si nous nous concentrons vraiment sur nos problématiques métier, nos volumes n’atteindront pas les niveaux du transactionnel. L’accès à des puissances de traitement est facile et peu onéreux grâce au Cloud ou à l’achat de serveurs privés. Par conséquent, ce critère sera facilement atteint.\n\nLes données massives en Data Science\n\nLe cas du traitement de données massives en Data Science peut être considéré différemment. Vouloir privilégier et démocratiser des outils ultraperformants et pointus aux dépens des outils d’analytique accessibles peut finir par laisser les analystes sans solutions et les renvoyer dans les tableurs. C’est un peu comme supprimer les petites berlines pour les trajets quotidiens au profit de Formule 1, parce que l’on a un champion de la course automobile dans l’équipe.\n\nLa rapidité de modélisation et d’adaptation\n\nLa majorité des données de base telles que les ventes, la paie, les stocks, les clics sur le site internet, etc. vont peu évoluer dans leur structure. En tant qu’analystes, nous connaissons les métriques et les attributs nécessaires : nous les avons établis, nous les récupérons à chaque ETL et nos rapports de base peuvent fonctionner. Ce qui va faire l’objet d’évolutions régulières sont :\n\n– les nouvelles informations que nous allons devoir collecter pour enrichir nos analyses et répondre à de nouvelles questions métier ;\n\n– les nouveaux points de vue que nous allons considérer pour regarder une situation sur d’autres angles ;\n\n– de nouvelles préparations de données pour des besoins plus raffinés.\n\nIl faudra que bases de données et processus de maintenance soient suffisamment agiles pour coller aux besoins du terrain et préparer la donnée en conséquence.\n\nLa possibilité d’avoir des zones de prototypage\n\nElles sont souvent appelées bacs à sable (sandbox) pour les utilisateurs. La création de ces espaces a plusieurs effets vertueux :\n\n– tester l’intégration et l’utilisation de nouveaux jeux de données dans un environnement data et non pas dans un enchevêtrement de tableurs ;\n\n– engager les analystes à se bâtir une culture data par une pratique régulière proche des règles de l’art.\n\nCe dernier point risque de picoter un peu, car il y aura à la fois une courbe d’apprentissage et une courbe de changement (voir chapitre 6, La courbe de changement digital), mais c’est cette orientation qu’il faut prendre, sous peine de rester coincé dans l’enfer des tableurs.\n\nElle peut être effectuée en plusieurs temps avec tout de même un premier prototypage rapide sur Excel ou fichiers plats, plus un deuxième plus détaillé dans le bac à sable, avant l’intégration éventuelle dans la base de données décisionnelle pour une mise à disposition plus large. Cette dernière étape n’est pas forcément nécessaire, car certaines analyses peuvent être trop spécifiques pour justifier une industrialisation et un partage large au travers de l’entreprise.\n\nTableur contre prototypage en « bac à sable »\n\nLa première étape sur feuille de calcul permet d’être extrêmement agile dans un environnement que l’on connaît. Cependant, essayer de l’éviter le plus souvent possible accélère l’apprentissage des réflexes « base de données » par l’usage. L’excellence acquise par la répétition permet de prototyper aussi vite dans le Data Warehouse que dans Excel. Ce peut être plus lent au démarrage certes, mais ce retard est plus que comblé, lors des rafraîchissements de données et des calculs, par l’identification précoce de challenges dans l’organisation des informations et par l’économie d’un portage ultérieur du tableur à la base de données. Il permet de plus une application d’outils de reporting dès le départ, évitant ainsi la création de rapports temporaires sur tableurs.\n\nCes prérequis sont rarement pris en compte dans les choix de bases de données. Nous faisons de ces bases décisionnelles une affaire de technologie ou de data brute, alors qu’elles représentent le point de contact entre l’informatique et les métiers et qu’elles devraient être un formidable travail collaboratif… Des Data Warehouse sous-utilisés et des armées de travailleurs sous Excel ruinent les grands objectifs d’une data démocratie.\n\nUn article8 évoque la nécessité de donner aux utilisateurs décisionnels des bases de données qu’ils peuvent vraiment maîtriser, quitte à laisser de côté certaines avancées technologiques et un peu de performance. À quoi servent-elles en effet si on ne peut pas les maîtriser ?\n\nRetour vers le futur\n\nPendant la dernière décennie, les entreprises ont fait l’expérience des bases de données avancées (type NoSQL) (voir chapitre 5, Les bases de données NoSQL (Not Only SQL)) en y voyant une arme absolue pour stocker toujours plus de données et réaliser des calculs toujours plus gourmands en ressources.  \nMais, très rapidement, elles se sont rendu compte que des données entreposées sans contrôle et sans but n’avaient pas de valeur. Elles ont aussi réalisé que ces bases de données, d’accès complexe et technique, était coupées du monde des analystes et n’apportaient pas leurs promesses analytiques car :  \n– elles nécessitaient un support technique chevronné, laissant les métiers sans autonomie et les informaticiens peu formés sans contrôle ;  \n– elles étaient lourdes en codage et en maintenance ;  \n– elles ne pouvaient pas coller à une réalité opérationnelle changeante ;  \n– elles stockaient une donnée peu raffinée et impropre pour une analyse rapide et fiable ;  \n– elles n’étaient pas accessibles avec les langages familiers des analystes tels que le SQL ;  \n– elles n’étaient pas structurées en tables, plus naturelles à comprendre ;  \n– elles n’étaient pas toujours ACID (atomicité, consistance, isolation et durabilité) (voir chapitre 5, Les bases de données relationnelles) ;  \n– enfin, paradoxalement, elles n’étaient pas aussi rapides que certaines solutions conventionnelles, car elles passaient parfois plus te temps à distribuer les données et les traitements sur plusieurs machines qu’à effectivement réaliser les calculs.  \nRapidement les entreprises, même les plus avancée, ont dû réintroduire des approches plus conventionnelles pour maintenir la data et les métiers en prise.\n\n\\> La place du Cloud dans le traitement des données\n\nLa facture des services Cloud est arrivée.\n\nLe Cloud ne change rien aux configurations détaillées plus haut. Comme bien souvent, les nouvelles technologies ne modifient pas les paradigmes de base de la data : sa chaîne de valeur et de traitement est restée la même depuis la naissance de l’humanité. Le Cloud est une formidable révolution, mais, pour notre pratique décisionnelle, s’attarder sur le jargon en se perdant sur des détails techniques n’est pas pertinent. Mon confrère et ami Edward Roske, directeur général d’InterRel (société de consulting en analytique) et un des experts en Business Analytics aux États Unis en donne deux définitions :\n\n– le Cloud, c’est juste un ordinateur avec un très long câble réseau ;\n\n– le Cloud, c’est juste vos applications qui tournent sur un ordinateur qui ne vous appartient pas."
        }
      },
      {
        "json": {
          "pageId": "PAGE046_1757157753082",
          "text": "Finalement, nous avons tous des expériences similaires au Cloud, lorsque nous nous connectons en interne à nos serveurs d’entreprise. Pour l’utilisateur, ces changements de paradigme technologique ne sauraient justifier l’abandon de la discipline et de la rigueur avec laquelle il doit gérer sa data. Le Cloud n’est pas plus magique que les systèmes en local.\n\nNous verrons dans le chapitre 5 les différents types de base de données et leurs domaines de prédilection."
        }
      },
      {
        "json": {
          "pageId": "PAGE047_1757157753082",
          "text": "## Les solutions d’analyse\n\nDans ce chapitre sur les outils, j’aurais pu ne parler que de ces merveilleuses solutions d’analyse qui transforment la donnée en information raffinée, en graphiques colorés ou en tableaux riches de calculs en ligne et en colonne.\n\nMais cela aurait été comme parler en premier du glaçage d’un gâteau dans une recette de cuisine : j’aurais omis tout le travail en amont pour réaliser le gâteau. En effet, une fois notre donnée collectée, raffinée, préparée et stockée, le travail d’analyse ne repose plus vraiment sur de la technologie, mais surtout sur notre sens critique et notre curiosité.\n\nLes solutions d’analyse sont aussi appelées solution de Business Intelligence, parfois de reporting ou encore par le nom du vendeur de logiciel ou de son logiciel (tableau, PowerBI, Qlick, Spotfire, Pyramid Analytics… Excel). Encore une fois, ce qui nous intéresse n’est pas le nom et les débats sans fin pour faire la différence entre du Data Analytics, du Mining ou de la Business Intelligence. Nous devons nous concentrer sur l’objectif de cette étape : trouver les pépites d’information nécessaires à la prise de décision.\n\nMême si elles varient d’une solution à l’autre, la plupart des solutions d’analyse offrent les mêmes fonctionnalités : la data étant ce qu’elle est, il n’y a pas cent façons d’en faire des tableaux et des graphes.\n\n\\> La présentation de la donnée\n\nPouvoir prendre des en-têtes de colonnes d’un clic et les déposer dans des cases afin de bâtir les analyses, sans se soucier des risques d’altération de données, d’oubli de cellules et avec une rapidité jusque-là jamais atteinte a changé notre façon de travailler.\n\nNous y gagnons en temps, en précision, en qualité de restitution, en performance et en confort : tout cela en même temps. Seule est sacrifiée l’option de pouvoir retoucher, altérer ou cosmétiser son rapport à la dernière minute par le changement d’une cellule ou l’ajout d’un élément graphique atypique.\n\nIllustration 4. Écran typique d’une solution analytique\n\nLes colonnes disponibles pour l’analyse sont à gauche, les « étagères » pour les cliquer-déposer et former les tableaux et graphiques sont dans la colonne adjacente sur la droite. Les différentes étagères pilotent ce qui va être en ligne, en colonne, en couleur, en taille, en filtre, en infobulle, etc. Le résultat apparaît dynamiquement dans la partie centrale, avec sa légende à droite.\n\nCette représentation a été réalisée en quelques secondes sur la base de plusieurs sources de plusieurs dizaines de milliers de lignes chacune, seulement à la souris9.\n\n\\> Les calculs finaux\n\nIl est toujours préférable de concentrer les efforts de calcul en amont de la phase d’analyse. Néanmoins, certaines opérations devront attendre cette dernière phase :\n\n– les calculs ad hoc visant à répondre à une question précise. Ils peuvent se faire à partir de colonnes, parfois de lignes, avec une grande diversité de formules ;\n\n– les agrégations en somme, moyenne, comptage, minimum/maximum, etc. de la donnée pour obtenir un niveau de granularité pertinent ;\n\n– la transformation de certains champs pour des raisons esthétiques ou pour corriger des problèmes de qualité de dernière minute (voir chapitre 5, Tirer le meilleur de sa donnée avec une bonne préparation).\n\n\\> La création de mini-processus analytiques en local\n\nComme nous l’avons vu plus haut, certaines solutions d’analyse offrent tout ou partie de la panoplie des outils data à leur utilisateur pour lui apporter autonomie et lui donner la possibilité de développer localement des analyses à partir de données brutes.\n\nLes solutions de BI actuelles offrent souvent :\n\n– des capacités d’ETL pour aller chercher de la donnée qui n’a pas encore été prise en compte dans les bases de données ;\n\n– des capacités de préparation pour raffiner localement des tables brutes, sans avoir à passer par des étapes manuelles ou des tableurs ;\n\n– la possibilité de créer en mémoire une mini-base de données pour organiser sa donnée proprement avant l’analyse.\n\nOn pointe souvent le risque de création de shadow IT ou shadow datamart (développement utilisateurs fait en dehors du contrôle de l’informatique et de la gouvernance d’entreprise). Je préfère considérer le potentiel de ces outils qui permettent le maquettage rapide de nouvelles analyses par les utilisateurs, dans un contexte de bonne pratique data et avec des développements qui pourront être facilement portés vers le Data Warehouse."
        }
      },
      {
        "json": {
          "pageId": "PAGE048_1757157753082",
          "text": "## Le reporting\n\nBien souvent, les solutions analytiques incluent un volet reporting pour connecter directement la production d’analyses à sa diffusion. Cette fonction est particulièrement pratique, car elle permet d’intégrer une couche cosmétique et de diffuser ses résultats sans avoir à changer d’environnement. Elle incite néanmoins parfois à bâtir des usines à gaz pour des présentations parfaites au pixel près et fait perdre de vue l’objectif principal.\n\nCette dernière étape est trop souvent confondue ou incluse avec la phase d’analyse. L’analyste veut se faire plaisir et devient un Picasso du reporting aux dépens de l’analyse et de l’investigation. Il est souvent sous la pression du demandeur du rapport qui peut exiger une œuvre avec laquelle il pourra parader en réunion, sous le regard fier de l’analyste qui affichera avec fierté : « C’est moi qui l’ai fait. »\n\n\\> À quoi sert le reporting ?\n\nSi l’on considère que l’analyse a été bouclée en aval, il ne reste plus qu’à préparer la mise en forme et la diffusion de cette information. Quels sont les éléments qui doivent nous guider pour ne pas rater cette tâche ?\n\nIl faut revenir à la question de base : quel est l’objectif d’un reporting ?\n\nUn reporting efficace et utile doit entraîner :\n\n– une décision : nous comprenons la situation, nous voyons le problème, nous sommes informés et nous décidons ;\n\n– une discussion : nous avons repéré des éléments intéressants, mais nous avons besoin d’être orientés, voire aidés pour continuer notre démarche. Nous déclenchons alors un échange avec notre hiérarchie, nos équipes ou nos confrères ;\n\n– un confort, une confirmation : la situation est sous contrôle et nous pouvons continuer à nous consacrer aux problématiques qui comptent pour l’entreprise.\n\nEn d’autres termes, un bon reporting permet que nos analyses aient un impact et que tout ce travail de préparation-stockage-analyse de données ait un retour sur investissement.\n\nLa taille, l’épaisseur, les couleurs et le degré de raffinement sont-ils importants pour remplir ces objectifs ? Cela va dépendre de l’audience. Choisir le format du reporting doit être directement relié à ce que les lecteurs préfèrent recevoir. Nous avons tous nos sensibilités : certains d’entre nous préfèrent lire, d’autres aiment les images, j’aime personnellement une conversation qui va droit au but. Cer­tains veulent beaucoup de détails pour se rassurer, d’autres demandent juste la synthèse, quant à moi, je préfère une présentation à géométrie variable au tableau blanc. Si nos yeux sont fatigués, il nous faudra de gros caractères ; si nous sommes pinailleurs, il faudra des titres, des sous-titres et des légendes partout ; si nous avons confiance en nos équipes, des informations simples et pertinentes suffiront.\n\nUn bon rapport est un rapport qui « déclenche » une action.\n\nOn peut dès lors se poser la question de la valeur « temps passé » par les équipes à peaufiner des rapports jusqu’au moindre détail de police de caractères. S’il est normal de présenter des documents propres avec un format constant dans la durée pour que le lecteur s’y retrouve, est-il vraiment raisonnable d’utiliser le temps de brillants analystes à cela, quand nous aurions pu nous contenter d’une simple page d’analyse, d’un bref mémo et d’une discussion ?\n\nL’exemple suivant illustre un cas extrême.\n\n→ La Heat Map de Meena\n\nMeena est une jeune analyste au parcours académique brillant dans une des sociétés de pointe de la Silicon Valley au sein du service d’analyse financière. Sa renommée en interne, qui a fait le tour des services, s’est bâtie sur sa fameuse Heat Map, un rapport d’analyse d’écart qui affiche en matrice, par filiale et centre de coût, l’importance des écarts réel/budget par voie d’intensité de couleur. Les fonctionnalités de la dernière version d’Excel, si longuement attendues, lui permettent d’afficher des couleurs dégradées et chatoyantes qui font l’admiration des collègues.\n\nMeena passe deux à trois jours par mois à faire ces graphiques, soit entre 10 et 14 % de son temps mensuel hors week-end. Ses automatisations requièrent plus de maintenance que prévu et il y a toujours un nouveau détail graphique qu’il est bon d’ajouter. À chaque fin de période comptable, elle présente à son directeur financier cette impressionnante liasse bigarrée d’une vingtaine de pages, imprimée en cinq exemplaires. Sa contribution à la déforestation et à l’achat de cartouches d’encre couleur est palpable.\n\nL’échange est irréel : son patron se penche sur le document, acquiesce devant la qualité graphique et l’impeccable maîtrise du tableur et, après quelques minutes de revue, pointe du doigt les zones rouge orangé en demandant : « Nous sommes clairement en décalage ici. Meena, tu as plus d’information ? » Meena approuve cette analyse fine du chef. Avec un regard entendu, elle se lève et répond : « Je vais m’en occuper, boss, pas de problème. Je reviens avec l’info demain. »\n\n» À votre avis, quels sont les éléments qui pourraient être améliorés pour gagner du temps sur vos rapports actuels ? Quelles optimisations permettraient de vous concentrer sur ce qui compte et de finalement consacrer vos compétences à ce qui est important ?\n\nJe vous propose les observations suivantes, avec une formulation volontairement provocatrice, qui ne tiennent pas compte du fait que Meena et son patron avaient forcément les meilleures raisons de procéder de la sorte (et c’est là ma première observation provocatrice) :\n\n– pourquoi passer plus de 10 % de son temps à faire de la cosmétique sur tableur ? Quand on est diplômé d’une grande université ou riche d’une grande expérience, est-ce la meilleure utilisation de son temps ?\n\n– pourquoi produire à grands frais un rapport qui n’est qu’une étape d’une démarche d’analyse d’écart ?\n\n– pourquoi ne pas simplement présenter les écarts en rouge et, avec le temps gagné par la simplification du rapport, investir dans les analyses de ces écarts ?\n\n– pourquoi passer par une représentation graphique chatoyante quand l’audience est une personne qui veut juste aller droit au but ?\n\n– pourquoi le directeur financier, pourtant garant de la rentabilité de l’entreprise, laisse-t-il autant de valeur salariale et humaine se perdre dans des activités aussi futiles (le coût complet de ce rapport jetable mensuel, fondé sur le taux horaire de la personne, était à l’époque de 3 000 dollars) ?\n\n\\> Les grandes familles de reporting\n\nÀ la question « quel type de reporting choisir ? », la réponse est à la fois :\n\n– simple : le reporting qui aura un impact sur son lecteur ;\n\n– et compliquée : le reporting qui aura un impact sur son lecteur.\n\nCe qui nous laisse à la fois la liberté dans les options, mais également la difficulté de choisir quelles options. Aussi, je vous propose de les revisiter non pas en décrivant leur infinie variété, mais en fonction de leur performance au regard de la rapidité avec laquelle ils peuvent amener à faire prendre une décision. Nous verrons les grands types de visualisations plus loin dans le chapitre.\n\nOn peut considérer deux grandes familles de reporting :\n\n– les revues récurrentes telles que les reportings mensuels, quotidiens, etc., qui s’assurent du maintien d’une performance ou d’une orientation stratégique ;\n\n– les revues ad hoc pour répondre à une question particulière.\n\nLes revues récurrentes\n\nLes reportings récurrents sont ceux qui donnent le pouls d’une activité : ils rassurent, ils visualisent les grandes tendances, les indicateurs clés de l’entreprise. Parmi les classiques, on retrouve les rapports comptables, d’écarts budgétaires, de vente, de production, etc. Ils ne sont pas à proprement parler des vecteurs de décision, mais plutôt des supports d’information pour donner à tout le monde le choix de se faire une opinion sur le cours de l’activité ou d’un projet.\n\nSont-ils de ce fait en opposition avec les objectifs plus haut ? Non, s’ils sont le support d’une discussion qui à son tour mène à des décisions et à des actions. Dans ce cas, nous pourrions arguer que cela devrait être à l’analyste d’arriver déjà préparé avec ses propres analyses complémentaires et des premières idées à travailler ou à trancher. Si ces rapports sont le support d’une vraie réunion de travail, dans laquelle des experts partagent ouvertement leur réflexion sur ces données communes dans le but de définir des plans d’actions ou bien sont l’occasion de connecter des éléments que l’analyste ne pourrait pas saisir à son niveau, ils justifient alors pleinement leur fonction.\n\nLes rapports tels que les bilans, les comptes de résultat, les liasses fiscales, les rapports annuels, qui visent plutôt l’information générale, parfois réglementaire, de lecteurs qui pourront poser des questions, mais n’auront pas directement ou pas du tout de capacité de décisions opérationnelles (actionnaires, fisc, communautés, etc.), ne rentrent pas dans ces critères. Il s’agit plus de communication et nous verrons qu’ils peuvent être produits différemment (voir infra, Les infographies et autres publications sur mesure). On pourra arguer que l’analyste se doit de préparer de nombreuses analyses pour valider et expliquer les chiffres dans le cas où un texte d’accompagnement est nécessaire, ou simplement pour se préparer aux questions des parties tierces.\n\nCompte tenu de leur valeur décisionnelle, ces reportings récurrents de situation ne devraient pas encombrer les journées des équipes opérationnelles. Parce que ces rapports sont souvent standardisés et stables dans le temps, ils devraient être complètement automatisés, une fois leurs sources de données validées et expliquées… par les analyses ad hoc.\n\nLe reporting ad hoc\n\nCes rapports peuvent avoir un caractère récurrent comme exceptionnel : leur point commun est que ce ne sont pas des rapports formels et fixes . Ils visent l’analyse d’un point ou d’un domaine particulier dans le but d’expliquer une problématique et de déclencher une action ou une réflexion de plan d’action. Ils reflètent la séparation saine entre la phase de recherche et d’analyse et celle de la communication de résultats dans laquelle l’analytique n’est pas freinée par des considérations esthétiques.\n\n\\> Les grands formats de reporting\n\nL’objet n’est pas ici d’établir un inventaire de modèles à suivre, mais plutôt de décrire les options disponibles, celles auxquelles vous n’auriez pas pensé, que vous auriez dédaigné ou qui pourraient vous inspirer. Nous allons également lister leurs points forts (quoique le réel point fort d’un reporting soit, encore une fois, sa capacité à générer des actions et des réflexions) et surtout leurs points faibles ou leurs inconvénients, car c’est souvent là que beaucoup de valeur se perd.\n\nLe tableau de bord\n\nIl vise à rassembler tous les indicateurs nécessaires pour le suivi d’une activité. Sous format électronique ou papier, il donne une vue d’ensemble d’une situation à travers plusieurs angles et permet de l’analyser dans sa globalité.\n\nAttrayant, coloré, dynamique dans sa version électronique, il a été pendant longtemps et est certainement encore la Rolls-Royce du reporting (on entend encore souvent résonner dans les salles de conseil d’administration : « Je veux mon tableau de bord ! »). Sa création et sa gestion posent toutefois quelques questions :\n\n– il est coûteux en temps et en complexité de mise en œuvre. Il nécessite un important travail de collecte, d’agrégation et d’alignement de données variées. Il requiert une maîtrise de la mise en page et/ou de la technologie qui le supportera s’il est électronique ;\n\n– il est également peu agile. Reconstruire ou adapter un tableau de bord n’est pas toujours trivial.\n\nDans le contexte d’un tableau de bord couvrant les fondamentaux d’une activité, on peut trouver un bon retour sur investissement. Ces fondamentaux n’évoluant que très peu en principe, le tableau de bord peut valoir la peine d’être développé consciencieusement.\n\nLe rapport"
        }
      },
      {
        "json": {
          "pageId": "PAGE049_1757157753082",
          "text": "Ce dernier prend souvent la forme papier, ou papier électronique (PDF). Son format reprend souvent les codes et les styles des documents de l’entreprise. Ils font l’objet d’une attention minutieuse dans leur création, avec des alignements, des fontes et des couleurs. Leurs créateurs rivalisent souvent d’inventivité pour présenter des nouvelles techniques de mise en page.\n\nLe rapport peut trouver sa justification s’il trouve sa cible et crée l’action comme tout autre reporting. Il souffre néanmoins de quel­ques faiblesses qu’il est intéressant de considérer avec pragmatisme :\n\n– comme les tableaux de bord, les rapports sont complexes à bâtir et à maintenir ;\n\n– ils restent souvent et jalousement « la propriété » et sous le contrôle exclusif de ceux qui les ont créés. Leur existence se limite souvent à la durée de poste de leurs créateurs. Ces œuvres d’art seront souvent remplacées à l’arrivée de nouveaux responsables qui sans nul doute pourront faire mieux, plus vite et surtout plus beau, en profitant des nouvelles fonctions gadgets des versions logicielles ultérieures ;\n\n– ils n’offrent pas la possibilité d’interactions avec la donnée et doivent donc faire un compromis habile entre concision et couverture d’un sujet et de ses variations ;\n\n– leur automatisation peut être obtenue comme celle du tableau de bord, mais leur complexité intrinsèque conduit souvent à des ajustements, à des corrections, à des « bidouillages » et à des crises de nerfs sur des macros et des liens interfichiers qui consomment toujours beaucoup plus de temps que prévu ;\n\n– l’habitude créée par un format particulier force parfois les analystes à tordre leurs outils BI pour leur faire reproduire les formats livrés par les solutions précédentes au pixel près.\n\nIl en résulte souvent que ces rapports sont rarement accompagnés d’analyses détaillées, faute de temps pour les réaliser : alors qu’ils devraient être le point de départ de l’analyse, ils en sont souvent la fin.\n\nLe rapport reste néanmoins une valeur sûre dans les chaînes de communication, même s’il n’est peut-être plus en parfaite adéquation avec les exigences d’un monde qui évolue chaque jour.\n\nLe tableau de chiffres\n\nLes tableaux de chiffres restent très populaires. Ils présentent moins de complexité à bâtir, même si la quête du style parfait continue à animer des analystes en besoin de créativité artistique.\n\nIls sont particulièrement efficaces pour les personnes qui aiment s’immerger dans les chiffres pour alimenter leur courant de pensée. Ils sont aussi redoutables pour mettre à plat une situation de manière détaillée avec des matrices de données ou des tableaux croisés.\n\nNéanmoins, ils demandent à leur lecteur un travail d’analyse. Dans certains contextes, il s’agit d’une phase intéressante de supervision d’analyse comme pour le rapport, voire le tableau de bord. Ces revues à plusieurs niveaux permettent de repérer davantage d’erreurs ou de problèmes. Mais ils s’adressent souvent à des lecteurs cadres supérieurs, voire dirigeants, qui devraient se concentrer sur des missions plus stratégiques. Éplucher les chiffres produits par ses équipes traduit soit un manque de confiance en elles, soit une nostalgie d’un temps où ces responsables étaient eux-mêmes analystes, soit un artifice pour repousser au niveau hiérarchique suivant la responsabilité de l’action ou de la décision.\n\nLes visualisations classiques\n\nSouvent inclus dans les tableaux de bord ou les rapports, ce média peut aussi être un reporting à lui seul. L’image qui vaut mille mots peut aussi valoir des millions de chiffres tant est grande sa capacité à synthétiser une situation complexe.\n\nQuel est le meilleur graphique pour une situation donnée ? Cela dépend de son objectif : toujours et encore déclencher une action, une décision ou une réflexion. Si certaines visualisations sont plus adaptées à certains cas, leur choix devrait dépendre de leur capacité à faire bouger leur audience en attirant l’attention par exemple sur une tendance, une différence, une concentration ou une exception atypique, en jouant sur les formes, les tailles et les couleurs et leur arrangement.\n\nParmi les visuels classiques, nous allons retrouver :\n\n– les graphiques en barres ;\n\n– les graphiques en ligne ;\n\n– les « camemberts ».\n\nL’application de quelques règles fondamentales permet de leur conserver utilité et pertinence : le simple a parfois du bon.\n\nLe graphique en barres va permettre de comparer des tendances et/ou des répartitions. Il peut être exprimé sous plusieurs variations telle que :\n\n– les barres horizontales ou verticales ;\n\n– les barres superposées ou côte à côte ;\n\n– les barres qui montrent les proportions ;\n\n– les barres dont la largeur varie ;\n\n– les barres qui s’enchaînent en partant du niveau de la précédente (graphique en cascade) ;\n\n– les barres boîtes à moustaches, aussi appelée box-and-whisker plot.\n\nIllustration 5. Graphiques en barres\n\nIllustration 6. Exemple de graphique en cascade : suivi de l’évolution d’un solde bancaire\n\nLes « boîtes à moustaches »\n\n  \nIllustration 7. Exemple de graphique en boîte à moustaches : revue de l’espérance de vie d’une sélection de pays à la fin des six dernières décennies  \nLa dernière boîte indique un « haut de moustache », à savoir l’espérance de vie la plus haute à 80 ans, un premier quartile à 77 ans et une médiane à 71 ans. On observe facilement l’amélioration de l’espérance de vie avec une amplitude entre les pays qui reste assez stable sauf en 2000.  \nLes boîtes à moustaches sont un moyen simple de visualiser les éléments clés d’une série statistique. Elles synthétisent la médiane, les quartiles/déciles, le minimum et le maximum des populations mesurées. Le rectangle central va du premier quartile au troisième quartile. Il est coupé par la médiane. Les segments aux extrémités donnent les valeurs extrêmes, ou les premier et neuvième déciles. Ce sont les moustaches du graphique.\n\nLeur application idéale est pour comparer des valeurs entre grou­pes, tels que des pays, des mois ou des clients par exemple. Le choix d’un format horizontal, vertical ou d’une variante dépend du message à passer.\n\nAttention, les graphiques en barres utilisés pour des nombres de catégories élevées perdent de leur lisibilité. Des catégories telles que des jours pourront mieux être représentées par une ligne.\n\nLes graphiques en ligne(s) montrent des tendances, des saisonnalités sur des catégories plus continues.\n\nOn retrouve :\n\n– les lignes qui relient les points de chaque catégorie ;\n\n– les lignes lissées (spline) ;\n\n– les lignes à paliers, qui représentent les plafonds d’un graphique en barre invisible ;\n\n– les lignes seulement désignées par leurs points.\n\nIllustration 8. Graphiques en ligne(s)\n\nIllustration 9. Exemple de graphique en ligne : production de bière d’un pays en milliers d’hectolitres par mois\n\nOn observe clairement sur l’illustration 9 les grandes tendances ainsi que les variations saisonnières.\n\nIllustration 10. Variations du graphique de l’illustration 9 avec différents types de lignes\n\nIl est à noter que la dernière itération, avec les mois colorisés, permet de mieux suivre l’évolution d’un même mois d’une année sur l’autre.\n\nEn dernier type de visualisation classique, nous trouvons les camemberts. En variation des camemberts, nous retrouvons :\n\n– les camemberts en secteurs\n\n– les « donuts » avec un trou au milieu ;\n\n– ou plus récemment les pyramides ou les entonnoirs.\n\nIllustration 11. Graphiques en camemberts\n\nL’utilisation des camemberts est moins évidente, car elle introduit des biais d’optique et, lorsque le nombre de catégories augmente, ces représentations sont moins lisibles. Beaucoup de praticiens lui préfèrent le graphique en barres. Je vous laisse juger de la pertinence de l’un ou de l’autre.\n\nIllustration 12a. Graphique en barres : ventes de véhicules hybrides et électriques aux États-Unis en 202010\n\nIllustration 12b. Graphique en camembert : ventes de véhicules hybrides et électriques aux États-Unis en 202010\n\n» Quel graphique trouvez-vous le plus lisible ?\n\nLes visualisations avancées\n\nLes visualisations avancées sont l’évolution naturelle des graphi­ques tels que nous les avons connus sous tableur et outils de Business Intelligence. Leur développement s’est fait en parallèle avec l’avènement des technologies Big Data afin de représenter de façon concise des situations complexes et de gros volumes de data. Les capacités des cartes graphiques et des moniteurs, le développement de bibliothèques de graphiques et de langages tels que le Python ont permis leur démocratisation.\n\n• Une infinité de possibilités\n\nOn peut accéder à de nombreux graphiques avancés en standard sur les logiciels d’analyse.\n\nIllustration 13. Options de graphiques classiques ou avancés dans Tableau, PowerBI et Pyramid Analytics\n\nCertains peuvent être ajoutés via des places de marché ou via l’intégration de bibliothèques comme sur Microsoft PowerBI ou sous langage Python.\n\nIllustration 14. Fenêtre d’accès de PowerBI à des visualisations complémentaires\n\nEn sachant appliquer un peu de code simple, et sur la base de données bien préparées (nous verrons qu’un simple fichier CSV peut suffire), il existe de nombreuses bibliothèques accessibles en ligne.\n\nIllustration 15. Galerie de la bibliothèque D311 (accessible avec du Javascript)\n\nIllustration 16. Extraits de la galerie de la bibliothèque Matplotlib (accessible avec du Python)\n\nLe choix pléthorique de formats ne doit pas faire oublier les objectifs de ces graphiques, sous peine de tomber dans les mêmes pièges qu’avec les reportings gadgets qui deviennent plus des passe-temps. Voici quatre exemples d’applications durables possibles pour des travaux d’analyse.\n\n• Le graphique en bulles\n\nPour une analyse multicritère visant à se concentrer sur la recher­che de situations atypi­ques (outliers) ou de corrélation, le graphique en bulles est très performant. Il permet, sur les deux dimensions d’un papier ou d’un écran, de visualiser simultanément cinq aspects d’une data, voire six ou sept s’il est possible de faire bouger le graphique sur l’écran.\n\nCe type de graphique fait apparaître quatre métriques ou attributs de la donnée :\n\n– sur l’axe X, l’abscisse ;\n\n– sur l’axe Y, l’ordonnée ;\n\n– selon la taille de la bulle ;\n\n– selon la couleur de la bulle.\n\nUne cinquième facette de la donnée peut apparaître dans la forme de la bulle : carré, triangle, croix, losange, etc. Une sixième peut être ajoutée sous la forme d’un curseur faisant varier par exemple la date. Une septième pourra même être un troisième axe dans le graphe.\n\nIllustration 17. Exemple de graphique en bulles : espérance de vie et PNB par pays et par continent\n\nLe diagramme de l’illustration 17 reprend un travail de Hans Rosling12. La magie de ces graphiques est que, même si les détails sont petits, on visualise immédiatement :\n\n– une corrélation intéressante entre PNB et espérance de vie ;\n\n– les grands pays (Chine, Inde, États-Unis, Brésil) et le peloton des grands pays industrialisés d’Europe ;\n\n– l’existence d’exceptions avec des bulles hors de la tendance qui pourraient être des champs immédiats d’investigation. Quelques bulles en bas du groupe posent question.\n\n• Le diagramme en cordes (chord diagram)\n\nCette représentation permet de visualiser des relations entre zones, entités ou personnes. La taille de la corde représente la grandeur de la mesure, sa couleur le sens ou l’origine du flux et les arcs de cercle en périphérie leur source et leur origine.\n\nFigure 6. Exemple de diagramme en cordes : trajets de courses de taxi dans une ville\n\nDans la figure 6, les quartiers de départ et d’arrivée forment le cercle. Les cordes représentent les trajets tandis que leur épaisseur représente leur nombre. Combien de secondes nous faut-il pour identifier les trois voire quatre courses les plus fréquentes ? Voilà un exemple de visualisation qui est rapidement claire et compréhensible, même si elle peut être déconcertante de prime abord.\n\n• Le diagramme Sankey (Sankey diagram)"
        }
      },
      {
        "json": {
          "pageId": "PAGE050_1757157753082",
          "text": "Ce diagramme représente à merveille des flux dans un processus à plusieurs étapes. En partant de la gauche, il montre comment des flux entrants se répartissent au cours de leur traitement jusqu’à leur destination finale.\n\nFigure 7. Exemple de diagramme Sankey : emplois/ressources d’une ville\n\nLes applications en finance, en logistique et même en RH ouvrent la voie à de nouvelles façons de représenter des situations complexes de flux. Elles peuvent rendre palpables des constats et les exposer au plus grand nombre, pour des prises de conscience et des actions !\n\nLes infographies et autres publications sur mesure\n\nPourquoi dépenser de l’argent et du temps pour livrer des représentations raffinées de nos analytiques, alors que nous venons de dire qu’il était primordial de nous concentrer sur leur impact ? Les plaquettes et autres illustrations sont ici destinées à une population très large ou d’une grande diversité de sensibilité, de perception ou d’objectifs, qui ne dispose pas de beaucoup de temps pour des recherches et a besoin d’un média commun afin de se faire une opinion ou de suivre une recommandation. Souvent, ces publications sont rendues obligatoires par une loi, une réglementation ou une habitude. On trouvera par exemple des plaquettes annuelles, des rapports pour des conseils d’administration ou de larges campagnes d’information.\n\nLa production de ces documents est souvent plus coûteuse en design (et en papier). Le retour sur investissement ne se justifie plus par l’impact des actions qui vont être prises, mais par le nombre de personnes touchées ou par le format qualitatif rassurant qu’ils vont livrer à des groupes. Nous passons ici du côté reporting au côté promotion et communication.\n\nLe mémo\n\nÉlément paradoxal d’un bon reporting : il devient inutile dès qu’il est publié. En effet, si les points présentés sont clairs et étayés, son lecteur doit immédiatement passer à la phase suivante de réflexion et d’action. Selon moi, un bon reporting s’efface du paysage dès qu’il est lu.\n\nLes rapports qui nécessitent de longues revues et des manipulations de filtre sans fin montrent que :\n\n– soit l’analyse n’est pas finie et qu’il faut continuer à creuser, auquel cas ce n’est pas un bon rapport ;\n\n– soit son lecteur est ou se prend toujours pour un analyste, ou il n’a pas confiance en ses analystes.\n\nDans les deux cas de figure, le « mauvais rapport » illustre souvent un problème plus profond de procédure, de compétence ou de politique.\n\nPar conséquent, pourquoi ne pas se contenter d’un bref mémo listant les conclusions de l’analyse de la semaine, illustré en annexe par quelques points de suivi généraux et surtout des zooms sur les points analysés en détail ?\n\nLe mémo de Jeff\n\nLe fondateur et P.-D.G. Jeff Bezos révèle que les dirigeants de l’entreprise « ne font pas de PowerPoint » ou toute autre présentation sous forme de diapositives. Au lieu de cela, ils « créent des notes narratives de six pages qui sont lues au début de chaque réunion, un peu comme une session de “salle d’étude” ».  \nSelon Bezos, « la raison pour laquelle écrire un “bon” mémo de quatre pages est plus difficile que “rédiger” un PowerPoint de vingt pages est que la structure narrative d’un bon mémo oblige à mieux réfléchir et à mieux comprendre ce qui est plus important que quoi. \\[…\\] Souvent, lorsqu’un mémo n’est pas génial, ce n’est pas l’incapacité de l’écrivain à reconnaître le niveau élevé, mais plutôt une mauvaise attente sur la portée13 ».\n\nChoisissez votre légende ! La distribution du mémo de reporting : les chiffres ne sont pas bons. Il n’y a plus de papier dans l’imprimante pour sortir le rapport. Excel a buggé et ils ne vont pas finir dans le temps."
        }
      },
      {
        "json": {
          "pageId": "PAGE051_1757157753082",
          "text": "## Pour conclure\n\nContrairement aux apparences, aux croyances et au marketing, les solutions décisionnelles ne se limitent pas à un outil d’analyse. Un processus d’analyse doit couvrir une chaîne qui va de la collecte de l’information brute jusqu’à la livraison d’une conclusion. Le data pionnier devra toujours avoir à l’esprit que c’est l’alignement de solutions sur l’ensemble de cette chaîne qui fera la performance et la qualité de ses analyses (voir chapitre 7, L’optimisation des neuf étapes d’un processus analytique)."
        }
      },
      {
        "json": {
          "pageId": "PAGE052_1757157753082",
          "text": "# Comment passer des solutions transactionnelles aux solutions décisionnelles ?\n\nLa compréhension des solutions décisionnelles, comme nous venons de le voir, est aisée. Pourquoi les analystes rechignent-ils à quitter leurs processus manuels pour embrasser leur puissance et leur confort ? Plusieurs facteurs organisationnels et humains freinent ces évolutions."
        }
      },
      {
        "json": {
          "pageId": "PAGE053_1757157753082",
          "text": "## L’accès à l’équipement\n\nSi la technologie n’est pas une condition sine qua non pour le traitement de données, elle devient rapidement cruciale pour des traitements rapides et fiables, FAIR et fluides (voir chapitre 3, Collecter une donnée pour une utilisation efficace, précise et durable). Nos premiers pas vers une analytique maîtrisée et durable peuvent se faire avec des outils bureautiques classiques, mais nos progrès seront rapidement conditionnés par l’accès à des solutions dédiées d’analytique. Cette étape est souvent douloureuse car, paradoxalement, l’accès à des outils qui pourraient rendre le travail des métiers plus performant n’est pas toujours possible, et ce, pour différentes raisons :\n\n– le budget de licence est insuffisant et tout le monde ne peut pas être équipé. Aujourd’hui, bon nombre de solutions reviennent moins cher qu’un abonnement Netflix et le seuil de retour sur investissement est extrêmement bas. L’argument financier est moins évident qu’il y a une dizaine d’années, surtout ramené aux heures économisées par chacun chaque semaine. Néanmoins, les objectifs de développer une culture data chez les analystes ne sont pas toujours accompagnés de leur équipement ;\n\n– les services techniques choisissent un outil trop technique et peu attrayant pour les utilisateurs finaux. Dans un souci d’homogénéisation des solutions, de la formation et du support, les groupes établissent parfois des standards qui, involontairement, conduisent à ces situations (et à la précédente également). La logique de cette approche est parfaitement défendable, mais, dans notre monde digital émergent, de nouveaux éléments prô­nent des outils plus accessibles et pas forcément uniques pour des populations variées ;\n\n– la bataille de l’analytique se gagnant dans la question et la préparation des données, bien avant l’analyse elle-même, l’outil analytique n’est que la partie émergée de l’iceberg. Il ne compte pas beaucoup dans la pertinence et la qualité de l’analyse. Ce qui est clé, c’est la question métier et la data collectée pour y répondre : la manière de l’agréger, de l’analyser et de la visualiser est secondaire, tant qu’elle demeure efficace ;\n\n– les utilisateurs finaux, habitués à des interfaces propres, légères, rapides, vont moins adhérer à des environnements plus techniques. En fonction de leur sensibilité, ils vont d’ailleurs être attirés par des solutions analytiques différentes ;\n\n– alors que les équipes diverses montent en compétence, leurs préférences et leur besoin évoluent, et sont moins susceptibles de se conformer à un seul moule de logiciel analytique ;\n\n– l’autonomie des utilisateurs est souhaitée mais pas voulue. La perte de contrôle et de pouvoir (et de magie) liée à la démystification et l’adoption d’outils entraînent de nouveaux rapports de force ou de synergie auxquels toutes les équipes ne sont pas prêtes.\n\nÀ charge aux métiers de montrer leur appétit à apprendre, leur volonté de progresser, afin de ne pas prêter le flanc à la critique et de se voir refuser ces outils pour les raisons ci-dessus."
        }
      },
      {
        "json": {
          "pageId": "PAGE054_1757157753082",
          "text": "## Le frein organisationnel\n\nLa puissance des outils actuels et leur capacité à croiser et à visualiser rapidement de gros volumes de données modifient les rapports de force et conduisent à repenser les priorités et les objectifs de chacun. Les métiers, nouvellement équipés, se retrouvent pris en tenaille entre des équipes data qui « perdent » une partie de leur rôle d’appui tactique (qu’elles n’arrivaient pas à tenir de toute façon compte tenu souvent du nombre des demandes) et les équipes informatiques qui voient diminuer les activités de production d’analyse et de rapport (qu’elles n’arrivaient pas à tenir dans tous les cas aussi).\n\nLe statu quo fonctionnait car, jusqu’à présent, il y avait un vrai déficit de compétence data dans les métiers et ces équipes data et techniques ne faisaient que pallier ce manque. Aujourd’hui, les équipes opérationnelles, grâce aux data pionniers, peuvent prendre en charge les rôles d’analytique terrain pour laquelle elles sont les mieux placées.\n\nMoins qu’une perte de pouvoir pour les équipes data et technique, considérons cette tendance comme une réelle opportunité pour ces dernières de se concentrer sur les vrais défis de demain, tels que la cybersécurité, la mise en place d’infrastructures ultraperformantes, la résilience d’un réseau de plus en plus partagé entre le bureau et le reste du monde (domicile, restaurant, transports), la gouvernance pour bâtir une donnée homogène, connectable et qui respecte l’éthique. Les équipes métier vont passer par une courbe de changement forte pour monter en compétence data : le même effort devra être entrepris par les groupes support si l’on veut casser le statu quo."
        }
      },
      {
        "json": {
          "pageId": "PAGE055_1757157753082",
          "text": "## Le frein humain\n\nNous allons toutefois retrouver des obstacles à cette migration technologique dans la hiérarchie, dans nos équipes et en nous.\n\nL’attitude du management est souvent directement liée à la faible adoption de nouvelles solutions et de processus appuyés sur les technologies décisionnelles. Plusieurs facteurs sont en cause :\n\n– la méconnaissance, et de fait une certaine méfiance, du domaine. Trop souvent « brûlé » par des projets data ratés ou des investissements sans débouchés, le management ne voit pas toujours d’un bon œil l’adoption de ces approches analytiques qu’il maîtrise mal ;\n\n– la crainte de perte d’autorité. Le paradigme du manager « sachant » est encore bien présent : « Je suis le dirigeant, donc je sais. » Donner plus de pouvoir d’analyse à une équipe change cet équilibre (voir supra, Le reporting) ;\n\n– la persistance des demandes de rapports manuels, ultracosmétisés et souvent sur Excel ;\n\nOui mais dans l’avion…\n\nUn responsable préférerait un rapport tableur prenant une heure de préparation à un document automatisé en ligne en donnant l’argument suivant : « Oui, mais si je dois prendre l’avion et que je n’ai pas de connexion, je fais comment ? » Pendant des années, cette posture a bloqué toute possibilité d’évolution du processus et même de réflexion sur les options possibles pour son optimisation.\n\n– la reconnaissance et la récompense de pratiques obsolètes n’incitent pas au risque d’essayer de nouveaux outils en analytique. Qui récolte les lauriers et les accolades le plus facilement ? L’au­teur d’un rapport magnifique, raffiné, obtenu de haute lutte par la maîtrise des dernières suites bureautiques, ou l’humble analyste qui se concentre sur l’impact de ces analyses dans des processus itératifs (test, échec, apprentissage, progrès) ? L’équipe qui ostensiblement travaille nuit et jour pour finir ses rapports, parce qu’entravée dans des processus manuels et fragiles, ou bien les groupes se reposant sur des processus lean qui peuvent se permettre de prendre du temps pour réfléchir et pour progresser ?\n\nNous devrions être rémunérés en partie sur la durabilité de nos processus analytiques. Bâtir des usines à gaz qui ne vivent que le temps de notre poste n’est pas le meilleur investissement de notre salaire pour une organisation, si tout s’évanouit à notre départ.\n\nLe bonus durable\n\nN’oublions jamais de regarder notre courrier, peut-être que, des années après, un employeur nous remerciera avec un chèque envoyé par la poste pour tous les processus durables que nous aurons mis en place. À l’heure où la transformation est le maître mot, l’évolution des méthodes de management autour de l’analytique et du reporting ne devrait pas être oubliée.\n\nNos coéquipiers ne seront pas toujours sources de support pour plusieurs raisons :\n\n– nous bouleversons une situation qui, sans qu’elle soit satisfaisante, avait au moins le mérite d’être enracinée dans une routine ;\n\n– nous risquons d’exposer au grand jour des pratiques particulièrement inefficaces qui jusqu’à présent étaient considérées comme l’état de l’art, faute de point de référence ;\n\n– nous rasons certains bidouillages analytiques qui avaient fait la réputation de leur concepteur et nous proposons à leur place des solutions simples, transparentes, accessibles et partagées.\n\nEnfin nous sommes parfois notre propre ennemi. Les raisons de ne pas faire pourraient être listées dans un ouvrage à part entière. Voici quelques réflexes ou habitudes que nous allons devoir traiter pour progresser :\n\n– la paresse ou le manque de courage : nous sommes tous un peu paresseux. Se rajouter du travail et une courbe de d’apprentissage dans des journées déjà bien remplies n’est pas un engagement anodin ;\n\n– le choix de carrière, qui va avoir un impact à deux niveaux : choisir de faire comme tout le monde et se conforter finalement dans une analytique manuelle, ennuyeuse, mais reconnue ; parfois choisir des solutions sur la base de la renommée de leur éditeur est l’assurance de se voir reconnaître une compétence dans les positions futures.\n\nSe former, se rapprocher de collègues embarqués dans la même aventure, se faire appuyer par un mentor et identifier un manager qui apportera le soutien nécessaire pendant les phases de tests et d’échecs, tout cela va permettre de passer cette phase.\n\nYes we can !\n\nJ’enseigne la data depuis 2008 sur des programmes de formation continue comme ceux de Stanford, Berkeley, sur des programmes exécutifs, sur des bachelors et masters, ainsi que dans des programmes d’inclusion et de retour à l’emploi par l’apprentissage de la data et de l’analytique. J’ai formé des milliers d’élèves de tous âges, expériences et horizons socio-économiques : s’ils l’ont fait, vous le pouvez !"
        }
      },
      {
        "json": {
          "pageId": "PAGE056_1757157753082",
          "text": "# Pour conclure\n\nLe schéma de pensée décrit dans ce chapitre va nous aider à mieux comprendre, reconnaître et ensuite positionner mentalement les solutions autour de nous dans leur contexte. Cette approche, simple mais non simpliste, permet de décoder les nouvelles solutions et les gadgets qui font constamment irruption dans notre monde. En utilisant des termes plus précis, sans tomber dans le jargon, nous allons pouvoir mieux décrire nos besoins et savoir où et comment bénéficier de la technologie.\n\nHumainement, nous serons plus respectés dans nos échanges avec l’informatique en n’étant plus des innocents aux mains pleines, mais des partenaires de réflexion et de coconstruction.\n\n------------------------------------------------------------------------\n\n5. Charpin D., Hammu-Rabi de Babylone, PUF, 2003.\n\n6. Voir McCue I., Qu’est-ce qu’un ERP (Enterprise Resource Planing), [www.netsuite.com](http://www.netsuite.com), 2016.\n\n7. BM Journal of Research and Development, II (4), octobre 1958.\n\n8. Clark J., Google retourne vers le futur avec la base de données SQL F1, The register, 30 août 2013.\n\n9. Source des données : [https://open-numbers.github.io/datasets.html](https://open-numbers.github.io/datasets.html)\n\n10. Source des données : kaggle.com\n\n11. [https://d3js.org/](https://d3js.org/)\n\n12. Hans Rosling est un statisticien suédois, décédé en 2017. Son livre Factfulness, publié à titre posthume et coécrit avec Anna Rosling Rönnlund et Ola Rosling, est devenu un best-seller international.\n\n13. Umoh R., Pourquoi Jeff Bezos oblige les dirigeants d’Amazon à lire des mémos de 6 pages au début de chaque réunion, cnbc.com, 23 avril 2018.\n\nCHAPITRE 5  \nMaximiser le potentiel de la donnée\n\nLes 152 000 lignes ne tiennent pas dans le tableur !\n\nMaintenant que nous avons acquis les principes de base de la donnée et que nous avons pris connaissance des boîtes à outils mises à notre disposition, nous sommes parés pour commencer à travailler avec cette matière première qu’est la data.\n\nNous procéderons en plusieurs étapes :\n\n– démystifier les différents types de base de données. Ce sont les containers de nos données, nous devons connaître les options à notre disposition ;\n\n– structurer et organiser sa data. 80 % du travail de l’analyste est la préparation de la donnée ;\n\n– considérer les approches plus avancées pour tirer le maximum de nos données le plus efficacement possible ;\n\n– appréhender le Big Data avec confiance."
        }
      },
      {
        "json": {
          "pageId": "PAGE057_1757157753082",
          "text": "# Les types de bases de données\n\nLa confusion ambiante, savamment entretenue par certains éditeurs ou informaticiens, ne nous aide pas à cerner ces technologies pourtant centrales. Une fois leur rôle fondamental et trivial de  \nstockage de données compris, nous pouvons nous concentrer sur les raisons pour lesquelles il existe différents choix et options technologiques.\n\nLes différentes bases de données ont toute leur raison d’être, bien qu’aucune ne soit la panacée pour toutes les applications du décisionnel. À l’image d’une voiture, si une petite hybride, une grosse berline, un SUV, une Formule 1 ou un van peuvent tous transporter passager(s) et bagage(s), leur format n’est pas adapté à tous les contextes, toutes les routes, tous les garages et tous les conducteurs. Pour mieux comprendre ces technologies clés pour nos analyses, nous pouvons considérer les bases de données en six groupes, de la plus simple à la plus complexe."
        }
      },
      {
        "json": {
          "pageId": "PAGE058_1757157753082",
          "text": "## Les fichiers plats, texte ou CSV\n\nÉvoqués dans le chapitre précédent, ils représentent la forme la plus simple de stockage d’information. Ils sont le plus souvent sous forme de lignes et de colonnes délimitées par des éléments de texte tels que la virgule, le point-virgule, la tabulation, etc. Il existe d’autres formats, tels que le JSON (JavaScript Object Notation), qui permettent de stocker les données de manière légèrement plus lisible pour l’humain, mais qui ne sont pas toujours lus avec facilité par les outils du marché.\n\nIllustration 18. Extrait d’un fichier JSON\n\nNous avons vu que les fichiers texte sont des médias de choix en raison de leur simplicité et de leur universalité. Néanmoins, ils présentent rapidement des limitations quand il s’agit de :\n\n– définir des relations entre différents fichiers. Ces relations, appelées jointures (voir infra, Connecter la data pour une analytique plus riche et pertinente) sont toujours possibles, mais elles ne peu­vent pas être vraiment gouvernées et encadrées par des règles communes à tous les analystes. Chaque utilisateur doit bâtir ses jointures à chaque fois ;\n\n– contrôler les formats des colonnes. La qualité de la donnée et l’homogénéité des formats au sein d’une même colonne ne peuvent pas être garanties à l’avance ni forcées. Le fichier texte stockera ce qu’on lui injecte sans aucune forme de contrôle. On pourra ainsi voir des dates, des nombres ou des textes cohabiter dans une colonne, rendant délicate voire impossible l’analyse ;\n\n– gouverner des fichiers plats localisés sur de nombreuses machines, dans des dossiers différents, n’est pas simple. Il revient à chaque analyste de gérer ses fichiers sans vraiment de coordination globale possible, en évitant les effacements par erreur.\n\nLes bases de données relationnelles vont pallier ces manques. Avec l’encadrement plus strict des règles et des normes de stockage, elles vont perdre un peu de l’agilité des fichiers plats. Elles ont aussi un coût, si ce n’est en licence, au moins en temps et en effort de maintenance.\n\nFigure 8. Représentation des bases de données texte dans les diagrammes de flux des données"
        }
      },
      {
        "json": {
          "pageId": "PAGE059_1757157753082",
          "text": "## Les bases de données relationnelles\n\nLes bases de données relationnelles peuvent être imaginées comme un ensemble de tables, similaires à des fichiers plats mis dans un environnement dédié. Cet environnement apporte de nombreux avantages tels que :\n\n– une formalisation des formats requis. Nous allons pouvoir définir et forcer des formats spécifiques comme des dates, des types de nombre ou des longueurs de texte ;\n\n– une formalisation des liens entre les tables et des modèles qu’elles forment. Cela nous permettra de ne pas avoir à reconstruire des liens entre les tables à chaque nouvelle analyse (voir infra, Connecter la data pour une analytique plus riche et pertinente) ;\n\n– des procédures de contrôle d’accès et de sauvegarde ;\n\n– les contraintes ACID qui permettent d’assurer l’intégrité de toutes les transactions saisies, modifiées ou effacées dans le système :\n\n• l’atomicité : dans une transaction impliquant deux ou plusieurs éléments d’information distincts, soit tous les éléments sont engagés, soit aucun ne l’est ;\n\n• la cohérence : une transaction crée un nouvel état de données valide ou, en cas d’échec, ramène toutes les données à l’état où elles se trouvaient avant le début de la transaction ;\n\n• l’isolement : une transaction en cours et non encore terminée doit rester isolée de toute autre transaction ;\n\n• la durabilité : les données engagées sont sauvegardées par le système de telle sorte que, même en cas de panne et de redémarrage du système, les données sont disponibles dans leur état correct.\n\n– un langage de requête et de calcul dédié, pour effectuer des traitements sur un serveur puissant et non plus seulement sur notre machine.\n\nCes bases relationnelles sont aussi parfois appelées base SQL14 (Structured Query Language), parce que leur langage de gestion est dans la grande majorité des cas le SQL. Leur intérêt en décisionnel est que leur structure en tables est naturellement compréhensible. La logique de connecter des tables entre elles est facilement visualisable. Le langage SQL est aussi accessible car lisible simplement en anglais.\n\nCes bases de données ont toutefois quelques limites :\n\n– leur environnement très structuré peut ralentir les développements itératifs ;\n\n– leur structure tabulaire n’est pas efficace pour travailler sur des calculs en ligne. Autant, en colonne, il est possible d’effectuer des traitements et de stocker les résultats dans une nouvelle colonne, autant réaliser des opérations sur des lignes est délicat : où stocke-t-on les résultats ? Une nouvelle ligne serait considérée comme un nouvel enregistrement et non pas comme une donnée spécifique calculée, et la table perdrait sa consistance ;\n\n– leur moteur de calcul n’est pas toujours approprié pour appliquer des formules au travers de tout un modèle ;\n\n– elles deviennent moins performantes et compliquées à gérer lorsque le nombre de colonnes augmente ;\n\n– enfin, elles ne sont pas adaptées au stockage de données non structurées.\n\nC’est pour cela que d’autres formats de base de données ont été développés.\n\nFigure 9. Représentation des bases de données relationnelles dans les diagrammes de processus analytiques\n\nLe siège d’Oracle à Redwood Shores en Silicon Valley\n\nUne claire référence à la base de données relationnelle qui a fait la fortune de la société."
        }
      },
      {
        "json": {
          "pageId": "PAGE060_1757157753082",
          "text": "## Les cubes de données\n\nAussi appelées bases OLAP (On Line Analytical Processing), hypercubes ou bases « multidimensionnelles », ces technologies permettent de résoudre au moins deux limitations des bases de données relationnelles :\n\n– la possibilité de faire pivoter les lignes en colonnes et vice versa, de manière à pouvoir appliquer des calculs sur toutes les dimensions ;\n\n– l’application de calculs à tous les niveaux d’une hiérarchie ou à certains membres sélectionnés.\n\nCes options en font des outils remarquablement adaptés à l’analytique financière ou au contrôle de gestion. Néanmoins, elles ne sont pas l’arme absolue. Elles trouvent leur limite en capacité de stockage comparée aux bases relationnelles et en termes de nombre d’attributs. Plus on augmente leur nombre, plus le nombre de cellules à créer pour chaque autre dimension du cube grandit exponentiellement. Elles sont aussi plus coûteuses à l’achat et à la maintenance. Elles requièrent des qualifications techniques plus rares et donc plus onéreuses que celles requises pour les bases de données relationnelles.\n\n→ Tu prendras du OLAP, mon fils\n\nJ’ai été confronté à une demande, ou plutôt à une injonction de passer notre analytique sur une base OLAP il y a quelques années. Je connaissais très bien la technologie demandée et j’avais vu avec quelle habileté l’équipe commerciale du fournisseur avait convaincu l’équipe dirigeante de la pertinence de ce choix (les repas, le golf et les conférences faisaient partie des arguments !). La solution était bonne, ultraperformante et semblait être une évidence, même si nos besoins analytiques se satisfaisaient très bien des solutions en place qui étaient loin d’être à leur limite.\n\nLe point vendeur de mon patron était que la solution n’était pas si chère. Effectivement, le prix du logiciel était raisonnable et nous pouvions imaginer un retour sur investissement rapide. Néanmoins, en ajoutant le coût des serveurs et de l’implémentation, le budget faisait plus que doubler. En additionnant les frais de formation de l’équipe, la nécessité de recruter un responsable pour le contrôle de la solution et également les charges du consultant côté informatique pour une maintenance à mi-temps, nous avions triplé la dépense et créé une dépense récurrence conséquente.\n\nLes bases OLAP sont par conséquent un excellent complément des bases relationnelles. Ces dernières stockent massivement les informations avec de grands niveaux de granularité et de détail, et peuvent exporter une sélection de données pertinentes sous forme de cubes des sélections de données pour des analyses spécifiques.\n\nComment choisir entre base de données relationnelle et base de données multidimensionnelle ?\n\nOLAP et « multidimensionnel » sont devenus des termes standards pour qualifier ces technologies en cube. Ce ne serait pas un problème si ces mots ne biaisaient pas les choix de base de données en amenant à ne penser qu’à ces options lorsque nous voulons faire des analyses suivant différents angles ou agrégats. Les bases relationnelles peuvent faire des analyses multidimensionnelles par produits, clients, pays etc. Elles peuvent gérer des hiérarchies. Et avec les outils d’analyse d’aujourd’hui, elles peuvent être représentées par des tableaux croisés dynamiques et approcher l’agilité de cubes. Par ailleurs, leur interrogation peut être faite… en ligne, online. Une base de données relationnelle peut donc être aussi OLAP et multidimensionnelle. Attention à choisir les cubes de données pour les bonnes raisons."
        }
      },
      {
        "json": {
          "pageId": "PAGE061_1757157753082",
          "text": "## Les bases de données en colonnes\n\nAvec l’émergence des réseaux sociaux, la création massive d’informations individuelles lors d’une navigation internet ou d’expérience en ligne, les volumes de données ont non seulement explosé, mais leur nombre d’attributs (de colonnes) aussi. De plus, ces attributs ont commencé à devenir de plus en plus variés pour capter les facettes des individus ou de leur comportement, sous la pression des analystes toujours plus gourmands en données pour connaître leurs clients ou leur marché. Les bases de données relationnelles (tout comme les cubes) n’offrent pas cette flexibilité d’ajout massif de colonnes et perdent rapidement en performance lors des phases d’analyse sur des tables larges.\n\nLes bases de données en colonnes (columnar database) ont permis de répondre à ce défi en stockant chaque colonne indépendamment des autres. Chaque valeur de colonne est bien sûr attachée à une clé qui permet de les relier à leur membre. L’intérêt de cette approche est que, pour l’analyse d’une caractéristique parmi des centaines, il suffit de prendre la colonne qui la contient et, indépendamment des autres, d’effectuer les opérations voulues. La machine n’a pas à lire toute une ligne d’informations non pertinentes pour récupérer cette donnée et, de ce fait, le temps d’accès est largement réduit.\n\nLà encore, ces technologies ne sont pas le Graal de l’analytique : elles sont plus des techniques à gérer, elles n’offrent pas la performance des bases de données relationnelles en écriture/effacement de nouveaux enregistrements (car il faut écrire dans autant de fichiers qu’il y a de colonnes). Elles sont également moins facilement prises en main pour des utilisateurs métier et généralement plus onéreuses en licence et en maintenance."
        }
      },
      {
        "json": {
          "pageId": "PAGE062_1757157753082",
          "text": "## Les bases de données NoSQL (Not Only SQL)\n\nNous entrons maintenant dans le Far West des bases de données. Ces bases de données NoSQL ont été créées pour plusieurs raisons à l’origine :\n\n– stocker toutes les nouvelles données que le monde (organisations, humains et machines) a commencé à générer grâce à la révolution numérique (texte, images, vidéo, voix, etc.) ;\n\n– stocker sans limite grâce à leur capacité de distribuer leur stockage sur plusieurs machines et disques ;\n\n– calculer de manière massive en distribuant la charge sur plusieurs serveurs et processeurs ;\n\n– s’affranchir des grands éditeurs avec des technologies Open Source15 ;\n\n– se libérer de contraintes de gestion pour aller au plus vite dans la capture de nouvelles informations.\n\nLeur avantage est certain dans un contexte où la boulimie des entreprises pour la donnée explose. Ces technologies ne sont néanmoins pas l’arme absolue. Leur technicité et leur complexité sont un frein pour beaucoup d’équipes, surtout dans les métiers. L’accès direct par des analystes n’est pas réaliste. Ce sont potentiellement d’excellents réservoirs de données brutes, mais leur aptitude à livrer une donnée préparée et adaptée à l’analyse est limitée."
        }
      },
      {
        "json": {
          "pageId": "PAGE063_1757157753082",
          "text": "## Les bases de données décisionnelles virtualisées\n\nCe mode de présentation de données pour l’analytique s’est progressivement démocratisé au début des années 2000.\n\nCette approche permet de ne pas forcément stocker toute la donnée et de ne traiter que la donnée qui est demandée pour l’analyse. Elle crée une vue utilisateur similaire à un modèle de données d’analyse (voir infra, Les options pour travailler la donnée : à la volée ou stockée) et la solution se charge de gérer les liens, les requêtes et les traitements de données en tâche de fond.\n\nL’avantage de ces solutions est qu’avec la virtualisation de la base les coûts de stockage diminuent. La modification d’une logique de présentation ou de préparation de données est également plus agile, car il suffit de changer le code sous-jacent et non pas la structure entière d’un modèle.\n\nL’agilité peut avoir une conséquence sur la performance des analyses, car chaque requête d’un analyste nécessite de traiter toute la chaîne virtuelle. On pourra donc arbitrer les éléments à virtualiser ou à implémenter physiquement pour choisir où l’agilité ou la performance sont le plus critiques. Même si ces choix sont techniques, le data pionnier ne devrait pas hésiter à faire part de ses contraintes opérationnelles pour optimiser au mieux la chaîne analytique."
        }
      },
      {
        "json": {
          "pageId": "PAGE064_1757157753082",
          "text": "## Pour conclure\n\nNous voyons que, même dans un domaine relativement technique, le pragmatisme métier reste fondamental. Beaucoup de choix s’offrent à nous et souvent nous sont imposés. Comprendre les technologies qui sous-tendent nos sources de données est important puisqu’elles conditionnent l’agilité de nos analyses. Sans entrer dans les détails, cette hiérarchisation des fonctionnalités va permettre de mieux utiliser le potentiel de nos bases de données, voire d’en influencer le choix.\n\n→ Tu prendras Hadoop, mon fils\n\nTout au long de ma carrière, mes équipes data se sont souvent vu imposer des choix de bases de données. Était-ce notre rôle de gérer la sélection et de faire un choix final ? Probablement pas. Néanmoins, ces choix ont considérablement impacté notre performance et notre agilité d’analyse. Rétrospectivement, je pense que ce sont les besoins métier qui devraient primer. Imposer des choix techniquement bons mais sous-optimaux pour les analystes, ne serait-ce pas voir le problème à l’envers ?\n\nLors de mon passage dans un grand groupe de l’Internet, en tant que responsable de la donnée financière, mon équipe d’analystes s’était vu imposer une base de données Open Source NoSQL : Hadoop. J’étais moi-même assez intrigué et enthousiaste à l’idée de me former à ces nouvelles solutions. J’ai malheureusement rapidement déchanté et je suis allé de déconvenue en déconvenue. Non pas que la technologie ne fût pas bonne ou que les équipes techniques ne la maîtrisent pas, mais parce qu’une série d’inquiétudes et de déceptions se sont imposées à mes équipes :\n\n– nous n’avions aucune maîtrise des flux entrants de données. Nous ne pouvions pas ajuster nous-mêmes la sélection des informations et encore moins leur préparation. Nous n’avions pas de visibilité sur la bonne fin des processus de chargement des données. Cette absence de transparence est rapidement devenue pré­judiciable dans le cadre du support des processus d’analyse financière récurrent : quand le chronomètre tourne et que, pour chaque besoin de mise à jour de données, vous devez vous référer à un ingénieur pour déclencher la remontée des données et leur contrôle, vous augmentez votre vulnérabilité ;\n\n– nous étions également fortement dépendants des équipes techniques pour les extractions d’informations. Les outils attachés à la base de données n’avaient rien de simple pour des utilisateurs métier. Nous étions à la merci de la disponibilité d’un technicien pour chaque nouvelle extraction de données ;\n\n– nous verrons plus bas que profilage, contrôle qualité et préparation de données sont le pilier de l’analytique. Or, ces étapes nous étaient désormais inaccessibles au niveau de la base de données et nous étions livrés au mieux à des outils de Business Analytics et au pire à des tableurs, au prix d’une grande perte de temps et potentiellement d’intégrité de données ;\n\n– notre dépendance à l’informatique s’exprima de manière encore plus nette en période de vacances, de longs week-ends ou de vendredi off. Les clôtures financières ne suivent pas les rythmes des loisirs de chacun et nous nous sommes retrouvés sur des processus urgents avec personne vers qui nous tourner pour des mises à jour de données. Notre urgence n’était pas de facto une priorité pour des équipes qui avaient d’autres engagements ;\n\n– pour couronner le tout, les temps de traitement étaient inférieurs à nos systèmes existants. Il s’est avéré que les temps de distribution des processus sur plusieurs machines puis leur récupération prenaient plus de temps que le traitement lui-même, et que le codage des ingénieurs était loin d’être aussi performant que celui de nos solutions, développées par des éditeurs dont c’était le métier depuis des décennies.\n\nNous étions littéralement otages d’une solution que nous ne pouvions pas maîtriser. Nous avions la responsabilité des délais et de la qualité de nos données, sous contrôle d’équipes dont nous n’étions pas la priorité.\n\nEn épilogue : j’avais pris soin de conserver tous les processus intacts et de ne rien basculer avant les tests complets, malgré la pression d’un groupe technique qui ne rêvait que de sortir un éditeur du parc informatique. Mais le non invented here (non inventé ici), qui fait référence au dénigrement de technologie ou de pratiques extérieures, n’a pas tenu. Nous avons conservé nos outils, non sans une certaine amertume : j’espérais vraiment pouvoir trouver de nouvelles opportunités avec ce projet. Mais la performance opérationnelle devait primer (voir note 8)."
        }
      },
      {
        "json": {
          "pageId": "PAGE065_1757157753082",
          "text": "# Organiser la donnée pour une meilleure analytique\n\nUne fois la donnée extraite des sources et stockée dans la base de données décisionnelle, une deuxième phase commence par trois grandes étapes :\n\n– la prise de connaissance du jeu de données récupéré et de sa structure ;\n\n– l’audit de sa qualité et de sa pertinence ;\n\n– l’organisation des données et leur mise en relation avec les données existantes en vue des analyses ultérieures.\n\nEnsuite, nous nous tournerons sur les techniques de préparation et d’enrichissement pour faciliter nos analyses."
        }
      },
      {
        "json": {
          "pageId": "PAGE066_1757157753084",
          "text": "## Les fondamentaux pour comprendre la donnée\n\n\\> Identifier les formats de la data\n\nLa data qui nous entoure peut revêtir des formats aussi variés que des sons, des images, des textes, des odeurs, des sentiments. Quand il s’agit de faire des analyses fluides et de conserver une donnée FAIR (voir chapitre 3, Collecter une donnée pour une utili­sation efficace, précise et durable), toutes ces données devront être numérisées. Elles seront alors codifiées dans quatre grandes familles :\n\n– les chaînes de caractères alphanumériques (String) : ces données peuvent contenir tout type de caractère et des textes entiers, tels que des revues clients ;\n\n– les nombres : ces champs vont pouvoir varier en fonction du type de nombre et pourront distinguer entre autres :\n\n• les entiers (integer), qui pourront être longs ou courts (int16, 32 ou 64) selon la taille des nombres que nous devons stocker ;\n\n• les décimaux (decimal) qui stockent jusqu’à 28 décimales après la virgule et les décimaux doubles qui vont stocker de plus grandes valeurs, mais avec 16 décimales. Les premiers seront à employer pour des calculs plus précis ;\n\n– les Booléens (boolean) vont, eux, indiquer « juste » ou « faux », « oui » ou « non », 1 ou 0 de manière binaire ;\n\n– les dates/heures, qui pourront prendre différents formats selon :\n\n• la région (le mois figure avant les jours dans une data amé­ricaine : le 12 avril 2020 s’exprimera 04/12/2020 au lieu de 12/04/2020 en Europe) ;\n\n• la convention retenue dans la base de données : 12042020, 12/04/2020, 2020-04-12. Malgré leurs expressions différentes, ces valeurs seront considérées comme une même date si leur logique est reconnue par le système qui les gère ;\n\n• la précision souhaitée : l’heure pourra être attachée à la date avec différents niveaux de précision. On ne pourra d’ailleurs retenir que la partie heure d’une date.\n\nConnaître ces formats va s’avérer important pour le data pionnier. En effet, il arrive parfois que la donnée ne soit pas reconnue comme elle devrait l’être ou comme nous le voulons dans nos solutions d’analyse. Nous aurons la possibilité de demander un formatage précis au niveau de la base de données, ou bien, si nous travaillons à partir de fichiers texte, soit de forcer le format dans notre logiciel, soit de modifier la donnée à la source pour qu’elle devienne compatible avec le format souhaité.\n\nVoici quelques exemples classiques que nous rencontrerons souvent, surtout dans le cas de travaux sur des fichiers plats qui n’ont pas été normalisés en étant intégrés dans une base de données :\n\n– le nombre qui est reconnu comme une chaîne de caractères :\n\n• il y a probablement des champs avec des valeurs nulles sous forme d’espace dans la colonne ;\n\n• le séparateur de décimale est une virgule là où le logiciel attend un point ou vice versa (par exemple 183,33) ;\n\n• le séparateur de milliers est un espace, ou une virgule là où on n’en attend pas, ou vice versa (par exemple 1 182 329 ou 976,678,768.67) ;\n\n– la date qui n’est pas reconnue comme une date ;\n\n• le format international avec la position des mois avant les jours, ou vice versa, n’est pas reconnu par la solution d’analyse ;\n\n• les séparateurs jours-mois-année, qui peuvent être des tirets, des points, des barres de fraction, des espaces, ou simplement ne pas exister, sont mal interprétés ;\n\n• l’existence de l’heure à la suite de la date empêche la reconnaissance en tant que date ;\n\n• le nombre de positions pour coder les différents éléments de la date peut être différent de ce qu’attend le logiciel. Par exem­ple, le codage d’un jour ou d’un mois peut être sur un ou deux chiffres, ou bien forcé sur deux chiffres. Le codage d’une année peut être sur deux chiffres ou quatre chiffres. Par exemple, les chaînes suivantes peuvent exprimer les mêmes dates, mais leur format peut conduire à une interprétation comme une chaîne de caractères : 12/04/2021, 04-12/2021, 20210412, 12 Avril 2021, 12/04/21 12h31m34.23s ;\n\n– la chaîne de caractères qui est reconnue comme un nombre :\n\n• souvent des codes d’identification, des clés (voir infra, Jauger la qualité de données initiale : profiler la donnée) peuvent être numériques et vont donc se retrouver à être traités comme des nombres avec l’ajout de décimales ou l’agrégation en somme, ce qui n’a aucun sens et aucune utilité pour les analyses ;\n\n• des dates sans délimiteur telles que 20200412 ou des noms faits de chiffres peuvent également se retrouver qualifiés de nombres et perdre tout leur sens ;\n\n• la valeur booléenne peut être une chaîne (Oui/Non), un nombre (1/0) ou prendre une forme binaire spécifique (Da/Niet, Ok/Not OK, Approuvé/Rejeté, etc.) et elle peut ne pas être interprétée comme telle.\n\nPlusieurs options vont nous permettre de résoudre ce problème :\n\n– forcer les formats dans la base de données ou dans le logiciel d’analyse si les contenus de colonne le permettent ;\n\n– insérer un caractère alphanumérique à la saisie ou dans les traitements pour éviter la qualification de nombre.\n\nNous retrouvons dans l’illustration 19 les options de (re)classement de type de données de Pyramid Analytics, Tableau et PowerBI. Avec des variations sémantiques ou de langue, nous retrouvons sans difficulté les différents types.\n\nIllustration 19. Exemples d’options de choix de format de donnée dans un logiciel de Business Intelligence\n\nLe casse-tête des dates\n\nLes formats de dates sont un véritable problème à gérer lors de l’acquisition de données brutes. Lorsque la donnée est extraite d’une base de données, la normalisation des formats et le classement en type de données date évitent cet écueil, mais bien souvent, lors d’analyses exploratoires, nous n’avons pas le luxe d’avoir des sources de données propres et bien gouvernées.  \nIl va par conséquent falloir procéder à des nettoyages des chaînes que nous voulons interpréter en dates pour que toutes les données aient le même format interprétable. Les techniques classiques sont entre autres :  \n– permuter les mois/jours pour les dates américaines/reste du monde ;  \n– forcer ou non les formats à deux chiffres pour les jours et les mois, et à quatre chiffres pour les années ;  \n– supprimer ou normaliser la partie « heure » du champ « date » ;  \n– remplacer les délimiteurs qui ne sont pas dans les normes.  \nSi cela nous paraît fastidieux, c’est normal. C’est toujours un point de contention dans la préparation de données. Il vaut mieux engager des actions de gouvernance et de sensibilisation proactive de nos fournisseurs de données pour éviter d’avoir à dépenser notre énergie dans ce travail de nettoyage.\n\n\\> Identifier les rôles de la data\n\nSe retrouver face à de nouvelles données est souvent déroutant au départ. Face à une table, à des lignes et à des colonnes, nous ne savons pas où regarder ou par où commencer.\n\nReconnaître les rôles de la data va nous guider pour mieux la comprendre et la jauger.\n\nIl existe de nombreuses variations dans les noms utilisés pour définir ces rôles. Entre les différents jargons et le choix internes d’entreprise, ce qui est important est de reconnaître leurs finalités et les méthodes de contrôle et d’utilisation pour en faire le meilleur usage.\n\nUne table de données est la plupart du temps constituée de quatre grands types de colonnes :\n\n– les colonnes de métriques ou mesures ;\n\n– les colonnes d’attributs ou dimensions ;\n\n– les colonnes de clés, identifiants ou index ;\n\n– les données calculées ajoutées à la donnée d’origine.\n\n» Prenez un de vos fichiers tableurs ou un extrait de données favorites et essayez d’identifier ces différents éléments (voir illustration 20). Avec l’habitude, cela deviendra un réflexe.\n\nIllustration 20. Exemple d’un fichier client d’une société de télécommunication\n\nLes métriques ou mesures\n\nIls sont l’élément quantitatif ou qualitatif au cœur de la question métier. Nous pouvons les additionner, en faire la moyenne, rechercher les minima/maxima. Nous pouvons également compter les occurrences ou compter les éléments distincts de données qualitatives pour les transformer en métriques. Ce sont tout type de nombre : entier, décimal, etc. Leur qualité sera dépendante de leur saisie ou de leur capture et il sera difficile de savoir si une métrique est bonne ou mauvaise intrinsèquement. Un nombre est nombre : nous saurons s’il est bon grâce au profilage et à l’analyse (voir infra, Jauger la qualité de données initiale : profiler la donnée).\n\nLes attributs\n\nIls apportent des informations sur les mesures. Parce que les attributs ajoutent des moyens de regrouper, de trier et d’organiser les données, nous les appelons parfois des dimensions. Même s’il ne s’agit généralement pas de mesures, nous pouvons également les compter ou dénombrer les éléments distincts. Ils peuvent être dans n’importe quel format : numérique, alphanumérique, date, booléen, etc. Leur qualité, à savoir cohérence et homogénéité, sera dépendante de leur format. Ces données appartiennent à une catégorie spéciale : les données maîtres, les Master Data (voir infra, Gérer les données maîtres : le Master Data Management (MDM)).\n\nLes clés\n\nElles permettent de localiser ou d’identifier un enregistrement spécifique. Elles peuvent être utilisées pour connecter des ensembles de données et apporter plus de données à l’analyse en faisant correspondre des lignes entre elles quand elles ont la même clé. Elles peuvent être dans n’importe quel format : numérique, alphanumérique, date. Habituellement, le format le plus court et constant est préférable pour limiter le risque d’erreur. Des clés peuvent aussi être constituées en combinant des colonnes : nous pourrons les obtenir grâce à un champ calculé (voir infra, Créer des clés pertinentes pour de nouvelles jointures). Leur qualité dépendra de la constance de leur format et parfois de leur unicité.\n\nLes données calculées\n\nCe sont de nouvelles données générées à partir de nos données d’origine. Elles peuvent être obtenues par des formules mathématiques, des transformations de texte ou toute autre action que nous souhaitons appliquer à notre donnée initialement capturée. Elles peuvent être exprimées dans n’importe quel format, numérique, alphanumérique, date, booléen, et peuvent ensuite avoir le rôle que nous souhaitons. Leur qualité est dépendante des mesures, des formules, ou des ordres de priorité dans leur exécution.\n\n\\> Connaître ou définir les règles d’agrégation de la data\n\nNous avons vu dans le chapitre précédent que certains rôles de data permettaient certains calculs mais pas d’autres. Revenons en détail sur un dernier point clé : comment nos données vont s’agréger lorsque nous allons les synthétiser ?\n\nTableau 5. Types d’agrégation pertinents pour le jeu de données à partir d’un extrait de la table de l’illustration 20\n\nIllustration 21. Exemples d’options d’agrégation dans un logiciel de Business Intelligence\n\n\\> Pour conclure\n\nTableau 6. Récapitulatif des rôles que peuvent avoir les données\n\nÀ la lumière ces éléments, la table en figure 2 du chapitre 2 va prendre des couleurs. Ces lignes et ces colonnes vont enfin apparaître comme une vraie matière première à affiner et à analyser avec toute une liste d’options de format et d’agrégation que nous devrons valider ou changer.\n\nFigure 10. Organisation d’une table de données\n\nNotre œil va devoir s’habituer à identifier au premier regard ces éléments qui vont tous jouer leur rôle dans les étapes d’analyse suivantes.\n\nL’œil (du tigre) de l’analyste"
        }
      },
      {
        "json": {
          "pageId": "PAGE067_1757157753084",
          "text": "Lorsque nous travaillons aux côtés d’analystes aguerris, regardons les mouvements de leurs yeux à la première lecture d’une table. Avant « d’attaquer la découverte » avec des techniques de profilage (voir infra, Jauger la qualité de données initiale : profiler la donnée), leur regard balaie souvent les colonnes en long et en large, et ils identifient au premier abord les clés, les métriques et les attributs en présence pour avoir une première idée de la pertinence de la donnée qu’ils regardent. Ils vont repérer en quelques secondes des éléments souvent rédhibitoires ou problématiques comme :  \n– l’absence de clé ou leur homogénéité en prévision de comptage ou de lien avec d’autres tables ;  \n– la présence d’attributs avec une très forte hétérogénéité qui risquent de rendre les regroupements et les agrégations délicats ;  \n– la présence de colonnes totalement ou partiellement vides qui vont créer des trous dans les analyses et biaiser les résultats ;  \n– le format des dates et les traitements (pénibles) à venir qui vont nécessiter du temps de préparation ;  \n– l’absence de richesse des attributs et la diversité des métriques qui vont compter pour des analyses pertinentes ;  \n– la présence d’attributs sous forme de code qu’il faudra interpréter et traduire pour les analyses ;  \n– les colonnes booléennes qui vont départager les datas de manière nette (binaire)."
        }
      },
      {
        "json": {
          "pageId": "PAGE068_1757157753084",
          "text": "## Jauger la qualité de données initiale : profiler la donnée\n\nLa compréhension de la structure intrinsèque de la donnée est une première étape. Cette phase d’audit indispensable, surtout lors de la découverte de nouvelles données, qui consiste à passer en revue le jeu de données avant son analyse, est appelée profilage (profiling). Elle va permettre de prendre connaissance de la donnée, de comprendre sa structure et d’évaluer sa qualité. Avec un peu d’habitude, c’est aussi durant cette étape que nous prenons conscience de la charge de travail de préparation et du potentiel analytique du jeu de données.\n\nUne première étape est de comprendre la géométrie de la table : nombre de lignes et de colonnes, ainsi le taux de remplissage des colonnes. Cela va nous être utile pour éventuellement ajuster les outils de traitement pour faire face au volume ou choisir de travailler sur un échantillon.\n\nTypiquement, nous récupérons deux nombres à ce stade, compte de lignes et compte de colonnes), comme (2488225, 14) : 2 488 225 lignes et 14 colonnes.\n\nLa deuxième étape du profilage est d’identifier les métriques, les attributs et les clés, voire les éléments calculés existants et potentiellement pertinents à ajouter. Nous allons également revoir les types de données et la cohérence des formats utilisés. Cette phase peut se faire avec un simple éditeur de texte si nous travaillons sous fichiers plats ou par visualisation de la table correspondante. Reconnaître les métriques donne une idée de ce que le jeu de données va nous permettre de mesurer. Les attributs indiqueront le contexte de ces données et les différents axes d’analyse. Les clés permettront d’évaluer le niveau de granularité des informations et leur propension à être jointes à d’autres données. Les éléments calculés enfin, qu’ils existent ou qu’ils soient prévus par des calculs ultérieurs, permettront de juger du potentiel analytique de la table au-delà des colonnes existantes. Cela pourra aussi nous donner une idée du volume de préparation nécessaire, voire de la validité du set dès le départ. Il est inutile de commencer une analyse si aucune cohérence n’apparaît dans les données de la table.\n\nEnsuite, nous allons plonger un peu plus en détail et analyser la qualité des différents éléments constitutifs du jeu de données. Comme ils ont des types et des rôles différents, notre attention va se porter sur des aspects spécifiques de chacun d’entre eux.\n\nProfiler est toujours bon, mais tout n’est pas bon à profiler\n\nCette phase préliminaire de profilage est primordiale lors des premières prises en main, elle pourra être limitée à un audit plus élémentaire sur un processus mature et récurrent. Elle peut s’avérer fastidieuse sur des jeux de données larges, aussi doit-elle se concentrer sur les colonnes pertinentes pour l’analyse. Inutile de chercher à auditer des pans de données que nous n’utiliserons pas. D’ailleurs, que font-ils toujours dans nos tables s’ils ne sont pas utiles ?\n\n\\> La revue des clés existantes\n\nL’identification des clés ou des identifiants de lignes est fondamentale. Une table sans identifiants pour ces éléments pourra difficilement être rattachée à d’autres jeux de données, mais, surtout, nous ne pourrons pas identifier un élément plutôt qu’un autre dans nos requêtes. Faute de colonne « clé », nous devrons regarder si des combinaisons de colonnes peuvent permettre de générer cette clé. Nous pourrions ainsi combiner une date avec un type d’événement, une taille avec une couleur pour tenter de trouver cet identifiant unique. Notre attention devra aussi se porter sur la concision et la régularité du format de ces clés. Leur hétérogénéité ou leur format complexe peut être un signe de la difficulté de leur utilisation future.\n\nLa revue de la qualité des clés pourra se faire par des comptages, une mesure de leur structure ou longueur, une revue visuelle ou des graphiques.\n\nUn compte et un compte distinct des clés permettent de valider leur unicité de manière simple et rapide.\n\nUne revue visuelle peut permettre de percevoir les formats de clé standards.\n\nUn compte de caractères peut permettre d’évaluer la stabilité d’une clé.\n\nDes graphiques représentant la distribution des valeurs ou des longueurs de clé permettent une première validation rapide.\n\nÊtre le maître des clés\n\nLorsqu’une table ne possède pas de clé unique permettant d’identifier un enregistrement particulier, il peut être pertinent de créer une clé à partir d’attributs existants pour générer un identifiant unique.  \nPrenons un jeu de 52 cartes. Un tableau comptant le nombre d’occurrences d’une couleur montre qu’elle n’est pas une clé unique. De même pour la valeur d’une carte. Notre seul moyen de pouvoir extraire une carte en particulier est de combiner la clé couleur et la clé valeur. Si nous demandons un pique, nous aurons 13 cartes en retour. Si nous demandons une dame, nous aurons 4 cartes en retour. Cependant, si nous appelons la dame de pique, nous n’aurons qu’une seule carte correspondante.  \n  \nIllustration 22. Compte des clés couleur, valeur et valeur-couleur dans un jeu de cartes.  \nSeule la combinaison valeur-couleur est une clé unique.\n\n\\> L’audit des attributs\n\nLa revue des attributs permet d’abord de voir leur répartition au travers du jeu de données. Par exemple, une colonne {âge, sexe, prix} devrait se répartir selon une certaine courbe, comme une courbe de Gauss par exemple. Elle va permettre de voir combien d’attributs ne sont pas renseignés : ils représenteront des trous dans notre analyse.\n\nDans l’illustration 23, la courbe en cloche est un indicateur d’une probable bonne qualité des attributs classe de cabine et classe d’âge.\n\nIllustration 23. Ventilation du compte des survivants et des victimes du Titanic par classes d’âge et par classes de cabine16\n\nEnfin, cette revue permet d’examiner la variabilité de ces attributs : comme pour le profilage des clés, avons-nous plusieurs expressions pour un même attribut, avec ou sans espaces, avec ou sans majuscules, avec des erreurs de frappe, etc. ?\n\nLa qualité des attributs conditionne celle de nos analyses, de nos représentations graphiques et du résultat de nos algorithmes.\n\n\\> Le contrôle des métriques\n\nLa validation des métriques est plus délicate. La donnée doit tout d’abord être un nombre correctement constitué de chiffres. Il arrive que des grands nombres comportent l’exposant « E » ou que des fichiers étrangers n’aient pas le même signe pour marquer les décimales (un point ou une virgule), voire comporte des séparateurs de milliers, ce qui conduit à leur prise en compte comme des chaînes de texte alphanumériques. Ces formats vont empêcher les agrégations et les calculs ultérieurs. Ils vont bien souvent forcer les solutions d’analyse à considérer la colonne comme une colonne d’attribut.\n\nEn dehors de ce risque de format, il est souvent délicat de savoir si un chiffre est correct dans l’absolu. Ce constat nous conduit à procéder par revue analytique pour auditer sa variabilité dans le temps, ou à faire une série de calculs élémentaires tels que somme, moyenne, minimum et maximum, voire déviation standard, pour identifier si les nombres ont une certaine cohérence.\n\nCes phases peuvent être faites par des calculs ad hoc, sous forme de tableau. Elles peuvent également être réalisées sous forme de graphique, comme pour une analyse classique.\n\nLes deux graphiques du haut de l’illustration 24 indiquent des valeurs a priori cohérentes sauf peut-être à partir de la seconde moitié de l’abscisse. Dans le deuxième, le nombre de pièces moyen atteint les 100, ce qui semble anormal. Celui du bas, qui donne l’âge moyen des maisons, semble présenter beaucoup de variabilité mais dans des valeurs d’âge cohérentes (entre zéro et 100 ans).\n\nIl est parfois plus facile à l’œil de repérer un problème de cohérence entre plusieurs métriques dans un nuage de points ou sur une courbe dans le temps que dans un grand tableau.\n\nIllustration 24. Trois exemples de visualisation de la variabilité d’une métrique\n\nIllustration 25. Exemple de relative cohérence entre deux variables en x et y\n\nNous voyons un groupe de points en bas à droite de la figure 25 qui ne semble pas être en ligne. S’agit-il d’une erreur de saisie ou d’une opération particulière ? Ce sera au data pionnier d’investiguer.\n\nMalheureusement, l’absence de variabilité ou d’anomalies n’est pas synonyme de validité : elle ne remplace pas l’expérience et la communication avec le terrain que nos gains en efficacité nous permettront d’avoir régulièrement.\n\n\\> La validation des calculs\n\nPour la validation des calculs, la relecture des formules, leur test sur des valeurs connues ou extrêmes telles que 0, 1, – 1 ou leur projection dans des graphiques ou des matrices de résultats évitent les erreurs classiques.\n\nLe second risque est celui de leur ordre d’application. Nos analyses agrègent souvent de gros volumes de données avec des sommes, des moyennes ou des comptes (voir infra, Optimiser les analyses par les données calculées/préparées). Si des éléments calculés sont déjà des ratios calculés à l’origine entre colonnes, alors leur regroupement en sommes peut poser un problème d’ordre de calcul. Ce sont les fameux problèmes d’associativité ou de commutativité des opérations.\n\nNous pensions que ces notions de collège ne nous serviraient plus ? Elles reviennent hanter nos rapports en nous forçant à positionner nos calculs aux bons endroits dans la chaîne de traitement, et parfois à la toute fin d’un processus, pour éviter que la moyenne des sommes ne devienne la somme des moyennes.\n\n\\> Les fonctions automatisées de profilage (profiling)\n\nGrâce à des fonctions de logiciel d’analyse ou de langage de programmation, il est possible d’automatiser la production de rapports de profiling assez détaillés. Regardons cela à partir de cette simple table répertoriant les performances motrices de voitures dont voici les premières lignes. Nous distinguons les attributs, les clés et quelques métriques.\n\nNous pouvons appliquer une première analyse de nos métriques.\n\nCertains rapports de profilage, tels que pandas-profiling, accessibles à partir de commandes Python, vont aller encore plus loin dans la revue avec par exemple le nombre de cellules manquantes et celles dupliquées, le nombre de catégories ou de valeurs numériques.\n\nIls vont nous permettre d’aller en profondeur pour chaque colonne.\n\nCes rapports vont même nous proposer des analyses plus qualitatives telles que des corrélations ou des avertissements sur des problèmes potentiels.\n\n\\> Pour conclure\n\nCes tâches de profilage qui nous donnent la carte d’identité et le bulletin de santé de toute nouvelle data sont la condition nécessaire pour pouvoir travailler dessus avec sérénité. Elles doivent devenir un réflexe.\n\nTableau 7. Synthèse des points d’attention en profilage"
        }
      },
      {
        "json": {
          "pageId": "PAGE069_1757157753084",
          "text": "## Connecter la data pour une analytique plus riche et pertinente\n\nUne seconde phase de la chaîne analytique va être la manière dont nous organisons nos données. Cette phase est peut-être la plus technique pour les utilisateurs métier, car elle contient des concepts et des termes auxquels nous ne sommes pas habitués. Les prochaines pages vont être un peu délicates à digérer du fait de l’emploi de nouveaux termes, mais, une fois les concepts compris, ils seront acquis pour toujours et sembleront simples et logiques.\n\n\\> La notion de jointure\n\nUne jointure consiste à relier deux tables entre elles par l’intermédiaire d’une clé commune.\n\nPrenons l’exemple d’une liste de factures qui contient les codes de client. Si nous souhaitons détail­ler notre chiffre d’affaires en utilisant le pays du client ou son type d’activité, nous allons devoir chercher cette information dans la table client. Afin de join­dre chaque facture à ces attributs, nous allons les joindre à leur client correspondant de l’autre table en utilisant comme moyen de liaison le code du client, présent dans les deux tables. Ce numéro est appelé clé de jointure.\n\nCes jointures sont la clé d’une analyse avancée, car c’est grâce à elles que nous allons regrouper toutes les données, les facettes d’une situation, et les soumettre à des analyses holistiques.\n\nTraditionnellement, nous situons les tables jointes en fonction de leur position à droite ou à gauche sur le papier ou l’écran. Nous verrons dans les étapes suivantes que cette notion a son importance.\n\nNous voyons ici que les clés, qui, paradoxalement, n’ont qu’un petit rôle d’identifiant unique utile pour la sélection ou le comptage, ont un rôle fondamental dans la création de modèles de données, eux-mêmes cruciaux pour des analyses futures riches et pertinentes.\n\n\\> La cardinalité d’un modèle\n\nSous un nom complexe se trouve un concept simple : combien de clés à gauche de la jointure correspondent à combien de clés à droite. Logiquement nous avons trois cas de figure.\n\nLe One to One ou « un vers un »\n\nChaque clé à gauche n’aura qu’un équivalent à droite et réciproquement. Cela se produit quand nous joignons des données de listes finies telles que des pays, des catalogues produits, des groupes d’individus.\n\n→ Exemple de one to one\n\nUne table du PNB par pays jointe à une table de population par pays pour ensuite calculer un PNB par habitant constitue une relation one to one, car il n’y aura (en principe) que des pays uniques à gauche et à droite de la jointure.\n\nLe One to Many ou le Many to One, le « un vers plusieurs ou plusieurs vers un »\n\nÀ chaque clé unique d’un côté de la jointure va correspondre plusieurs clés de l’autre côté. Ce sera le cas le plus répandu. Souvent nous avons une longue liste d’enregistrements que nous souhaitons enrichir d’attributs stockés dans des tables de référence. Chaque enregistrement va être connecté à cette table et il sera tout à fait possible que certains d’entre eux pointent vers la même référence.\n\n→ Exemple de One to Many\n\nUne liste de factures sera certes composée de factures uniques, mais les clients sur ces factures pourront apparaître à de multiples reprises (un client pouvant faire plusieurs achats). Relier cette table de factures à la liste des clients pour obtenir un grou­pement de celles-ci par types ou villes du client donnera une jointure Many to One, ou bien, si vous permutez visuellement les tables, One to Many. Ainsi, si vous avez n factures pour un client, votre jointure à la table clients, où chaque client est unique, vous obtiendrez n lignes, enrichies chacune des détails du client que vous aurez sélectionné.\n\nLe Many to Many ou « plusieurs vers plusieurs »\n\nPlusieurs clés à gauche pointent vers plusieurs clés à droite. Cette option est la seule qui, même si elle est techniquement réalisable, ne donnera aucun résultat sensé. En effet, les n clés à gauche vont se connecter aux m clés à droite et vont générer n × m lignes de résultat. L’analyste n’aura plus un compte de lignes correct, qu’elles proviennent de la table de gauche ou de droite.\n\n→ Exemple de Many to Many\n\nNous croisons ici une table Commandes et une table Factures avec des clés « Code Client » apparaissant plusieurs fois dans chacune. Pour la jointure sur le Code Client, nous allons avoir 3 x 2 = 6 lignes de résultats, ce qui n’aura aucune pertinence analytique. Toutes les métriques de ces tables apparaîtront plusieurs fois (trois fois pour les montants de commandes et deux fois pour les montants de factures).\n\nCe type de jointure crée ce qu’on appelle un produit cartésien. Outre le fait que leurs résultats sont inutilisables, ces jointures peuvent gravement nuire à la performance d’un système. Lancées sans contrôle sur des milliers de lignes avec des clés répétées, elles peuvent rapidement générer des grands nombres de lignes en résultat (juste croiser 10 000 clés identiques avec 5 000 aboutit à 50 000 000 lignes), ce qui conduit invariablement à ralentir, voire à bloquer les bases de données qui croulent sous les volumes de lignes générées.\n\nC’est pour cela que nous devons nous garder de travailler en direct sur les modèles complexes des bases transactionnelles. Il est très facile de créer de produits cartésiens sans le savoir et d’entraîner des crashes système, avec les conséquences opérationnelles (voir chapitre 4, L’analytique transactionnelle n’est pas l’analytique décisionnelle).\n\n\\> Les types de jointures\n\nEn plus de leur cardinalité que nous venons de voir, les jointures vont également avoir un sens et une dynamique, d’où l’importance du positionnement des tables évoqué plus haut.\n\nIl existe quatre dynamiques de jointures (on passera les moins courantes).\n\nLa jointure simple ou inner join\n\nElle ne retourne que les éléments correspondants dans les deux tables jointes. Cette jointure est la plus restrictive. Elle peut sembler la plus courante car la plus simple, mais elle a l’inconvénient d’éliminer les enregistrements sans correspondance de part et d’autre.\n\n→ En reprenant l’exemple précédent, les pays qui n’apparaissent pas dans une des tables de statistiques que nous joignons (de nombreux pays n’ont souvent pas la capacité statistique de tout mesurer tous les ans) seront tout simplement complètement éliminés des résultats. Ils n’apparaîtront ni pour leur PNB ni pour leur population. Un analyste négligent les fera donc disparaître de son rapport.\n\nLa jointure gauche ou left join\n\nElle conserve toujours tous les enregistrements de la partie gauche et ajoute, s’ils existent, les éléments correspondants de la partie droite.\n\n→ Dans le même exemple, elle permettrait de conserver tous les pays de la table PNB avec ou sans statistique de population dans les résultats.\n\nLa jointure droite ou right join\n\nC’est l’inverse de la gauche.\n\n→ Toujours dans l’exemple, elle permettrait de conserver tous les pays de la table Population avec ou sans statistique de PNB dans les résultats.\n\nLa jointure full outer\n\nElle apporte dans les résultats l’intégralité des deux tables, même s’il n’y a pas de correspondance de part et d’autre.\n\n→ Nous aurons donc cette fois-ci l’ensemble les pays des deux tables, dont certains auront PNB et Population, d’autres n’auront que PNB ou Population.\n\nLes deux jointures droite et gauche sont les plus courantes car elles préservent l’intégralité de l’une des tables dont nous voulons conserver l’intégrité. Elles évitent par conséquent de se retrouver avec des extractions qui omettent des éléments quand une jointure n’a pas pu être trouvée.\n\n→ Dans l’exemple de la table des factures jointe à la table du détail des caractéristiques clients, il est classique de faire une jointure gauche (car la table facture est à gauche du diagramme) de manière à assurer la conservation de l’intégralité de chiffres d’affaires dans l’analyse.\n\nLe choix d’un type de jointure n’est donc pas anodin. Il dépend de la question posée. Une jointure peut avoir un effet :\n\n– filtrant, lorsqu’elle est stricte et ne retient que des éléments en commun ;\n\n– enrichissant, pour une base existante lorsqu’elle est gauche ou droite ;\n\n– combinant, lorsque l’on souhaite marier deux bases, même s’il n’y a pas forcément 100 % de correspondance.\n\n\\> Les résultats de jointures\n\nQu’obtenons-nous en résultat de ces jointures ? La requête nous restitue une table combinant les informations croisées. Nous pouvons accessoirement choisir les champs de chacune des tables à conserver dans les résultats.\n\nLes relations One to One\n\nRegardons les différents résultats de jointures de ces deux tables sur la clé Code Client. La table Factures a huit enregistrements et la table Créances Clients en a neuf. Cette jointure va nous permettre d’estimer le risque que nous prenons à émettre de nouvelles factures sur des clients qui ont déjà de gros encours de dettes sur des durées importantes.\n\nAppliquons les quatre grands modèles de jointure.\n\n• Jointure simple : seules les lignes avec des clés existant dans les deux tables vont être retournées par la requête. Au total, six lignes ont des clés communes.\n\n• Jointure gauche : toutes les lignes de la table de gauche, les factures, vont être retournées par la requête et celles qui ont une correspondance dans la table de droite, les créances clients, seront enrichies avec des données d’encours de dette.\n\nAu total, les huit lignes de la table Factures sont conservées et deux d’entre elles n’auront pas de données de créances correspondantes.\n\n• Jointure droite : toutes les lignes de la table de droite, la créance client, vont être retournées par la requête et celles qui ont une correspondance dans la table de gauche, les factures, seront enrichies avec des données de facturation. L’intégralité des neuf lignes de la table dette va être restituée, que des données de factures correspondent ou pas.\n\n• Jointure outer : les lignes des deux tables vont être restituées. Il y aura dix lignes dans les résultats. Trois lignes n’auront pas de détail de facture et deux lignes n’auront pas de détail de dette.\n\nLes relations « un vers plusieurs » ou « plusieurs vers un » (One to Many ou Many to One)\n\nNous ne détaillerons qu’un seul cas, la jointure gauche, les autres pouvant être déduits.\n\nCet exemple joint les dépenses par département d’une entreprise à leur détail par catégorie.\n\nLe résultat de la requête sera le suivant :\n\nLes lignes du côté « un » sont multipliées par le nombre de lignes correspondant coté « plusieurs ». Par conséquent, une partie de l’information générée se répète sur certaines colonnes. Les calculs de somme ne seront plus possibles sur celles-ci, car leur métrique ou attribut, si nous les comptons, est multipliée par le nombre de lignes du côté many.\n\nAfin d’éviter ce problème et qu’un utilisateur lambda ne tombe pas dans ce piège de l’agrégation, nous pouvons :\n\n– forcer l’agrégation par défaut à la moyenne, au minimum ou au maximum pour ne pas être impacté par la répétition ;\n\n– prédiviser les nombres par le nombre attendu de répétitions de manière que l’agrégation par la somme, qui est souvent celui par défaut, retourne à un résultat juste.\n\n\\> Pour conclure\n\nCes jointures sont finalement un exercice assez naturel. Dans la vie courante, nous appliquons des jointures régulièrement, lorsque nous regardons un sommaire, un index, un catalogue de références, ou que nous regardons les personnes communes de notre réseau professionnel ou personnel avec celui d’un ami.\n\nNous voilà prêts avec les trois concepts clés de la modélisation. Avec ces bases, nous allons pouvoir joindre tout type de données.\n\nSouvenir lointain d’Excel\n\nCela nous rappelle sans doute le recherchev (vlookup) d’Excel : effectivement, les tableurs suivent la même logique pour joindre deux tables. La logique de la donnée en table qui peut être jointe à d’au­tres s’applique tout simplement à toute technologie, et les tableurs ne font pas exception."
        }
      },
      {
        "json": {
          "pageId": "PAGE070_1757157753084",
          "text": "Nous allons donc pouvoir progressivement joindre de la donnée et enrichir notre vision d’un problème de nouveaux points de vue. Petit à petit, notre table principale, qui contient les métriques que nous voulons étudier, va s’enrichir de tables de part et d’autre. Cette table centrale devient notre « table de fait » et les jointures tout autour forment les rayons d’un modèle qui ressemble à… une étoile : d’où le nom de ces représentations, le modèle en étoile.\n\nLa table de fait\n\nUne table de fait est une table qui contient les données que nous voulons observer/mesurer (les faits) sur le sujet que nous voulons analyser. Ici, il s’agit de la table des ventes.  \nIllustration 26. Modèle en étoile\n\nLes jointures de l’illustration 26 vont nous permettre d’établir une vision très riche de nos ventes.\n\nNous allons pouvoir répondre à des questions beaucoup plus fines qu’avec la seule table de Ventes initiale. Par exemple, nous pourrons mesurer :\n\n– les ventes par superviseur pour chaque pays ;\n\n– la famille de produits qui se vend le mieux pour chaque client ;\n\n– comment la météo influence les ventes, etc.\n\nNous avons vu au chapitre 4 que des modèles très enrichis pourront nécessiter des visualisations avancées pour présenter tous leurs éléments sur une seule vue. Nous verrons que l’intelligence artificielle peut être nécessaire pour détecter des corrélations et faire des inférences ou des prédictions à partir de toutes ces données reliées (voir infra, Le Big Data : ses mythes, sa réalité).\n\nLes modèles en flocon\n\nL’apparence de ces modèles a aussi inspiré d’autres noms tels que le modèle en flocon lorsque celui-ci présente plusieurs niveaux de jointure dans chaque branche de l’étoile.\n\nLes éléments clés d’un beau modèle de données décisionnel  \n\nPhilippe Champleboux, diplômé de l’École supérieure d’informatique, a démarré sa carrière en 1988. Après une première partie de carrière orientée informatique de gestion où il participa à des projets de toutes tailles, depuis des projets mainframe engageant le budget de la nation jusqu’aux développements d’application spécifiques pour micro-ordinateurs exploitant la « nouveauté » des réseaux locaux. En 1998, il s’oriente résolument vers la BI, prenant plaisir à contenter les utilisateurs avec des applications spécifiques, conçues pour eux, répondant à leurs besoins propres et ciblés d’indicateurs et de pilotage. À partir de 2008, il élargit son domaine de compétence en travaillant sur des projets d’élaboration budgétaire qu’il juge complémentaires à la BI (on exploite les informations du passé, on se donne des objectifs et on se projette dans le futur).  \nEn 2014, il rejoint un cabinet d’experts de l’informatique décisionnelle, Next Decision, et travaille indifféremment sur des projets BI et d’élaboration budgétaire, tout en collaborant avec l’IGR-IEA de Rennes dans la structuration d’un nouveau cours, « élaboration budgétaire » adossée à l’un des logiciels phares du marché actuel.  \nPourquoi un beau modèle (physique ou virtualisé) est clé pour une analytique facile, performance et juste.  \nLa modélisation d’un modèle de données BI doit répondre à plusieurs objectifs, en apparence contradictoires, notamment la lisi­bilité, afin que des utilisateurs puissent intuitivement retrouver l’information qu’ils désirent retrouver, la simplicité d’utilisation, l’unicité (univo­que) des termes, vocabulaire et concepts employés, la performance en termes de restitution et d’exploitation, la maintenabilité et l’évolutivité.  \nCes objectifs sont autant de défis à relever, dont la réalisation permettra de voir fleurir un sourire de contentement et de satisfaction de la part des utilisateurs (alors que je n’ai jamais vu un utilisateur bondir de joie quand il recevait un nouveau progiciel de gestion !).  \nMais… qu’est-ce qu’un « beau » modèle ? Hormis les questions d’esthétique, somme toute assez subjectives, un beau modèle est un modèle de données qui répondra à tous les objectifs cités précédemment.  \nChaque utilisateur devra pouvoir retrouver une vision qui lui est propre et uniquement celle dont il a besoin. Un responsable commercial sera intéressé par la performance et le travail de ses commerciaux ; un gestionnaire RH sera intéressé par l’importance et l’évolution des masses salariales ; le directeur financier par ses encours et sa trésorerie ; le trésorier par l’évolution des taux du marché, des cours de devise, des échelles d’intérêt, etc.  \nChacun des métiers de l’entreprise peut avoir besoin (ou pas !) d’informations statistiques lui permettant d’analyser la situation vivante, en termes de quantité, de qualité, de valorisation, d’évolution, de comparaison, de pourcentage d’atteinte, et de décliner ces analyses selon ses propres axes métier (produits, clients, géographie, circuit de distribution, devise, etc.).  \nOn y parlera de la manière la plus précise possible, distinguant par exemple le CA brut sur facture du CA net après remises, du CA net après déduction des frais de transport, du CA net après déduction des coûts variables, etc.  \nMes recettes métier pour un modèle élégant et performant  \nA) Soyez extrêmement précis dans les termes que vous employez et pensez à autant d’indicateurs en cas de risque de confusion.  \nN’oubliez jamais que, lorsque vous sortirez un reporting, une statistique, un tableau de bord, il faudra que tous parlent le même langage et qu’il soit impossible (bon, d’accord extrêmement difficile) de produire la même statistique avec des montants différents. Pensez que, d’un service à l’autre, la même notion peut cacher des significations différentes. Qui n’a jamais entendu dire qu’il faudrait réconcilier le chiffre d’affaires commercial et le chiffre d’affaires comptable ?  \nAfin de satisfaire l’ensemble des interlocuteurs concernés par cette notion de chiffre d’affaires, il faudra dresser :  \n1. un certain nombre d’indicateurs successifs permettant cette réconciliation : CA brut facturé, CA net après remises, montant comptable des remises arrière, montant comptable des avoirs, clients douteux, créances irrécouvrables, etc. ;  \n2. plusieurs tables de faits, chacune ayant la granularité souhaitée par les utilisateurs désirant exploiter individuellement chacun de ces indicateurs ;  \n3. une table de fait unique, avec le niveau de granularité et d’axes d’analyse permettant de stocker l’ensemble des indicateurs et d’avoir ainsi une vue comparative unique de cette réconciliation.  \nB) Effectuez et stockez tous les calculs possibles lors de l’alimentation de votre modèle.  \nRépondez simplement à la question suivante : est-il plus rapide d’effectuer un calcul, une fois, lors du chargement de votre modèle, ou plusieurs centaines de milliers de fois, lorsque chaque utilisateur voudra analyser le résultat d’un calcul ?  \nD’accord, la formulation de la question entraîne d’elle-même une seule réponse (la bonne réponse !).  \nLorsqu’un calcul, tel qu’une conversion, une addition, une multiplication par un taux peut se faire lors de l’alimentation et que le résultat de ce calcul n’évolue plus par la suite, il est conseillé de le calculer, de le stocker, afin que tous les utilisateurs puissent directement utiliser ces résultats (pas de risque d’erreur, pas de risque de formules dépendant de l’utilisateur, moins de risques de dégradation des performances). Il m’est arrivé, dans le cadre de la mise en œuvre d’un BI dans une entreprise de vins, liqueurs et spiritueux de stocker les indicateurs de volumes (commandés, achetés, fabriqués, stockés, vendus, expédiés, facturés, cassés, etc.) en différentes unités : nombre d‘articles (en unité), nombre d’articles (en contenance totale d’alcool), nombre d’articles (en équivalent « caisse standard »).  \nEt n’oublions pas tout ce qui va permettre d’effectuer des statistiques en « devise de transaction », « devise de reporting », « devise de comptabilité », pour lesquels on s’attachera à stocker les montants concernés dans chacune des devises concernées, mais également leur contre-valorisation en devise « statistique » permettant de tout sommer et additionner. (Bien sûr, utilisez le bon cours de conversion… pas forcément le même cours pour contre-valoriser du bilanciel et du résultat ?)  \nC) Unifiez vos référentiels et vos données. Appelez un chat un chat ! (Et nommez chacun de vos chats !)  \nUtilisez un identifiant unique pour chacun de vos axes d’analyses métier, et ce, quelle que soit l’origine (parfois multiple) de vos données de référence. Sans aller jusqu’à initier le concept MDM du « Golden record », vous devez impérativement croiser, mapper, identifier, faire correspondre vos référentiels article, clients, sites, société, usine, établissement, etc.  \nSi vous avez des données qui viennent à la fois de votre logiciel de gestion commerciale et de votre logiciel de fabrication ou de gestion de stock, vous devrez impérativement arriver à faire correspondre la codification « commerciale » des articles (commandés, vendus, facturés, cassés, etc.) avec la codification des articles (stockés, consommés, fabriqués, produits). Ce n’est qu’à ce prix que vous pourrez procéder (par exemple) à des calculs de marge sur coûts variables. Dans la même veine, unifiez vos unités internes pour ramener vos calculs à un système de mesure cohérent.  \nExemple des prix : prix à l’article, prix au kilo, prix à la palette, prix à la tonne… Quelle est la marge unitaire d’un article vendu 1 euro le kilo dont le prix de revient unitaire est de 10 centimes et le coût de transport de 50 euros la tonne ?  \nIl vous faudra parfois établir vos propres tables de correspondance (obligatoirement univoque) entre une codification article commerciale et une codification produit logistique ; cette table de correspondance pourra faire intervenir des coefficients de mappage.  \nAssurez-vous de la complétude de vos référentiels de données, qui seront autant d’axes d’analyse pour étudier vos indicateurs et en déduire vos axes d’amélioration.  \nIl y a quelques années de cela, alors que nous démarrions le chargement de notre Data Warehouse, après avoir (re)codifié nos référentiels articles et clients, nous avons constaté près de 20 % de rejets lors de l’alimentation des données « commandes ». Il nous manquait des clients ! « Impossible, nous dit le responsable informatique, je vous ai communiqué l’intégralité des clients autorisés à passer des commandes, les autres sont blacklistés par notre politique relative aux mauvais payeurs ! »… Les commandes indûment enregistrées par le progiciel ont pu, grâce au Data Warehouse, être bloquées. À l’heure où l’on parle de retour sur investissement, je pense que jamais projet informatique ne fut aussi rapidement amorti !  \nQuelques écueils à éviter  \nComparer des données ayant des niveaux de granularité différents  \nImaginez que les objectifs des commerciaux sont assignés par commercial, par région et par trimestre, et que, par ailleurs, vous ayez le détail du chiffre d’affaires par commercial, ville, mois et produit.  \nVous ne pouvez avoir qu’une seule table, car les axes ne sont pas les mêmes et certains axes communs n’ont pas le même niveau de granularité.  \nModélisez donc au minimum deux tables de faits, l’une contenant le détail du CA réalisé, l’autre contenant (par commercial, région et trimestre) le CA réel et le CA objectif. Cette seule table de fait vous permettra, quel que soit le logiciel d’analyse utilisé, de sortir tous vos indicateurs de performance, pourcentage atteint, évolutions, etc., selon une granularité ou maille qui convient à cette vision « Objec­tivation des commerciaux ».  \nFaire fi du futur"
        }
      },
      {
        "json": {
          "pageId": "PAGE071_1757157753084",
          "text": "Votre société évoluera (on vous le souhaite), vous êtes déjà au courant de futures évolutions (acquisition, innovations, nouvelle activité, etc.). Vous devez, lors de votre modélisation, lors de la définition de votre modèle de données, de la conception de vos dimensions et de vos tables de faits, penser au futur, vous poser la question d’axes d’analyse futurs (devise, langue, pays) qui sont déjà dans l’air du temps, mais peut-être pas encore vivants dans les faits.  \nPosez-vous alors la question : qu’est-ce que je sais (ou subodore) des évolutions, des projets, des changements dans la société et saurai-je y faire face ou ne puis-je pas, dès maintenant, prévoir un modèle « multisociété », « multidevise », « multi-activité », etc.  \nNe pas respecter les contraintes RGPD  \nVous ne pouvez plus ignorer les contraintes réglementaires de la protection des données personnelles, les conséquences en sont pécuniairement trop graves pour votre société.  \nGarantissez le cas échéant le droit à l’oubli, à l’anonymisation, à la communication.  \nFaites-vous aider par la personne (ou le service) en charge, dans votre société, du respect de ces contraintes réglementaires.  \nMes conseils aux data pionniers  \n« Ratissez large pour démarrer petit », ne soyez pas trop ambitieux pour votre (vos) premier(s) projet(s). Vous pouvez identifier et analyser un grand panel de besoins, pour différents services de votre société, mais ne commencez la réalisation que d’un domaine fonctionnel qui vous permettra de faire vos premières armes : tester la solidité des solutions logicielles implémentées et choisies vous apprendra les bons réflexes.  \n« Décomposer chaque sujet en plusieurs sujets aisément maîtrisables » : si un sujet, une vision, un besoin vous paraît trop vaste, trop complexe pour être traité dans son ensemble, décomposez ce sujet en plusieurs sujets plus simples et, le cas échéant, décomposez encore tout ou partie de ces sujets en sujets plus simples, jusqu’à arriver à un ensemble de sujets maîtrisables et réalisables (sujets que vous allez réaliser).  \nUne fois tous les sujets élémentaires traités, résolus, validés, leur intégration en un sujet plus vaste sera aisément facilitée."
        }
      },
      {
        "json": {
          "pageId": "PAGE072_1757157753084",
          "text": "# Tirer le meilleur de la donnée avec une bonne préparation\n\nOublier d’ajouter les clés dans un jeu de données.\n\nLes phases de modélisation associées aux phases de préparation sont essentielles pour la qualité et la performance des analyses. Nous avons vu que les bases de la modélisation tiennent en quelques éléments. Nous allons voir maintenant que les techniques de préparation de données peuvent également être résumées en une liste d’actions simples."
        }
      },
      {
        "json": {
          "pageId": "PAGE073_1757157753084",
          "text": "## Ajouter des tables de groupements ou des tables de hiérarchies\n\nL’objectif est de permettre une analyse à différents niveaux de granularité, comme si on regardait la data à différents niveaux de détails, à travers un télescope, un microscope, une loupe ou tout simplement à l’œil nu.\n\nUn chiffre d’affaires, par exemple, peut être considéré au niveau mondial ou par continent, pays, ville, commercial, type de produit, produit, jour, etc., en fonction des besoins d’analyse. Des entités peu­vent être analysées géographiquement, mais aussi par leur forme légale ou leur activité.\n\nCes regroupements ou hiérarchisations s’accompagnent de la possibilité d’agréger les métriques à chaque niveau par le biais de sommes, minima, maxima, moyennes, comptages etc. (voir supra, Connaître ou définir les règles d’agrégation de la data).\n\nAlors comment appliquer un groupement ou une hiérarchie à nos tables de faits, pour qu’elles ne soient pas seulement une liste interminablement détaillée dans laquelle il est difficile de se retrouver ?\n\nIl nous suffit de faire une jointure ! Une table qui hiérarchise des pays, des produits ou des entités, ou leur applique des attributs de regroupement peut instantanément s’appliquer à une table de faits à partir d’une simple jointure. Là où l’opération peut devenir périlleuse sur des tableurs dès que les volumes augmentent ou que le nombre de jointures se multiplie, elle conserve sa simplicité avec des tables de données. Une jointure est une construction logique qui s’applique indifféremment à 10 lignes ou à 1 million (voir supra, Connecter la data pour une analytique plus riche et pertinente).\n\nParmi les hiérarchies fréquemment retrouvées dans les décisionnels, nous trouvons :\n\n– les hiérarchies géographiques : du continent à l’appartement ;\n\n– les hiérarchies des secteurs d’activité : de l’industrie au détail de l’activité d’une entreprise ;\n\n– les hiérarchies de personnes : du patron au stagiaire par exemple.\n\nDans l’illustration 27, nous pouvons rattacher l’activité de la Barbade (Barbados) à celle de la région « Caribbean », qui est dans « Latin America and the Caribbean », qui fait partie des « Americas » et est supervisée par Carole Rylance.\n\nIllustration 27. Table de hiérarchie pays/sous-région/région/continent  \net d’attribution à des managers\n\nAutre exemple, voici une belle hiérarchie du règne animal, con­centrée sur une infime partie incluant les dinosaures.\n\nIllustration 28. Extrait de la hiérarchie des animaux : règne, phylum, classe, famille et nom, soit six niveaux de détails\n\nCertaines hiérarchies peuvent être développées spécifiquement pour un contexte. Rien n’empêche de créer des niveaux de regroupement pour tout élément analysé. Ces derniers n’ont pas forcément besoin d’être une hiérarchie, ils peuvent juste allouer des codes pour indexer des contenus suivant plusieurs axes d’analyse potentiels : par exemple, une liste de produits peut se voir attribuer des codes tels que « dangereux », « lourd », « fragile », « beau ».\n\nLa magie des reportings « multidimensionnels » est en fait liée à l’ajout de ces hiérarchies ou groupements. Toute table de faits peut devenir « diaboliquement » multifacette, dès qu’on lui joint des tables regroupant et enrichissant ses attributs. Il suffit d’une simple jointure. N’hésitez pas à en créer plusieurs pour supporter tous vos axes d’analyse."
        }
      },
      {
        "json": {
          "pageId": "PAGE074_1757157753085",
          "text": "## Créer des clés pertinentes pour de nouvelles jointures\n\nLe potentiel analytique d’une table de faits est fortement lié aux possibilités de la joindre à d’autres tables. Les clés jouent un rôle critique. Leur gestion ou leur création ne peut être laissée au hasard. Voici quelques points tactiques à garder en tête au niveau métier pour éviter des frustrations ou des catastrophes dans les analyses futures.\n\n\\> Se conformer aux clés du transactionnel\n\nC’est une évidence, beaucoup de clés sont définies dès le transactionnel : les codes clients, les codes produits. Parfois leur nomen­clature est imposée par le système, d’autres fois nous aurons la possibilité de la définir. Si les options de format sont théoriquement illimitées, certaines pratiques sont moins « bonnes » que d’autres.\n\n\\> Ne pas générer des clés très longues\n\nCertaines clés doivent être d’une taille suffisante pour capturer la diversité de la population qu’elle couvre. Par exemple, le VIN (Vehicule Identification Number ou Numéro d’Identification du Véhicule) sert d’empreinte digitale à la voiture, car deux véhicules en fonctionnement ne peuvent pas avoir le même. Un VIN est composé de 17 caractères (chiffres et lettres majuscules) qui agissent comme un identifiant unique. Un VIN affiche les caractéristiques uniques, les spécifications et le fabricant de la voiture.\n\n→ Avez-vous déjà vu le VIN de votre voiture ?\n\n\\> Ne pas choisir des clés numériques qui commencent par 0\n\nEn théorie, ce n’est pas un problème au sein d’une base de données où le format, comme nous l’avons vu, peut être contrôlé et normalisé. En pratique, cela peut rapidement poser un souci si la donnée doit être traitée hors système pour des analyses ponctuelles ou pour des processus dans des équipes qui n’ont pas accès à ces systèmes. En effet, lorsqu’elle est sortie de son environnement normatif, cette clé pourra être considérée comme un nombre et perdre son zéro. Excel ne fera aucun cas de ce zéro et le supprimera la plupart du temps pour faire de cette clé un simple nombre.\n\n\\> Éviter les clés sans format standard\n\nCe n’est pas un choix rédhibitoire. Cela va juste rendre moins délicat les actions de profilage qui ne pourront pas s’appuyer sur une règle précise pour contrôler la forme de la clé.\n\n\\> Choisir des clés universelles ou ISO quand c’est possible\n\nCela n’a pas d’impact quand nous travaillons en vase clos, mais cela peut rendre la jointure avec des tables externes plus difficile. Pourquoi continuons-nous à coder nos pays avec des codes internes alors que chaque nation a son code ISO, indépendant de la langue, qui permet immédiatement des jointures avec toute donnée économique, démographique ou politique de la planète ?\n\nTableau 8. Exemple de codes ISO rattachés à un pays\n\n→ Jointure impossible après le choix d’une clé locale\n\nUn analyste avait choisi de coder la devise américaine DOL, parce que USD (US Dollar), la dénomination standard, n’était pas intuitive. Dans un rapport, ce choix pouvait avoir un certain rationnel, et encore, mais dans des bases de données ? Quand il s’est rendu compte qu’aucune table contenant des devises ne pouvait être jointe, il était trop tard. Il a fallu créer une table de correspondance intermédiaire pour relier DOL des tables de faits à USD, puis USD aux autres tables du modèle."
        }
      },
      {
        "json": {
          "pageId": "PAGE075_1757157753085",
          "text": "## Optimiser les analyses par les données calculées/préparées\n\nDans notre recherche de nouvelles métriques et de gains de temps, ces données calculées vont être précieuses. Ajouter des calculs aux données de base peut sembler une évidence, mais nous ne poussons pas toujours la logique jusqu’au bout. Si nous revenons aux motivations premières de mieux maîtriser la data – gagner en visibilité et gagner en temps –, la création et le bon positionnement des données calculées peuvent avoir un impact immédiat sur les deux.\n\nOr, souvent, dans des équipes d’analystes, chacun récupère sa donnée (sous Excel très souvent) et fait ses préparations, ses agrégations et ses calculs de son côté. Pourquoi ne pas effectuer ces opérations en avance de phase une fois pour toutes, pour que plus personne n’y consacre la moindre énergie ? Certes, chacun aura besoin d’une donnée propre à ses questions métier, mais on peut penser qu’il existe tout un pan commun de data préparée à l’avance, que chacun pourrait récupérer. Plus nous raffinons notre matière première qu’est la data, moins il reste de travail à faire par la suite, et plus nous pouvons nous concentrer sur l’analyse et l’action.\n\nNous allons aborder ces éléments calculés en quatre groupes et nous verrons ensuite quelles méthodes et technologies peuvent nous aider à réaliser ces gains de productivité promis.\n\n\\> La préparation de calculs complexes\n\nCombien sommes-nous dans toutes les équipes d’analystes à travers le monde à faire les mêmes calculs à chaque nouvelle échéance de reporting ? N’y a-t-il pas des calculs que nous pourrions mutualiser ? Non seulement nous y gagnerions en temps, mais nous réduirions notre risque d’erreur à quasiment à zéro.\n\n→ La conversion de devises\n\nL’exemple de la conversion des devises pour des filiales reportant à leur maison mère est classique. Si chaque filiale fait un reporting dans la devise du groupe, nous nous exposons à :\n\n– des risques de prendre de mauvaises hypothèses telles que prendre les cours de change à la mauvaise date ;\n\n– appliquer le taux dans le mauvais sens (dollar vers euro au lieu de l’inverse) ;\n\n– appliquer les mauvaises quotités : certaines devises se cotent par 100, d’autres par 1 000, voire 10 000 ;\n\n– ou tout simplement faire une erreur de calcul en référençant de mauvaises cellules ou colonnes dans nos tableurs.\n\nUne solution simple est de ne pas demander ces conversions au niveau local, mais de récupérer les données dans leur devise d’origine, d’appliquer les conversions dans un environnement central, contrôlé et fiable, et ensuite de redistribuer les informations converties à chacune des équipes. Nous verrons que cette option est simple à mettre en œuvre, si nous travaillons avec des tables, de manière fluide… en dehors des tableurs.\n\nFaisons un calcul simple : si, dans 20 filiales, 2 personnes passent une demi-heure par mois pour faire ce travail de conversion (en tenant compte de la mise à jour des taux et des calculs, de leur vérification, etc.), cela fait 20 heures par mois qui pourraient être consacrées à des travaux à valeur ajoutée si cette conversion était faite en central de manière instantanée grâce aux ETL, outils analytiques et procédures stockées (voir infra, Optimiser le choix du moteur de calcul).\n\n\\> Les agrégations et les réductions du nombre de colonnes\n\nLes agrégations permettent d’ajuster le volume de données entrantes au besoin d’analyse. La tentation est grande de tout vouloir, car « on ne sait jamais ». On s’encombre alors d’une data qui ralentit nos processus et alourdit les transferts des reportings.\n\nSi notre seul but est de présenter un rapport (on ne parle pas de l’analyse sous-jacente), alors le volume de données peut être considérablement réduit.\n\nCombien faut-il de lignes et de colonnes pour faire le graphique de l’illustration 29 a minima, quand bien même chacun de ces vendeurs aurait généré des milliers de transactions chacun ? Il suffit de 3 colonnes et 15 agrégats au vendeur-produit.\n\nIllustration 29. Graphique présentant la répartition des ventes par commerciaux et par produits\n\n→ Trop c’est trop\n\nGulnar, contrôleuse de gestion pour la partie production/supply chain, se plaignait sans cesse de la performance de sa machine, du réseau, des serveurs. Il lui fallait jusqu’à quinze minutes d’attente pour rafraîchir un rapport de production hebdomadaire. Après des semaines de tergiversation, d’échange de machine, de changement de câblage et d’ajout de mémoire sur les serveurs, les services informatiques décidèrent un dernier audit avant d’abdiquer.\n\nPour chaque analyse de synthèse qu’elle lançait, Gulnar réextrayait deux années de données au niveau le plus fin avant de procéder sur tableur à une agrégation massive des informations à la semaine et à la gamme de produits, puis de réduire de 90 % le nombre de colonnes pour ne conserver que celles réellement utiles à son travail. La suggestion faite par l’analyste fut de ne récupérer que les données nécessaires et de les agréger à l’avance. « Oui, mais si j’en ai besoin, je fais quoi ? » répondit Gulnar.\n\nL’analyste lui proposa de faire une extraction plus précise quand elle aurait besoin de zoomer sur une zone particulière ou de préparer des extractions de données un peu plus riches que ses besoins pour éviter de recourir à de nouvelles requêtes trop souvent. Épilogue : le volume de données fut réduit d’un facteur 30, sans dégrader aucune des analyses, Gulnar gagna 90 minutes pour chaque processus et ce n’est que très rarement qu’elle alla chercher plus de détails.\n\n\\> Les corrections de format\n\nSelon leur système d’origine ou les personnes qui ont saisi les données, les informations peuvent arriver avec une grande diversité de formes. Certaines peuvent nuire à la lecture ou à l’esthétique d’un rapport, telles que l’usage de capitales, l’apparition de caractères spéciaux ou d’espaces avant les champs, sans mentionner les variations de dates (voir supra, Les fondamentaux pour comprendre la donnée). Plutôt que de procéder à une normalisation de ces chaînes de caractères en toute fin de processus, lors des phases d’analyse, nous pouvons bâtir des traitements en avance de phase qui effectuent ces tâches une fois pour toutes et pour tout le monde.\n\nLes capitales, ce n’est pas capital\n\nUne application immédiate et simple de ces corrections de format est celle du reformatage des intitulés de colonnes ou de noms apparaissant dans les rapports. Les noms de clients, de fournisseurs ou de personnes sont souvent en majuscules dans les systèmes transactionnels. Ce qui peut être avantageux pour des en-têtes de facture ou autre document commercial peut devenir un problème pour la lisibilité du rapport d’analyse. Plutôt que chaque analyste développe sa propre macro ou y consacre une quelconque énergie pour remettre les lettres en forme (par exemple, première lettre en majuscules et les suivantes en minuscules), ce travail peut être fait systématiquement pour tous les champs entrant dans le décisionnel.  \nUne étude réalisée en 1955 par Miles Tinker a montré que « le texte tout en majuscules retardait la vitesse de lecture de 9,5 à 19 % pour les limites de temps de 5 et 10 minutes, et de 13,9 % pour l’ensemble de la période de 20 minutes17 ». Tinker a conclu que, « de toute évidence, l’impression tout en majuscules ralentit la lecture à un degré marqué par rapport aux minuscules romaines18 ».  \nTinker l’explique de la manière suivante : le texte tout en majuscules couvre environ 35 % de surface d’impression de plus que le même texte mis en minuscules. Cela tend à augmenter le temps de lecture. Si l’on ajoute à cela la difficulté de lire les mots tout en majuscules comme des unités, l’entrave à la lecture rapide devient marquée. Dans l’étude sur les mouvements oculaires réalisée par Tinker et Patterson, la principale différence dans les schémas oculomoteurs entre les minuscules et les majuscules était la très forte augmentation du nombre de pauses de fixation pour la lecture des caractères tout en majuscules19.  \nTinker préconise que tout texte en capitales soit éliminé des rapports à chaque fois que la rapidité de lecture et l’opinion du lecteur sont importantes : en plus de ralentir la vitesse, elle est ressentie comme parfois offensive par les lecteurs. Cela est d’autant plus important lorsqu’il s’agit par exemple de matériel de lecture continue, d’affiches, de panneaux d’affichage, de textes publicitaires, de magazines, de titres de livres, de formulaires et de rapports commerciaux, de titres d’articles, de livres, de chapitres de livres et de titres de journaux20.\n\n\\> Les snapshots, les sauvegardes d’instantanés\n\nCette technique consiste à sauvegarder tout un jeu d’enregistrements à un instant donné afin d’en conserver la situation exacte, sans qu’elle ne puisse être altérée. Nous pourrions penser qu’il suffit de filtrer a posteriori les informations par date pour retrouver l’état en question. Ce n’est pas toujours possible. Il arrive que ces données soient modifiées au fil de l’eau sans que les étapes de leur évolution soient conservées dans les systèmes.\n\nLes portefeuilles de prospection des vendeurs sont un exemple classique. À un instant donné, les opportunités de ventes se trouvent à un certain stade. Quelques jours plus tard, certaines auront avancé vers une vente de contrat et d’autres se seront éteintes : les statuts antérieurs de ces opportunités auront disparu. Pour connaître la dynamique d’évolution de ces affaires jour après jour, le seul moyen consiste à stocker les bilans au fur et à mesure. L’exercice consiste donc à prendre une photo des éléments pertinents pour l’analyse et à la stocker dans une table où elle sera indexée avec, par exemple, la date de sa capture.\n\n→ Évolution d’un portefeuille de vente\n\nCe graphique donne un exemple de situation d’un portefeuille de vente d’un commercial sur dix semaines. Beaucoup de nouvelles opportunités apparaissent, mais peu d’affaires arrivent à être clôturées.\n\nFigure 11. Évolution d’un portefeuille d’affaires, semaine après semaine\n\n→ Photo de classe\n\nLa seule façon de conserver une réalité d’une classe d’élèves à un instant est de prendre une photo. Dès le lendemain, cet effectif pourrait changer à cause d’élèves absents, partis, etc. En reconstituant soixante ans plus tard la classe de l’époque avec ces mêmes élèves, la photo sera différente."
        }
      },
      {
        "json": {
          "pageId": "PAGE076_1757157753085",
          "text": "## Les options pour travailler la donnée : à la volée ou stockée\n\nIl existe deux manières différentes de traiter les données calculées. Aucune n’est meilleure que l’autre. À vous de choisir celle qui s’adapte le mieux à vos besoins analytiques et à vos contraintes opérationnelles.\n\nNous pouvons effectuer ces calculs une fois pour toutes et en stocker les résultats « en dur » dans les sources de données. L’intérêt est qu’il n’y aura plus de temps de préparation et d’attente : tout sera prêt à l’emploi. L’inconvénient est que, si les données sont trop volatiles, le calcul stocké peut ne plus être à jour. Calculer à l’avance des agrégats, des conversions ou des ratios sur des données d’un exercice fiscal clos est particulièrement pertinent : pourquoi recalculer à chaque fois quelque chose dont les hypothèses ne varieront plus ?\n\nNous pouvons également les faire calculer chaque fois qu’elles sont demandées. L’analyste n’a pas à effectuer les traitements comme précédemment : ces derniers sont justes calculés pour chaque donnée demandée. C’est l’assurance d’une donnée fraîche, mais aussi le risque d’attendre le temps que tout soit traité. Même cinq à dix secondes peuvent parfois sembler une éternité au cours d’un processus itératif d’analyse.\n\nCes deux alternatives de traitement présentent les résultats in fine de la même manière pour l’utilisateur, sous forme de table :\n\n– dans le premier cas, la table est physiquement écrite et prête à la lecture : c’est une « vraie » table ;\n\n– dans le second cas, les calculs sont faits à la volée, leurs résultats apparaissent dans une table virtuelle qui a l’apparence d’une table physique : c’est une « vue » qui, dès qu’elle est interrogée par l’analyste, cherche les données nécessaires et effectue les opérations prévues. Il y a donc un temps de latence pour l’analyste.\n\nComment choisir entre stocker les calculs et les faire à la volée\n\nLes calculs à la volée sont-ils la panacée ? Si nous pouvons attendre de longues secondes, voire des minutes, à chaque fois que nous demandons un calcul ou si notre puissance de calcul est illimitée, pourquoi pas ? Cependant, même dans ces cas-là, nous pourrions nous interroger sur ce besoin de tout recalculer à la dernière minute. Par ailleurs, certaines informations peuvent être largement suffisantes, même si elles ont été calculées la veille ou il y a quelques heures.  \nCe choix, c’est le vôtre. De plus, aujourd’hui, la capacité des disques n’est plus un facteur limitant pour stocker cette donnée préparée, donc pourquoi s’en priver ?"
        }
      },
      {
        "json": {
          "pageId": "PAGE077_1757157753085",
          "text": "## Optimiser le choix du moteur de calcul\n\nLes données calculées peuvent être créées à plusieurs niveaux :\n\n– dans l’outil analytique : cette option présente le mérite d’apporter l’agilité de dernière minute pour des calculs auxquels nous n’aurions pas pensé ou des besoins trop éphémères pour nécessiter un développement amont ;\n\n– dans la base de données : c’est aussi une puissante base de calcul, soit par le calcul de champs avec des formules que nous pouvons toujours programmer, soit avec quelques techniques particulièrement performantes (voir encadré infra Les procé­dures stockées dans les bases de données) ;\n\n– dans l’ETL : ce dernier peut effectuer de nombreux calculs de préparation. C’est le fameux T de l’ETL. Il peut également appeler des procédures stockées et se reposer sur les données du Master Data Management pour lancer des calculs encore plus complexes tout en conservant l’alignement des résultats avec les standards de l’entreprise. L’ETL peut devenir la bête de somme de l’analytique en effectuant des calculs massifs de préparation en avance de phase. La possibilité de programmer ces calculs à des périodes de faible activité des serveurs permet de limiter l’impact sur les sources ;\n\n– dans des petits programmes ad hoc : les Pandas sous Python. Le langage Python s’est progressivement imposé comme le langage agile et accessible dans les communautés de développeurs. Grâce à la bibliothèque Pandas, il permet de développer très rapidement des modules de préparation, de profilage et d’analyse de données, même sur des volumes de plusieurs millions de lignes. Ces morceaux de codes peuvent même parfois être appelés par les processus d’ETL. Leur aspect pratique et agile est séduisant, mais ne doit toutefois pas masquer le besoin d’un minimum de contrôle et de gouvernance de leur développement. Le risque est de voir proliférer de manière anarchique des bouts de programme partout dans l’entreprise, comme l’ont fait depuis des décennies les fichiers et les macros des tableurs.\n\nFigure 12. Forces et faiblesses des processus de calcul et de préparation de données en fonction de leur positionnement dans la chaîne analytique\n\nLes procédures stockées dans les bases de données\n\nDans les bases relationnelles, nous pouvons créer des petits programmes appelés procédures stockées qui accomplissent des suites d’opérations de manière systématique. L’intérêt de ces modules est qu’ils peuvent effectuer des traitements très complexes au niveau du serveur et donc bénéficier d’une puissance de calcul que nous n’aurions jamais sur notre poste utilisateur. En traitant la donnée granulaire dans la base et en n’envoyant que les résultats sur le réseau, ces procédures stockées permettent aussi de limiter la bande passante nécessaire pour les requêtes de données et de garantir plus de confidentialité en ne transportant jamais les détails des calculs.  \nCes procédures peuvent aussi contenir des boucles de programmation ainsi que toutes sortes de commandes récursives, ce qui leur apporte des possibilités que les outils analytiques utilisateurs ont rarement.  \nElles peuvent être enregistrées sous forme de fonctions avec des paramètres (comme sur tableur où des fonctions peuvent appeler des lignes ou des colonnes), qui peuvent être appelées par la plupart des outils de la chaîne analytique, tels que l’ETL ou les outils analytiques. Pour ceux qui douteraient de leur capacité à traiter des problèmes complexes, ces procédures stockées peuvent même réaliser des opérations très complexes de valorisation d’instruments dérivés, sur des matrices d’hypothèses de taux de change et de volatilité.  \nLes bases de données OLAP, multidimensionnelles et en cube peu­vent elles aussi effectuer des calculs qui s’agrègent en respectant les ordres des opérations, afin de conserver des moyennes, des taux, des variations en pourcentage, quel que soit l’axe d’analyse."
        }
      },
      {
        "json": {
          "pageId": "PAGE078_1757157753085",
          "text": "# Augmenter la valeur des données"
        }
      },
      {
        "json": {
          "pageId": "PAGE079_1757157753085",
          "text": "## Gérer les données maîtres : le Master Data Management (MDM)\n\nLes données maîtres (Master Data) sont un élément essentiel de la donnée. Nous les avons déjà évoquées dans les chapitres sur les systèmes qui aident à les gérer ou encore en abordant certaines de leurs formes telles que les hiérarchies ou les clés d’identification. Un peu à la manière d’un Monsieur Jourdain pour la prose, nous utilisons tous le Master Data sans forcément le savoir.\n\n\\> Définition\n\nLes données maîtres sont un ensemble d’éléments de données de base telles que des hiérarchies, des attributs, des propriétés, etc., que nous utilisons pour décrire un client, un produit, une entité juridique, un compte comptable, un employé, un fournisseur, un canal de marché, un emplacement géographique, etc. Ces données ne sont pas des données de transaction.\n\nFigure 13. Exemple d’une transaction et de ses Master Data\n\nSur cette liste d’achats, nous distinguons clairement les données relatives à la transaction :\n\n– la quantité choisie ;\n\n– la date ;\n\n– le montant total de la transaction.\n\nEn plus de ces données transactionnelles, nous retrouvons :\n\n– le nom de l’acheteur ;\n\n– le nom du produit.\n\nCe sont des données maîtres.\n\nAutant les données de transaction changent à chaque transaction, autant les données maîtres demeurent les mêmes. On peut imaginer une adresse de livraison comme une autre donnée maître : cette adresse pourra certes évoluer dans le temps, mais à des rythmes sans comparaison avec ceux des quantités et du prix total sur les factures émises quotidiennement. C’est pour cela que l’on parle aussi de slow moving data : données qui évoluent lentement, quand on parle de Master Data.\n\n\\> Quelques exemples de problèmes avec des données maîtres mal maîtrisées\n\nLes Master Data sont souvent les héros inconnus de l’analytique, d’où l’importance de leur maîtrise. Voici quelques écueils de leur méconnaissance.\n\nLa non-standardisation des hiérarchies et des groupements de données\n\nImaginons que les tables de groupement continent/région/pays/code postal/ville ne soient pas homogènes au sein d’une équipe de vente. Lors d’une réunion de suivi d’objectifs, nous allons rapidement voir que les mêmes régions sont reprises par plusieurs responsables de zones, que des budgets ont été réalisés suivant une répartition géographique différente d’une équipe à une autre. Dans des entreprises qui évoluent rapidement, ce type d’écart est fréquent. Il peut se produire aussi sur des analyses d’employés et leur rattachement à des équipes ou sur des filiales et leur dépendance à un hub.\n\nLa création de doublons pour un même client, fournisseur, produit, etc.\n\nC’est un classique dans des entreprises décentralisées ou lorsque plusieurs équipes ont accès à la création de nouveaux clients/fournisseurs/produits/entités. Pour des raisons de temps ou en raison de la difficulté de vérifier s’ils existent déjà sous une autre variation, nous allons créer une nouvelle donnée maître, sous un nouvel identifiant, alors que celle-ci a déjà été enregistrée. Ce doublon va par la suite empêcher de faire l’agrégation des analyses au niveau de cette donnée maître, puisqu’une partie des transactions seront rattachées à l’une des données maîtres et les autres ventilées sur un ou plusieurs doublons.\n\nLes raisons sociales de sociétés sont souvent cause de confusion. Nous voyons souvent dans les bases de données clients IBM, IBM WW, International Business Machines, International Business Machine (sans le « s »), IBM Inc., comme des variations de la célèbre marque. Comment peut-on avoir une vue agrégée des transactions si elles sont réparties sur autant de noms ? Nous pouvons imaginer la complexité supplémentaire si cette variabilité dans les dénominations se cumule avec celle d’un pays : par exemple, IBM France, IBM FR, IBM SA France, International Business Machine(s) FR, etc.\n\nSans Master Data unifiant ces noms de clients, nous n’aurions jamais la vision totale de l’activité avec le groupe IBM. Certes, nous pouvons faire ce regroupement à la main sur notre tableur ou dans notre solution analytique, mais lorsque ces duplications apparaissent sur des milliers de clients, cela devient ingérable.\n\nL’enregistrement d’un même attribut sous des noms différents dans des systèmes différents\n\nC’est une des plaies pour la vente multicanale. Lorsque notre paire de chaussures, notre boîte de rangement ou notre tee-shirt sont décrits avec une couleur, une taille ou encore des unités différentes quand nous passons du catalogue au magasin et au site en ligne, comment pouvons-nous être sûrs de ce que nous achetons ? Du côté vendeur, comment pouvons-nous faire des analyses sur ces critères variables d’un canal à l’autre ?\n\nSi nous voulons acheter un modèle de lunettes bleues et que celui dont la photo nous plaît est décrit « Bleu Pétrole » sur le site internet, « Oil Blue » sur le catalogue, que le vendeur du magasin nous confirme au téléphone que c’est le modèle « Deep Blue » qui n’est plus en stock, mais que le « Bleu cobalt » est disponible et qu’il semble être d’après lui de la même couleur que l’image en ligne… comment savoir quoi choisir ? Si nous ajoutons à cela les variantes de langues et les variations de photos, nous avons toutes les chances de choisir la mauvaise teinte.\n\nImaginons la complexité de ce problème pour des rouges à lèvres, pour la plupart… rouges.\n\nLe traitement de ce problème de Master Data est malheureusement complexe. C’est peut-être un des points les plus délicats à gérer en data, car il va nécessiter des solutions technologiques parfaitement paramétrées, des processus définis et clairs, et une discipline des équipes, le tout dans un environnement où chaque jour de nouveaux clients, fournisseurs, produits, etc. doivent être créés et ajustés dans de multiples systèmes et entités en plusieurs langues.\n\nL’importance du Master Data Management et cas illustrés  \n\nSalah Kamel est membre du conseil d’administration de Semarchy, la société éditrice du logiciel xDM, leader sur le marché du Master Data Management, qu’il a fondée et dirigée entre 2011 et 2021. Auparavant, S. Kamel était Platform Architect for Enterprise Infor­ma­tion Management and Integration, responsable de la rationali­sation du portefeuille d’intégration de données d’Oracle incluant Oracle Data Integrator (ODI). Il a été aussi Chief Technology Officer et l’un des premiers fondateurs de Sunopsis, un des principaux fournisseurs de logiciels d’intégration acquis par Oracle en 2006. Avant de rejoindre Sunopsis, il gérait les déploiements mondiaux d’entrepôts de données pour les grandes entreprises et était personnellement impliqué dans l’implémentation de très grandes bases de données.  \nPourquoi j’ai choisi de m’attaquer au défi du Master Data Management avec Semarchy : les problèmes observés dans l’entreprise  \n« Master Data Management » est un terme qui regroupe l’ensemble des initiatives menées par de grandes entreprises pour rationaliser leurs données de référence. Ces données souvent dispersées dans plusieurs applications/systèmes représentent le socle fondateur qui permet à l’entreprise de construire son système d’information transactionnel et analytique. Elles regroupent les données servant à décrire les Tiers (personnes physiques, personnes morales, employés, clients, partenaires, etc.), les Choses (articles, produits, services, actif, etc.), et les Lieux (sites, localisations, magasins, restaurants, usines, exploitations, points géographiques, etc.).  \nCes données de référence sont dispersées et répliquées dans plusieurs systèmes opérationnels (ERP, Finance, Achats, CRM, Marketing, Supply Chain, RH, etc.) et elles sont souvent mal gouvernées malgré leur importance vitale pour toute entreprise. En effet, l’ensemble des décisions prises par les grandes entreprises sont basées sur des indicateurs de performance provenant de divers systèmes analytiques tels que les Data Warehouse et les Data Lakes. Or, ces indicateurs sont agrégés à plusieurs niveaux et peuvent produire des résultats erronés si les données auxquelles ils se réfèrent ne sont pas bien définies et gouvernées. Cette gouvernance, appelée Master Data Management, inclut une variété de disciplines, telles que la modélisation et le stockage centralisé de ces données, le développement d’applications d’édition et de modification de ces données, la définition de processus de gestion du cycle de vie de ces données, l’intégration, le nettoyage, le dédoublonnage et la réconciliation de ces données, l’enrichissement avec des données en provenance d’applications externes, et enfin la distribution de ces données aux autres systèmes consommateurs.  \nLorsque j’ai fondé la société Semarchy en 2011, les grandes entreprises déployaient une énergie phénoménale et un budget colossal pour essayer de mettre en place leurs projets de gestion des Master Data. Prises de court par la course à la digitalisation, ces entreprises ont mis en place des solutions logicielles lourdes, inadaptées, onéreuses et souvent focalisées sur un seul vertical. Elles ont sous-estimé l’importance de l’intégration des utilisateurs métier dans la définition de la gouvernance de ces données, et elles se sont retrouvées coincées par les technologies rigides qu’elles ont choisies, inadaptées à l’agilité requise sur un marché en perpétuel mouvement.  \nSemarchy a alors vu le jour en proposant une offre logicielle complète, multidomaine, agile, incluant des algorithmes avancés d’enrichissement, de dédoublonnage, de mise en qualité, le tout facilement accessible au travers d’interfaces utilisateur intuitives.  \nCas réel  \nLorsqu’une caisse de retraite et d’assurance gérant plus de 10 millions de sociétaires (l’Entreprise) recherche de nouveaux leviers de croissance, elle se doit d’intégrer de nouveaux métiers, tels que la banque privée ou la gestion de patrimoine. Ces leviers sont souvent atteints par le biais de croissance externe via des fusions ou des acquisitions.  \nL’Entreprise qui n’avait que trois ou quatre applications métier se retrouve, après la fusion, à devoir intégrer, gérer et gouverner plus d’une dizaine d’applications, chacune contenant ses propres données Tiers (personnes physiques et personnes morales). En les comptant tous, ces Tiers représentaient 55 millions d’enregistrements ! Bien entendu, et bien que non identifiés, ces enregistrements faisaient souvent référence au même Tiers, avec des informations potentiellement divergentes (doublons). Il n’était alors pas rare, suite à la fusion, que des clients appellent cinq services différents de l’Entreprise, pour effectuer un simple changement d’adresse… Inutile de préciser également que les services métier avaient une connaissance parcellaire et disséminée des clients avec lesquels ils étaient censés interagir.  \nL’Entreprise s’est alors lancée dans un grand programme nommé « Connaissance Client » dont les objectifs principaux étaient : 1) d’améliorer l’expérience client pour toutes ses interactions avec l’Entreprise grâce à un identifiant unique quel que soit le point de contact ; 2) de mettre les données clients en conformité réglementaire (RGPD) ; 3) de réduire les risques de fraude en détectant les doublons de données créés volontairement par certains clients ; 4) de fournir aux services métier une vision à 360° de toutes les interactions et des événements générés par les clients pour détecter de nouvelles opportunités d’upsell (vente incitative) et de cross-sell (vente croisée)."
        }
      },
      {
        "json": {
          "pageId": "PAGE080_1757157753085",
          "text": "Avec une très forte implication des services métier et le choix des bonnes technologies agiles de gestion de données de référence, l’Entreprise a pu mettre en place un référentiel unique centralisé de tous ses clients en moins de six mois. Ce référentiel (ou Data Hub), basé sur la solution logicielle de Semarchy, se charge : 1) de collecter en temps réel l’ensemble des données clients situées dans toutes les applications de l’entreprise ; 2) de les enrichir, de les nettoyer et de les valider via des règles métier complexes ; 3) de les rapprocher avec toutes les autres données clients en vue de détecter de potentiels doublons ; 4) de fusionner intelligemment les doublons en gardant les données les plus pertinentes ; 5) d’attribuer des identifiants uniques à chaque client « consolidé » ; 6) de stocker et de distribuer ces données à toutes les applications Front Office ou Back Office qui sont en interaction directe avec les clients.  \nGrâce à la mise en place de sa gouvernance autour du Data Hub, l’Entreprise a pu bénéficier d’une réduction des coûts opérationnels liés à la gestion des données clients de plus de 25 % et d’une croissance estimée des nouvelles opportunités (upsell et cross-sell) de l’ordre de 15 %.  \nL’ensemble des objectifs de l’initiative initiale ont été atteints. Désormais, l’Entreprise dispose d’une vision à 360° de ses clients, quel que soit le service métier qui la demande. Les risques de non-conformité ou encore de fraude ont été largement réduits grâce au processus de dédoublonnage et à la centralisation des données. Enfin, les clients de l’Entreprise ont un accès unique pour leurs interactions, ce qui facilite grandement leur suivi et contribue à leur satisfaction.  \nLes solutions apportées par le Master Data Management  \nLe Master Data Management est une discipline complexe et qui apporte des solutions incontournables pour toute entreprise qui souhaite tirer profit de ses données. Les critères clés du succès d’une telle initiative reposent en grande partie sur la capacité d’évolution des solutions technologiques pour pouvoir absorber, au sein d’un environnement centralisé unique (Data Hub), l’ensemble des données vitales de référence en vue de les gouverner. Voici quelques solutions métier apportées par le Master Data Management (MDM).  \nRéférentiel client unique (B2B ou B2C)  \nUne grande majorité du temps passé par les entreprises est consacrée aux processus commerciaux qui impliquent les clients : les servir, interagir avec eux, leur vendre et leur fournir les biens et les services qu’ils désirent. Le MDM permet d’augmenter les opportunités de ventes incitatives et croisées, d’augmenter la fidélisation des clients avec une meilleure satisfaction, d’accroître la notoriété de la marque, de diminuer l’exposition aux risques réglementaires, de mieux gérer la vie privée, de détecter les fraudes de manière proactive, de diminuer les risques financiers dus à la méconnaissance client et de réduire les coûts opérationnels, de marketing et d’impacts liés à la non-qualité de données.  \nRéférentiel produit unique  \nLes processus commerciaux des entreprises sont également axés sur les produits et les services : les identifier, gérer leurs informations, les acheter ou les vendre, en faire la publicité, et suivre leurs performances et leur cycle de vie. Le MDM va contribuer à une mise sur le marché plus rapide des nouveaux produits et services, augmenter la confiance et la satisfaction, améliorant ainsi la notoriété de la marque, réduire les risques de non-conformité réglementaire (export, consommation, etc.), réduire l’exposition aux atteintes à la propriété intel­lectuelle, assurer une meilleure traçabilité des produits, accroître la précision, l’exhaustivité et la cohérence des données, assurant ainsi une meilleure visibilité pour les consommateurs, optimiser les coûts de la chaîne d’approvisionnement et réduire les coûts d’organisation interne avec une source de vérité unique.  \nRéférentiel organisationnel  \nLes processus de ressources humaines sont fondamentaux pour les entreprises ayant atteint une taille critique : mettre à jour les informations des employés, définir les parcours d’intégration en début ou en  \nfin de contrat, disposer de la structure organisationnelle à jour (sites, adresses, services, unités fonctionnelles, centres de coûts, etc.). Le MDM permet de mieux gérer les talents, d’améliorer les processus de fusion et d’acquisition, de renforcer une meilleure collaboration entre les employés, de réduire les risques de non-conformité sociale (vie privée, assurance, santé, charte, etc.), de réduire les risques juridiques liés aux contrats des employés, de maîtriser les coûts RH et leur budgétisation, et de réduire les coûts et les risques d’embauche, d’intégration et de départ.  \nLe futur du Master Data Management  \nLe Master Data Management est une discipline indispensable pour les grandes entreprises. Il est le socle sur lequel repose une grande partie de la gouvernance des données. Cependant, de nombreuses entreprises n’ont pas réussi à déployer des projets de Master Data Management en raison de leur complexité, de leur coût et de leur risque lié à l’ambition d’avoir un ensemble unique et accepté de définitions et de données partagées dans toute l’entreprise. Ces échecs sont principalement dus à l’échelle gargantuesque des projets, au manque d’outils collaboratifs pour orchestrer la gouvernance, à l’absence de vision d’ensemble permettant de privilégier les « victoires rapides », et au manque de maturité et d’agilité des solutions logicielles permettant d’outiller l’initiative.  \nDepuis peu, les entreprises qui réussissent leur initiative de Master Data Management se focalisent sur une nouvelle approche appelée « Intelligent Data Hub », qui leur permet d’évoluer au fur et à mesure en intégrant de nouvelles briques de Master Data au fil de l’eau, tout en ayant la possibilité de remettre en cause, à tout moment, les choix d’implémentations passées. La différence entre les solutions d’Intelligent Data Hub et de Master Data Management repose sur la richesse des services proposés par ces nouvelles plateformes logicielles. Outre les services de stockage, d’intégration, d’enrichissement, de dédoublonnage, de mise en qualité et de distribution des données, ces nouvelles plateformes incluent des services de catalogage des données existantes, de référentiel de gouvernance pour définir les sémantiques métier, d’outils analytiques, d’algorithmes de parcours de graphes complexes, de connecteurs applicatifs d’intégration et d’algorithmes précâblés d’intelligence artificielle.  \nGrâce à ces nouvelles plateformes, les entreprises gagnent en agilité et sont ainsi capables de faire évoluer leurs besoins au cours du temps à moindre coût. Elles peuvent « commencer petit et voir grand ». En outre, ces nouvelles plateformes sont souvent disponibles dans le Cloud ou en tant que Service (as a Service), facilitant d’autant plus l’adoption par le plus grand nombre.  \nJe pense que l’avenir de la gestion des données de référence sera largement influencé par et reposera sur des architectures plus évolutives, agiles, impliquant directement les utilisateurs métier. C’est en tout cas la promesse de l’Intelligent Data Hub…  \nMes conseils aux data pionniers  \nLes données de référence, et en règle générale toutes les données de l’entreprise, sont comme des affaires que l’on met dans un placard. Si elles ne sont pas rangées correctement, au bon endroit, lavées régulièrement et entretenues, elles sont inutiles. Pire, elles nous coûtent de l’argent. Cette analogie, je la tiens de la directrice générale des systèmes d’information de Chipotle, une grande chaîne de restaurants américains. Durant un repas de négociation commerciale, elle m’a posé une seule question : « Monsieur Kamel, comment rangez-vous vos placards ? » Interloqué, j’ai répondu : « J’ai très peu d’habits, mais ils sont bien rangés. Les chemises d’un côté et les pulls de l’autre… » Ce n’est que plus tard qu’elle m’a expliqué l’analogie.  \nEn tant que data pionnier, vous allez être amené à créer, à modifier ou à utiliser une très grande quantité de données, et peut-être à définir de nouveaux concepts pour offrir un avantage compétitif à votre entreprise. Les règles que j’ai toujours appliquées sont les suivantes :  \n– d’autres savent mieux que moi : quel que soit le domaine fonctionnel dont vous allez manipuler les données, questionnez vos pairs, les référents métier, sur l’origine des données, leur signification, leur cycle de vie (quand sont-elles créées, modifiées, supprimées ?), leurs habilitations (qui y a accès), leur provenance (sont-elles créées manuellement, achetées, importées ?) ;  \n– avec l’analyse, je peux remettre en cause le statu quo : demandez d’avoir les données disponibles et accessibles au plus vite pour pouvoir effectuer des analyses poussées avec des outils de profilage, des tableaux croisés, des graphiques. Défiez les experts métier lorsque vous trouvez des données qui divergent de ce qu’ils vous ont dit. Proposez des solutions ;  \n– trouver LA définition peut être difficile : chaque personne au sein de l’entreprise a sa propre définition et sa propre sémantique des données qu’elle manipule. Prenez le temps de définir de manière non ambiguë les concepts que vous manipulez. Partagez-les avec les experts. Référez-vous au dictionnaire d’entreprise s’il existe. Immergez-vous dans les processus métier qui ont permis de créer ou de modifier les données que vous manipulez ;  \n– après moi, quelqu’un d’autre peut retrouver la donnée : si je la range bien, là où elle est censée être, n’importe qui la retrouvera et pourra donc l’utiliser à bon escient. Ne créez pas des silos de données supplémentaires, cherchez à centraliser pour mieux partager. Et communiquez, documentez, collaborez…\n\nIl faut vraiment contrôler nos Master Data, parce que c’est la seule manière de pouvoir joindre les données de l’entreprise et de bâtir une base de données cohérente. Se passer de Master Data Management, c’est se limiter à l’analyse de silos de données et se priver du potentiel d’analyse holistique que l’IA ou le Machine Learning peuvent réaliser (voir infra, Le Big Data : ses mythes, sa réalité).\n\n\\> Le processus de Master Data Management\n\nLes techniques de gestion varient d’une organisation à l’autre, mais nous pouvons retenir un ensemble d’étapes clés.\n\nIllustration 30. Étapes clés du Master Data Management\n\nIdentifier et extraire les données maîtres dans les différents systèmes transactionnels avec tous leurs attributs\n\n• Enrichir et normaliser les données\n\nLe but est d’avoir dès le départ une homogénéité des attributs et d’augmenter le nombre de points communs potentiels. Pour cette phase, nous pourrons :\n\n– faire appel à des services externes de validation et de normalisation de l’adresse. Par exemple, trouver l’adresse complète « 123 woodland Avenue » deviendrait « 123 Avenue de Woodland, Paris, 75016 » avec ses coordonnées géographiques ;\n\n– appliquer des formats standards à l’e-mail ou au numéro de téléphone. Par exemple, reformater le numéro français 07-08-76-12-34 en +33708761234 ;\n\n– ajuster les caractères spéciaux et les majuscules. Par exemple, changer « Laura BLAKE\\_ » en « Laura Blake » ;\n\n– utiliser certaines inférences pour compléter des champs. Par exem­ple, pouvoir mettre France ou FR pour une adresse clairement identifiée pour le pays.\n\n• Valider les données maîtres"
        }
      },
      {
        "json": {
          "pageId": "PAGE081_1757157753085",
          "text": "Après la phase de nettoyage et d’enrichissement de la donnée, il faut pouvoir écarter les enregistrements foncièrement mauvais. Ce contrôle de qualité est très subjectif. Nous pouvons le limiter à l’exclusion de données contenant des attributs clairement hors norme qui peuvent venir influencer les prochaines étapes du processus ou qui dénotent de la mauvaise qualité de la saisie initiale. Par exemple, renseigner une date de naissance vieille de 220 ans illustre peut-être le manque total d’attention de la personne qui a rentré cette donnée et peut laisser présager des soucis de qualité sur les autres attributs.\n\n• Relier les données maîtres\n\nUne fois que les données maîtres sont enrichies, nettoyées et filtrées, commence leur mise en correspondance. Nous avons vu que les jointures entre deux enregistrements sont simples à cons­truire… lorsque nous avons des clés communes. Or, ces dernières sont plus difficiles à trouver lorsque les données maîtres viennent de systèmes différents. L’exercice est encore plus délicat quand un même système contient des doublons d’une même donnée maître sous des identifiants différents. C’est pour cela que nous allons utiliser une alternative à la jointure (voir supra, La notion de jointure) : la jointure floue, ou fuzzy matching. Celle-ci s’appuie sur des similarités d’attributs pour associer des données maîtres.\n\nLa similarité entre deux champs peut se mesurer entre autres par :\n\n– le même nombre de caractères ;\n\n– le même nombre de changements de caractères qu’il faut faire pour transformer un champ en l’autre : c’est la distance sémantique ;\n\n– leur même sonorité à la lecture : ce sont les algorithmes de type Soundex.\n\nLa jointure se fait en fonction du pourcentage de similarité. Choisir un taux de 100 % de similarité fait retomber dans le cas de la jointure de clé. Le choix d’un taux entre 1 et 99 % dépend du niveau de confiance ou de tolérance vis-à-vis des données et de leur contexte.\n\nNous pourrons panacher l’application de ces taux de similarité entre deux champs en la combinant avec d’autres tests de similarité sur d’autres champs. Par exemple, nous pouvons développer la logique suivante : si le nom est similaire à 90 %, le prénom similaire à 70 % et l’adresse similaire à 80 %, alors nous décrétons que les deux enregistrements correspondent.\n\nC’est dans la définition de ces taux que notre jugement est lourdement sollicité. En fusionnant deux personnes à qui nous allons envoyer un catalogue de vêtements, le risque est d’oublier un client ou d’envoyer deux catalogues au même. En fusionnant deux patients d’hôpital, les conséquences peuvent être dramatiques humainement et/ou légalement. Le choix de ces méthodes d’agrégation n’est pas compliqué techniquement, il est parfois humainement très délicat.\n\n→ Les jeux du matching et du hasard\n\nLa fusion de bases de données de joueurs de casinos avec des données plus larges relatives à leur séjour peut être problématique. Lorsque des hôtels-restaurants-casinos cherchent à mieux connaître leur clientèle pour cibler des actions marketing, ils fusionnent plusieurs bases de données telles que les réservations d’hôtels, les achats boutiques, les consommations spa, etc., y compris certaines sources externes telles que celle des distributeurs de billets situés dans leurs bâtiments. Cette pratique marketing, a priori classique, peut devenir très risquée dans le contexte de la réglementation actuelle aux États-Unis. En effet, la loi assure à tout individu qui le désire le droit d’être tenu à l’écart des casinos et de leurs sollicitations commerciales à des fins de protection contre les addictions aux jeux. Tout casino doit connaître parfaitement cette liste noire des personnes à ne pas contacter. Mais que se passe-t-il si Paul Dupont, interdit de jeux, est fusionné avec Paul Dupond, un gros joueur, et qu’il commence à recevoir des sollicitations commerciales à son adresse physique ou électronique ? Pour cette simple erreur de « surjointure » (overmatching) le casino risque des millions de dollars d’amende, voire sa fermeture.\n\n• Sélectionner les champs à retenir\n\nUne pratique tenace pousse souvent à choisir un enregistrement parmi ceux retenus à l’issue de cette phase de matching. Nous pouvons être un peu plus subtils dans cette approche en sélectionnant non pas l’enregistrement, mais les attributs gagnants, un par un, en fonction de la fiabilité estimée de leur origine.\n\nSi, par exemple, dans les données maîtres connectées se trouvent plusieurs numéros de téléphone différents, nous allons retenir celui qui vient du call center. Si nous avons plusieurs adresses physiques, celle des systèmes logistique et livraisons a le plus de chance d’être la bonne. L’e-mail aura plus de chance d’être valide, s’il vient des services e-marketing.\n\n• Construire le Golden Record, l’enregistrement maître\n\nÀ ce stade, nous allons enfin pouvoir physiquement ou virtuellement assembler notre donnée maître en une seule référence, issue du meilleur de tous les enregistrements lui correspondant. Nous allons lui adjoindre LA clé unique qui sera reliée à toutes les clés des enregistrements fusionnés. Nous aurons, à ce stade, une super table de correspondance pour joindre ces données dans nos analyses !\n\nQue se passe-t-il si l’on corrige les bases transactionnelles ?\n\nLa tentation existe de réécrire ces données fusionnées dans les bases transactionnelles dont elles sont issues. En effet, si nous corrigeons le problème à la base, les problèmes de Master Data décrits plus haut devraient progressivement disparaître. Malheureusement, cette option n’est pas forcément aussi triviale qu’elle en a l’air. Écrire dans les systèmes transactionnels n’est tout d’abord pas une tâche légère, car ces systèmes doivent conserver la plus haute intégrité. Ensuite, que se passe-t-il pour des transactions en cours sur un client ou un fournisseur fusionné ? Le début commence sur une entité et finit sur… une autre ? Les contraintes varient largement d’une entreprise à l’autre : il faut simplement retenir que ces mises à jour ne sont pas évidentes."
        }
      },
      {
        "json": {
          "pageId": "PAGE082_1757157753085",
          "text": "## Les subtilités et les risques du Master Data Management\n\nUne des raisons pour laquelle les projets MDM sont délicats est qu’outre leur complexité intrinsèque ils présentent des subtilités et des risques éthiques. Un ouvrage entier pourrait être consacré à ces sujets : voici une sélection de points de réflexion pour attiser votre curiosité.\n\n\\> Pareil ne veut pas dire le même\n\nFaire le matching entre deux individus peut devenir vraiment complexe, lorsque nous prenons en compte le contexte géographique.\n\n→ Questions de correspondance\n\nQue vaut la correspondance à 100 % d’un Jean Durand avec un autre Jean Durand à Paris ? Même si elle est parfaite d’un point de vue de la chaîne de caractères, elle risque d’aboutir à une « surjointure » (overmatching) tant le nom est courant en région parisienne. Même en ajoutant la rue, il pourrait y avoir un risque d’avoir une jointure qui marche entre deux individus différents.\n\nPrenons maintenant le nom Noémie De Ficci Lucas à Bennet, Vendée. Noé De Fichi dans la même ville a de fortes chances d’être cette même personne, tant le nom est particulier. Pourtant, la différence est grande entre les deux chaînes de caractères.\n\n\\> Peu d’attributs ne veut pas dire anonymat\n\nEu égard à la vie privée, nous avons parfois la conviction que ne travailler qu’avec un nombre limité de données est un gage de respect de l’anonymat. C’est sans compter sur le pouvoir des jointures et des inférences.\n\nUne étude du professeur Sweeney, de l’université de Carnegie Mellon (voir infra, L’anonymat à l’épreuve du MDM), a mis en évidence qu’il suffit de connaître le sexe, l’âge et l’ethnicité d’une personne pour que, dans certaines communautés, l’identification de la personne soit possible.\n\nImaginons que, grâce à nos capacités de Master Data Mana­gement, nous soyons maintenant capables de croiser suffisamment de données pour constater que, dans tel quartier de telle ville, une femme de tel âge, avec telle origine, ne peut être par élimination qu’une seule personne. Dans le même temps, si, avec un peu de recherche légale (ou non), nous accédons à des données médicales, religieuses ou politiques anonymisées détaillées par sexe, âge et adresse des individus, nous entrevoyons avec quelle facilité nous pouvons maintenant croiser ces données et commencer à bâtir des profils détaillés sans le consentement des personnes.\n\n→ L’anonymat à l’épreuve du MDM21\n\nConsidérons ce tableau. Si les trois enregistrements présentés faisaient partie d’une vaste base de données sur les résidents de l’Illinois, il pourrait sembler raisonnable de supposer que ces trois enregistrements soient anonymes. Cependant, le recensement fédéral de 1990 rapporte que le ZIP (code postal) 60602 consistait principalement en une communauté de retraités dans le Near West Side de Chicago et, par conséquent, très peu de personnes (moins de 12 individus de moins de 65 ans) y vivent. Le code postal 60140 est le code postal de Hampshire, en Illinois, dans le comté de Dekalb. Et deux femmes noires résidaient dans cette ville. De même, 62052 ne comptait que quatre familles asiatiques.\n\nDans chacun de ces cas, l’unicité des combinaisons de caractéristiques trouvées pourrait aider à réidentifier ces individus.\n\nLa loi protège en théorie de certaines de ces dérives, mais elle ne peut pas tout envisager. Conserver une pratique des Master Data éthique est de notre responsabilité.\n\n\\> Processus simple ne veut pas dire évident\n\nUn processus de MDM peut être détaillé simplement, comme nous venons de le faire plus haut. Il est néanmoins riche en subtilités non triviales. Que se passe-t-il par exemple quand nous le prenons à l’envers ? Comment pouvons-nous disjoindre des enregistrements précédemment « matchés » par erreur ? Comment recycler ces données redevenues orphelines pour qu’elles fassent à nouveau l’objet d’une tentative de matching avec les autres données ? Lorsque les conditions de matching sont inadaptées, comment mettre à jour une condition de jointure et réestimer tous les matchs réalisés jusqu’à présent ?\n\nLe traitement de ces points peut rapidement devenir complexe et venir à bout de nombreuses velléités de tentatives à la main ou sur tableur. Le MDM fait partie de ces disciplines en analytique où la technologie joue un rôle incontournable pour la traçabilité des opérations."
        }
      },
      {
        "json": {
          "pageId": "PAGE083_1757157753085",
          "text": "## Établir une qualité holistique de la donnée\n\nNombre de techniques évoquées jusqu’à présent contribuent à la qualité de la donnée, si critique pour des analyses fiables et durables. Ces dernières peuvent être regroupées dans des outils de gestion dédiés qui centralisent les phases clés de profilage et de correction : les solutions de Data Quality. Réaliser le rêve d’une qualité de données parfaite requiert toutefois une approche vraiment holistique du problème. C’est ce qui rend la discipline particulièrement complexe mais également passionnante.\n\n\\> Jouer en mode Street Fighter Style\n\nNous avons tous joué ou vu jouer des gens à ces jeux vidéo de combat un contre un, inspirés des mangas japonais. L’ob­jectif est simple : infliger à l’adversaire des coups qui finiront par le terrasser, avec en plus la possibilité d’engager des frappes exceptionnellement déclenchées lors de certaines séquences de boutons frappées frénétiquement par les doigts. Ces jeux sont accessibles à tous et souvent jubilatoires, car, en tapant comme des fous sur les boutons de la manette, on arrive souvent à des coups magistraux que les graphistes et les animateurs ont l’art et la manière de scénariser.\n\nQuel est le rapport avec la gestion de la qualité de donnée ? À l’exception de la frénésie du jeu (quoique), la pratique holistique de la qualité de la donnée se gagne par l’activation régulière et récurrente de plusieurs leviers.\n\nLes leviers préventifs\n\n• La sensibilisation et la formation continue des personnels à l’importance de la qualité de donnée et sur le fait qu’une donnée de mauvaise qualité va gêner toute une chaîne de traitement, dont ils font partie, car eux aussi vont devoir s’appuyer sur de l’information propre.\n\n• Le bridage des saisies des données dans les systèmes en forçant l’application des formats standards ou par la recherche en temps réel de suggestions de saisie déjà existantes. Les grilles d’entrée de donnée peuvent forcer l’entrée de chiffres uniquement, ou exclure certains caractères. En tapant le début du nom d’un client, le système pourra aussi aller chercher les correspondances potentielles dans le Master Data, ou faire appel à des bases des données extérieures pour suggérer des options.\n\n• Le retour rapide sur les problèmes identifiés. Si nous n’avons pas connaissance de nos erreurs, nous ne pourrons pas les corriger et encore moins les éviter dans le futur. Des rapports de fin de journée dressant un bilan de ce que nous avons saisi, à la manière des rapports de profilage (voir supra, Jauger la qualité de données initiale : profiler la donnée) vont permettre de nous autocontrôler.\n\nLes leviers réactifs\n\n• Le profilage des données reçues dans le décisionnel. L’analyse régulière des données nouvellement saisies permet d’identifier rapidement les problèmes et de les corriger avant que les erreurs ne viennent polluer d’autres processus.\n\n• La correction à la source. Avec une chaîne de contrôle courte, il est facile de retrouver les personnes ayant saisi les données erronées et d’effectuer les correctifs nécessaires dans les systèmes d’origine avant que, par exemple, une transaction soit complètement validée. C’est la meilleure option.\n\n• La correction a posteriori. Il est parfois délicat, coûteux, voire impossible de changer des données déjà impliquées dans des transactions. Afin d’éviter qu’elles ne prolifèrent dans le décisionnel, nous pouvons choisir de les ajuster à l’entrée dans les bases de données. Cela créera un écart avec les bases transactionnelles qu’il faudra documenter, mais économisera le temps précieux des analystes qui n’auront plus ce retraitement à faire manuellement. Il suffira alors de conserver une table d’inventaire de ces corrections que les analystes et les auditeurs apprécieront.\n\nLes leviers de gouvernance\n\n• Une prise de position forte exprimée par le management. Sans cette pression, nous nous concentrerons sur des tâches à meilleur retour sur investissement politique ou opérationnel plutôt que sur la correction de données.\n\n• Un cadre de gouvernance qui définit les actions standards à suivre en termes de saisie et de correction ainsi que les normes à respecter en data.\n\n\\> Se concentrer sur ses besoins\n\nLa quête de la donnée 100 % parfaite est vaine. L’arrivée en continu de nouvelles données ainsi que les erreurs humaines inévitables rendent l’exercice futile. En nous concentrant sur les données qui comptent vraiment pour la gestion de l’entreprise, nous pouvons considérablement réduire le volume à contrôler et à traiter, et augmenter nos chances d’avoir une data de qualité sur les domaines pertinents de nos analyses.\n\nL’imposition de règles de qualité strictes sur des données clés est plus facile à comprendre par les opérateurs et sera mieux acceptée que si elle est présentée comme « faisons de la qualité de la donnée une priorité ». Elle est également plus facile à superviser et à corriger.\n\nPar conséquent, avant de remuer ciel et terre, considérons nos enjeux métier et concentrons nos efforts sur les données nécessaires à leur traitement.\n\n→ Le tonneau de Danaïdes des données clients\n\nDans un espoir louable d’obtenir la meilleure donnée client possible, une équipe Master Data s’était formée dans la direction commerciale d’un grand groupe automobile. Ce projet pharaonique qui devait engager l’achat de logiciels, la redéfinition de processus et un effort général sur plusieurs mois fut arrêté net par le constat du responsable de la division. Que s’est-il passé ?\n\nLa base clients n’avait que 63 % des adresses postales, 34 % des adresses e-mail et 20 % des numéros de téléphone. Avant de courir après la déduplication, l’enrichissement et la correction des autres champs, ne valait-il pas mieux s’attaquer à ces trois éléments en priorité ?\n\nL’essence du marketing et de la vente n’est-il pas avant tout de pouvoir contacter le client ?\n\n\\> Ne jamais s’arrêter\n\nUn autre défaut des grands projets de gouvernance et de qualité de la donnée est que leur taille et leur complexité les rendent très vulnérables aux changements de priorité de l’entreprise. Lorsque la conjoncture change, que les objectifs commerciaux sont plus délicats à atteindre, l’attention et les efforts se détournent vite des activités de gouvernance qui semblent bien éloignées des besoins concrets et immédiats. La mise en place d’une politique de qualité de donnée est nécessaire pour coordonner les efforts, mais, dans sa construction, nous devons tenir compte de sa durabilité dans le temps.\n\n\\> Se souvenir que c’est la bonne décision qui compte\n\nQuand la donnée est-elle suffisamment bonne pour prendre une décision ? Quand nous nous éloignons du transactionnel, quand la tolérance à l’erreur augmente. Le point n’est pas de laisser des erreurs ou des inexactitudes proliférer dans nos analyses, mais plutôt de savoir quel est le degré de précision nécessaire.\n\nPlus que l’erreur d’arrondi ou le manque de certaines données, ce sont les biais d’analyse ou l’exhaustivité de la collecte qui doivent nous inquiéter. Avons-nous toute la data nécessaire pour une analyse du problème et nous sommes-nous posé la bonne question ? Mis à part les cas de mauvaise qualité majeurs, il y a de fortes chances pour que notre donnée soit suffisante pour permettre des décisions rationnelles. Comme beaucoup d’éléments en analytique, ce sont notre expérience et notre pragmatisme qui seront déterminants."
        }
      },
      {
        "json": {
          "pageId": "PAGE084_1757157753085",
          "text": "# Le Big Data : ses mythes, sa réalité\n\nLa data doit beaucoup au Big Data. Sans cette vague, elle serait certainement restée le domaine peu attractif de techniciens. Nous voulions tous être « génération data », mais sans mettre les mains dans une matière jugée complexe.\n\nL’arrivée de sociétés innovantes et fun telles que Yahoo, Google et le chapelet de start-up du début des années 2000, dont la raison d’être était la data, sa collecte, son analyse et sa revente, ont mis sur le devant de la scène une data tout à coup utile, sexy, pionnière, libérée.\n\nIl recommence un projet Big Data !\n\nLa gestion de la data et son application à la finance, à l’audit ou au marketing, vues alors comme une déviance froide et informatique, étaient en passe de devenir une discipline faisant partie intégrante de tout métier. Enfin, la data pouvait s’écrire avec un grand « D ». En parallèle, néanmoins, ce tournant salutaire pour la démocratisation de la data a eu pour effet pernicieux de faire oublier les fondamentaux nécessaires à de bonnes analyses : une data de qualité, bien préparée, répondant à des questions métier. Le Big Data a mis au second plan le pourquoi pour se concentrer sur le comment : nous devions tous avoir un projet Big Data. Mais cet engouement était la parfaite excuse pour se mettre à la data sans risque d’y perdre la noblesse de son métier (même les dirigeants, les grands stratèges n’avaient que ce mot à la bouche) et sans avoir à se poser les questions délicates autour de tout projet data. Faire du Big Data suffisait largement à convaincre ou à impressionner un comité de direction, ses employés, le grand public ou ses amis."
        }
      },
      {
        "json": {
          "pageId": "PAGE085_1757157753085",
          "text": "## Comment définir le Big Data ?\n\nLa définition des analystes a rapidement gravité autour des trois « V », un concept simple à retenir, qui sonnait bien et qui, somme toute, résumait bien la situation. Les trois V représentaient les trois caractéristiques de la data :\n\n– volume ;\n\n– vélocité ;\n\n– variété.\n\nRapidement, le Big Data étant de toutes les conversations et de toutes les conférences, d’autres V ont été ajoutés, tels que véracité ou valeur. Ces deux notions n’avaient rien de neuf pour les praticiens de la donnée : elles n’avaient juste jamais fait l’objet d’une attention particulière jusqu’à présent.\n\n» Vous aussi, ajoutez votre V au Big Data !\n\nReprenons chacun de ces trois V pour mieux comprendre le sujet.\n\n\\> Pourquoi volume et vélocité ont-ils été si moteurs au départ ?\n\nLe Big Data a commencé par l’explosion du volume des données pouvant être traitées. Les données avaient toujours été là depuis la nuit des temps, mais, en l’espace de quelques années, leur émission, leur capture et leur stockage numériques sont devenus possibles. Une gigantesque quantité de données devenait soudain accessible à l’analyse à des vitesses sans précédent.\n\nLa numérisation massive des données s’est faite grâce au développement des capteurs, de l’Internet des objets (IoT, Internet of things), des ordinateurs personnels, des téléphones portables, des appareils photo numériques et des réseaux sociaux. Avant ce tournant, la majorité des données utilisables étaient celles de l’ERP (Enterprise Ressource Planning) de l’entreprise ou de quelques services d’État, de sondage ou de marketing.\n\nEn l’espace de quelques années, machines et humains ont commencé à générer en continu des données digitales. Des progrès technologiques ont aussi propulsé la croissance exponentielle de cette révolution :\n\n– la densité du nombre de transistors, doublant tous les dix-huit mois, permet des calculs toujours plus complexes et massifs. Cette loi, appelée loi de Moore, continue à être valide ;\n\n– l’effondrement du coût du Mo de stockage permet de tout conserver ;\n\n– la bande passante des réseaux autorise des échanges d’informations volumineuses.\n\nÉchange, stockage, traitement : la technologie créait la « tempête parfaite » pour l’avènement des volumes et de la vélocité du Big Data.\n\nCette révolution a déclenché la création de nouvelles organi­sations dont le métier est la revente d’analyses ou de données : Google, Yahoo, Facebook, Twitter, etc. ont perçu l’analyse comme une nouvelle ressource de profits.\n\nLes entreprises des secteurs plus traditionnels, tels que la banque, l’assurance, le commerce, l’automobile, les biens de consommations, etc., faisaient déjà de l’analyse massive de données dans certains de leurs départements comme le marketing quantitatif, l’analyse de risque, le calcul de primes, le relevé de capteurs, etc. Cette vague leur a donné accès à plus d’informations, à plus de capacités de stockage et à un traitement à bas coût, créant une incitation pressante à appliquer le Big Data à tous les étages.\n\n\\> A-t-on vraiment besoin de volume pour être pertinent ?\n\nCe volume de données était-il devenu nécessaire à la prise de décisions, même stratégiques, des entreprises en dehors du secteur du numérique ? Considérer qu’il fallait désormais du Big Data pour répondre correctement aux problèmes s’est souvent révélé une décision coûteuse et peu pertinente.\n\n→ Capteurs sur des voiliers de course\n\nDonald analyse les données fournies par des capteurs sur des voiliers de course ultraperformants. Il a à sa disposition un flux massif de données constamment enregistrées sur tous les points sensibles du bateau. Il considère qu’il ne fait pas du Big Data, juste de la data. Ses arguments sont les suivants :\n\n– j’ai effectivement à ma disposition un gros volume de données stockées. La taille de ces jeux de données est toutefois relative. On est loin des données d’astrophysique ou de télémétrie de fusée ;\n\n– lorsque j’analyse un problème, ce ne sont pas des heures de données de navigation qui vont être pertinentes, mais les dizaines de minutes pendant lesquelles le bateau est dans la situation critique qui m’intéressent ;\n\n– ensuite, de manière générale, je porte mon analyse sur un point précis. Je n’analyse pas « le bateau », mais une pièce ou une zone. Je vais donc juste me concentrer sur quelques capteurs ;\n\n– quand je commence une analyse, j’ai rarement du Big Data. Le Big Data, c’est quand les machines commencent à souffrir. Mon ordinateur portable peut traiter des dizaines de millions de lignes sans difficulté. Cela me laisse beaucoup de marge pour commencer à dire que je fais du Big Data.\n\nLe concept de projet Big Data devrait plus souvent faire place à la résolution de big problems nécessitant, ou non, de gros volumes de données. Commencer par faire du Big Data, c’est mettre la charrue avant les bœufs.\n\nÀ partir de quel volume commence le Big Data ?\n\nSi nous nous sentons mal à l’aise avec la notion de Big (Data) et que nous nous demandons si nous devons passer du côté des technologies avancées dédiées à leur traitement, ramenons nos volumes de données à certaines références telles que le volume de données prévu par le réseau d’antennes de radiotélescopes en Australie : 35 000 DVD par seconde, soit 157 térabits par seconde.  \n\nEnfin, pour les statisticiens, plus de volume ne veut pas dire plus de pertinence statistique. En étendant trop largement les périodes temporelles de jeux de données, nous introduisons plus de bruits, plus d’éléments anciens qui polluent l’analyse de la situation actuelle.\n\nFinalement, si nous nous appuyons sur une question métier précise, que nous préparons notre donnée pour la mettre à la bonne échelle et sur le bon périmètre, il y a de fortes chances que nos jeux de données ne dépassent pas des volumes raisonnables pour des équipements conventionnels. Un domaine de l’analytique recourt néanmoins à des volumes massifs d’informations, bien souvent non structurées comme des images ou de la voix : le Machine Learning, qui va s’appuyer sur le plus de cas d’apprentissage possible pour définir des règles de comportement décisionnelles (voir infra, Les domaines du Big Data).\n\n\\> L’obsession illusoire de la vitesse\n\nAvec les progrès de la technologie et la génération de données digitales en continu par les humains et les machines, l’analyse en temps réel de situation est devenue techniquement possible. Jusque-là concentrée à juste titre sur des domaines transactionnels tels que le suivi de production, la surveillance, la télémétrie de machines ou de véhicules, par exemple, elle s’étend désormais au monde du décisionnel.\n\nDes équipes décisionnelles se sont équipées à grands frais de technologies qui donnent en temps réel des indicateurs de gestion. Les motivations allaient de « plus je colle à mon métier, mieux je le gère » à « les concurrents voisins gèrent en temps réel, il faut donc que nous aussi », ou bien encore « à la dernière conférence, ils ont dit que c’est l’avenir ». Sans remettre en cause le besoin de chacun, qui peut être parfaitement justifié, ainsi que tous les cas d’analyse qui se rapprochent de besoins transactionnels tels que la détection de la fraude, la publicité en ligne ou la mesure de qualité d’une production ou d’un service, nous pouvons nous poser la question de l’utilité du temps réel en gestion.\n\nIl y a toujours des décisions simples qui peuvent être prises de manière quasi automatique et rapide. Mais, de plus en plus aujour­d’hui, l’avantage concurrentiel provient de décisions complexes, fondées sur des revues holistiques. Choisir les meilleures options avec finesse ne peut plus dépendre d’une seule personne (le manager omniscient et omnipotent) et doit plus souvent se reposer sur un travail d’équipe concerté et réfléchi.\n\nOn pourrait également arguer que, si des décisions peuvent et doivent être prises en temps réel, alors, ce n’est peut-être pas le rôle d’un humain : la machine pourra agir non seulement plus vite mais aussi 24 heures sur 24. L’humain conservera une supervision, rythmée par les alertes de la machine, et pourra se consacrer à des tâches plus valorisantes que scruter un écran en temps réel.\n\n\\> Pourquoi cette ruée vers les volumes et la vélocité ?\n\nLes progrès techniques ont donc permis l’utilisation de grands volumes de données à haute vitesse, mais ce n’est pas parce que nous pouvons le faire que nous devons le faire. L’éternelle course au « toujours plus » entre praticiens mise à part, ce mouvement a été largement entretenu parce que volume et vélocité pouvaient être « vendus ». Ce besoin de Big Data, décrété à l’unisson par des éditeurs de logiciels, des fabricants de machines, des consultants, des start-up en intelligence artificielle ou en Machine Learning, la presse et la communauté des conférenciers ont poussé à de nouveaux investissements tous azimuts. En fin de compte, la plupart des problèmes métier n’avaient pas vraiment changé, mais il fallait tout à coup du Big Data pour les résoudre.\n\nLa majorité des défis d’analyse posés par des problèmes complexes n’avaient pas besoin de beaucoup plus d’artillerie lourde. Ce qui avait manqué jusqu’à présent était :\n\n– l’engagement des équipes dans la pratique de l’analytique, par absence d’éducation, de volonté ou de support de leur direction ;\n\n– la capacité du métier à connecter l’ensemble des données nécessaires à la résolution d’un problème du fait de leur nature et de leur variété ;\n\n– la nécessité d’employer de nouvelles méthodes peu maîtrisées par les opérateurs métier pour détecter des signaux dans des situations de plus en plus complexes.\n\n\\> La variété, le Graal du Big Data\n\nIl y a vingt ans, lors de la démocratisation de la Business Intelligence, les analyses qui permettaient d’avoir un avantage concurrentiel pouvaient se limiter à une vue de nos ventes, achats, production, etc., par client, fournisseur, pays, région, produit, année, budget, réel, devise, etc. Humainement, nos yeux, nos doigts (sur le clavier ou la souris), notre cerveau et d’une manière générale nos sens pouvaient appréhender ces données sous forme de quelques tableaux ou graphiques et détecter les points à traiter. Nous pouvions même arguer que notre expérience et notre œil aiguisé n’avaient pas besoin de tout cela pour connaître une situation.\n\nAujourd’hui, l’abord des problèmes avec ces jeux de données limités n’apporte plus vraiment davantage. Ces rapports doivent maintenant tenir compte de la diversité des facteurs nécessaire pour appréhender une situation dans sa globalité et surtout considérer le monde dans toute sa complexité et ses évolutions rapides. C’est dans ces contextes que les approches d’intelligence artificielle ont pris tout leur sens : nous avons besoin de suppléer nos limitations cognitives et cérébrales avec des outils qui peuvent traiter des situations beaucoup trop complexes pour un humain.\n\nDonnées d’hier et d’aujourd’hui\n\n  \nDans cette représentation de ce que pourrait être la donnée d’hier (grisée) comparée à celle que nous pouvons récupérer aujourd’hui pour une même problématique, il est clair que nous pouvons nous contenter de nos sens pour comprendre une logique produit/marché/client/fournisseur, mais que l’ajout des données des réseaux sociaux, du commerce en ligne, des capteurs sur les produits, sur les chaînes de production, de voix du centre d’appels ou de photos rend ce paysage incompréhensible sans l’aide d’outils d’analyse.  \nNous devons considérer l’Intelligence non pas comme artificielle, mais augmentée. Ses algorithmes vont nous donner des superpouvoirs de vision dans des enchevêtrements a priori inextricables de data !\n\nIA + personnes = intelligence augmentée"
        }
      },
      {
        "json": {
          "pageId": "PAGE086_1757157753085",
          "text": "Spécialiste de longue date de l’intelligence artificielle, Gini Rometty, qui a dirigé l’équipe qui a préparé Watson, une IA, chez IBM, voit l’avenir du travail dépendre de la capacité des machines à travailler aux côtés des humains. « Nous résoudrons tant de grands problèmes, mais ce sera un monde d’hommes et de machines », dit-elle. « Ils devront donc travailler ensemble, et cela va signifier beaucoup de changements. »\n\nAvec cette variété croissante des sources et des types de données, les défis de leur jointure ou de la maîtrise de leur qualité se multiplient. L’importance des clés et des Master Data unifiées s’est renforcée : il ne suffit pas de déverser des tonnes de données dans une grande base pour qu’elles s’organisent, s’harmonisent et se connectent par magie. Si chaque émetteur de données doit en augmenter sa maîtrise, les équipes devront aussi bâtir des processus collaboratifs performants, sans lesquels ces visions holistiques de problèmes ne pourront pas être traitées… de façon holistique."
        }
      },
      {
        "json": {
          "pageId": "PAGE087_1757157753085",
          "text": "## Les domaines du Big Data\n\nLe terme Big Data regroupe en sept lettres une multitude de domaines. Les querelles de jargon créent souvent des écrans de fumée qui nous empêchent de les comprendre. Dans un contexte professionnel, sans être statisticien, mathématicien ou analyste, nous pouvons appréhender ces techniques (voir chapitres 2 à 5). En effet, quel que soit le niveau de ces disciplines, nous devrons toujours retrouver :\n\n– la question métier qui doit sous-tendre tout projet analytique ;\n\n– la logique des flux et des briques technologiques qui les supportent : la data sera toujours extraite, préparée, stockée, traitée et représentée en vue d’une action ;\n\n– les éléments de structure de la donnée : les clés, les attributs, les métriques et les données calculées ;\n\n– l’impératif de la qualité de données et des données maîtres.\n\nAvec ces éléments en tête, passons en revue les termes qui se sont imposés en Big Data.\n\n\\> Qu’est-ce que l’intelligence artificielle ?\n\nLa définition exacte de l’IA fait l’objet d’un débat permanent. Selon la personne à qui vous posez la question, data scientists, data engineers, chercheurs, etc., la définition que vous obtenez peut varier.\n\nD’une manière générale, l’intelligence artificielle fait référence à la capacité d’une machine à faire preuve de toute forme d’intelligence afin d’accomplir une tâche donnée. Très souvent, surtout dans le monde des données, nous associons l’IA à l’apprentissage automatique (Machine Learning) et à l’apprentissage profond dans le cadre de l’analyse prédictive.\n\n\\> Qu’est-ce que le Machine Learning ?\n\nMachine Learning fait référence à un groupe de techniques utilisées par les data scientists et les data analystes qui permettent aux ordinateurs d’apprendre à partir des données. Le Machine Learning utilise des principes statistiques pour créer des modèles capables d’effectuer des tâches de prédiction, telles que la classification, la régression, ou de groupement, telles que le clustering.\n\n\\> Qu’est-ce que le Deep Learning ?\n\nDeep Learning peut être considéré comme une forme spécialisée d’apprentissage automatique, par laquelle un modèle est cons­truit pour simuler le comportement du cerveau humain : les réseaux neurones. Il est utilisé pour effectuer des tâches prédictives telles que la classification et la régression, mais, en raison de sa structure complexe, ses applications sont beaucoup plus profondes que celles d’un simple modèle d’apprentissage automatique.\n\nUn exemple d’application du Deep Learning est le développement de voitures sans conducteur, dans lesquelles le modèle doit être capable de détecter des objets tels que les arbres, les feux de signalisation et les limites de la route.\n\nL’IA par ceux qui la pratiquent  \n\nJ’ai demandé à trois experts leur définition de l’IA. Leurs définitions sont assez alignées et très humaines.  \nSarah Aerni, directrice IA et Machine Learning chez Salesforce : « Avec l’intelligence artificielle, il s’agit vraiment de faire en sorte que les machines pensent comme les humains, en leur apprenant à nous faciliter les choses. Lorsqu’elles automatisent des choses très fastidieuses, elles permettent aux humains d’atteindre leur niveau supérieur dans ce qu’ils font. Puis elles leur permettent d’en faire plus, de rendre tout plus intelligent, d’améliorer leur expérience. Dans tous les contextes où nous avons accès aux données, nous pouvons augmenter notre compréhension et notre expérience, et permettre aux humains qui font leur travail au jour le jour de devenir des super humains. »  \nRobert Brown, Vice-Président du Centre pour le futur du travail, chez Cognizant : « Il y a toujours l’idée de la super intelligence, c’est-à-dire l’IA Terminator qui se déchaîne, dans l’imagination populaire. C’est vraiment l’intrigue d’Hollywood. Je pense que là où l’action se situe pour la plupart des entreprises et pour les gens, c’est l’apprentissage automatique. Il est appliqué à de nombreux problèmes pour l’humanité, pour les entreprises et pour chacun d’entre nous individuellement. Et cela offre des possibilités d’atteindre l’excellence dans tout ce que nous faisons. »  \nDan Feld, Director Enterprise Business Devices & Services chez Google : « L’IA, pour moi, est une opportunité commerciale de se développer, et c’est même une opportunité personnelle pour nous tous. Si vous regardez tout ce que nous avons fait jusqu’à présent avec des bases de données et des systèmes basés sur des données dans les entreprises qui étaient des fragments de systèmes et des fragments de valeurs, nous réussissons à donner un sens à tout cela au prix d’un temps précieux, pris aux dépens de tâches à valeur ajoutée. L’IA nous donne l’opportunité de tout mettre à l’échelle, de relier les points et d’avoir un processus beaucoup plus cohérent et rapide pour que nous puissions nous concentrer sur ce qui compte. »\n\nCes techniques, aussi avancées soient-elles, reposent sur les connaissances opérationnelles des équipes de terrain qui auront elles-mêmes approfondi leurs connaissances par leur analytique conventionnelle. Les considérer comme des baguettes magiques, actionnées dans des bureaux clos, en se passant de la réalité de terrain, est un mythe. Si les data scientists se rapprochent progressivement des métiers, ces derniers doivent également appren­dre quelques fondamentaux pour faciliter les échanges et la compréhension mutuelle. C’est que nous allons maintenant aider à faire."
        }
      },
      {
        "json": {
          "pageId": "PAGE088_1757157753085",
          "text": "## Trois grandes approches algorithmiques à connaître pour mieux travailler avec les data scientists\n\nLes mathématiques et les statistiques apportent une aide précieuse lorsque nos sens et nos analyses classiques ne peuvent plus cerner une situation. Nous avons à notre disposition trois classes d’algorithmes qui se rapprochent fortement de nos pratiques intuitives ou manuelles.\n\n\\> Les régressions\n\nLes régressions ont un objectif simple : trouver la courbe qui correspond le mieux à un ensemble de mesures, dans le but de prédire leur valeur.\n\nL’algorithme apprend d’un échantillon de données x donnant chacune un résultat y, et propose une fonction mathématique pour ajuster une courbe qui passe au plus près de tous les points. Cette courbe peut être une droite – on parle alors de régression linéaire –, ou bien une courbe, exprimée par un polynôme – on parle alors de régression polynomiale. Une fois l’équation de cette courbe établie, nous pourrons ensuite calculer, pour tout x, une prédiction lui correspondant avec, pour la régression linéaire, une équation de type y = ax + b, et, pour une régression polynomiale, une équation de type y = anxn + … + a2x2 + ax + c.\n\nIllustration 31. Régression linéaire à gauche et régression polynomiale à droite\n\nNotre expérience, notre œil ou un simple graphe nous permettent d’appréhender intuitivement ces corrélations. Les mathématiques nous aident à valider et à affiner cette perception.\n\nL’exemple de la corrélation de la pointure de pieds avec la taille d’un homme ou d’une femme illustre une certaine corrélation (voir illustration 32). Notons la formule pour prédire la taille de l’individu en fonction de la taille de son pied : y = 3x +53,4 sur la base de l’échantillon étudié (100 personnes). Le fitting de la ligne n’est pas parfait, mais cela donne une idée du fait que plus on est grand, plus on a tendance à avoir des grands pieds !\n\n» Essayez sur vous-même et appliquez l’équation : quels résultats trouvez-vous ?\n\nIllustration 32. Corrélation entre taille et pointure des individus\n\nDes courbes seillantes\n\nUn des points à toujours vérifier lors de l’application d’algorithmes de régression et de classification sont les sous ou surajustements (under et overfitting). Le défaut d’overfitting apparaît quand la courbe tente de trop coller aux données, en prenant en compte des points hors norme et introduisant des biais dans l’analyse à vouloir trop connecter les points intermédiaires. Le défaut d’underfitting se présente lorsque la courbe omet de représenter des tendances et propose un modèle qui ne tient pas compte de certaines subtilités du jeu de données.  \n  \nIllustration 33. Exemples d’overfitting (à gauche)  \net d’underfitting (à droite)\n\n\\> Les classifications\n\nLes modèles de classification ne prédisent pas une valeur continue, mais plutôt l’appartenance d’une donnée à une « classe », un groupe, une catégorie. Les algorithmes de classification vont nous permettre de prédire plusieurs états, tels que « toxique » ou « non toxique », « va acheter » ou « ne va pas acheter », « fraude » ou « pas fraude », « spam » ou « non spam », « survivant » ou « victime ». Ils peu­vent également être utilisés pour déterminer l’appartenance d’une donnée à un groupe particulier tel qu’une espèce ou un groupe démographique.\n\nLes « classifications binaires » testent l’appartenance à deux classes. Lorsqu’il y en a plus de deux, le problème est appelé « classification multiclasse ».\n\nSur un des exemples précédents, nous avions évoqué une liste de clients d’une entreprise de télécommunication. À partir des attributs et des métriques de chacun des clients et d’une base de comportements observés, il est possible à l’algorithme de prédire ceux qui auront une propension à changer de fournisseur d’accès.\n\nIllustration 34. Prédiction du Churn (propension d’un client à quitter son fournisseur) à partir de plusieurs attributs et métriques collectés dans le système client\n\nMême si nous laissons souvent le choix de l’algorithme aux mathématiciens de la Data Science, sachons qu’il existe plusieurs groupes d’algorithmes pour la classification :\n\n– la régression logistique : variation de régression linéaire qui effectue une régression, puis utilise un certain seuil pour prendre une décision de classification ;\n\n– les voisins les plus proches : mesure de distance utilisée pour trouver les voisins d’un point de données, prise de décision de classification à partir de la proximité ;\n\n– les arbres de décision : structures arborescentes où une classification est faite à travers une série de petites décisions qui aboutissent finalement aux feuilles d’un arbre ;\n\n– les forêts aléatoires : interrogation d’un groupe d’arbres, chacun avec une partie aléatoire des données d’entraînement, afin de prendre la décision de classification par consensus ;\n\n– naïve bayes : statistiques bayésiennes appliquées aux données pour prendre une décision de classification.\n\n\\> Le clustering\n\nLes algorithmes de clustering sont différents des deux précédents au sens où ils n’apprennent pas d’un échantillon de données. Leur objectif principal est de découvrir des similarités dans des ensem­bles de données et d’identifier des groupes : des clusters. Ces relations fournissent des informations que nous n’aurions peut-être pas pu forcément voir ou postuler nous-mêmes. Ces algorithmes sont puissants, populaires et fondamentaux. Cependant, ils peuvent être difficiles à interpréter.\n\n→ Des algorithmes sur les iris\n\nUn exemple classique et assez pertinent de clustering est celui du clustering des familles d’iris, rien qu’à partir des caractéristiques de leurs pétales et de leurs sépales. Non seulement l’algorithme arrive naturellement à l’identification de trois groupes, mais aussi, ces groupes correspondent à ceux que les botanistes avaient déjà identifiés. Le graphique ci-dessous montre comment les fleurs mesurées, représentées chacune par un point et ses coordonnées x, y et z, en fonction de la longueur et de la largeur de ses pétales et de ses sépales, se regroupent et se distinguent les unes des autres.\n\nIllustration 35. Groupements d’iris en fonction de la taille de leurs sépales et de leurs pétales"
        }
      },
      {
        "json": {
          "pageId": "PAGE089_1757157753085",
          "text": "## Les biais en algorithme\n\nLa découverte d’un biais dans la sélection des données.\n\nConnaître la classe d’algorithme à utiliser est donc relativement simple. Choisir la version de l’algorithme en question peut être fait par essais successifs ou par consultation avec un expert.\n\nLe principal risque d’un projet de Data Science ne se situe pas à ce niveau. Outre le point de la préparation de donnée et de sa qualité, ce sont les biais que nous allons insérer plus ou moins inconsciemment dans le processus qui peuvent, à eux seuls, conduire à de grosses erreurs dans les résultats ou les interprétations.\n\nLe biais algorithmique est le reflet des préjugés humains dans le développement, la formation et l’application d’un algorithme.\n\nCes biais sont exprimés dans les choix concernant la sélection des données, l’identification des variables et la façon dont un modèle est appliqué. On trouve ainsi :\n\n– le biais de la question : la question que nous posons ne contient-elle pas un biais implicite dans notre préférence pour une réponse facile ? (voir chapitre 2, Comment poser la bonne question) ;\n\n– le biais de sélection : l’échantillon à partir duquel nous formons un modèle est-il influencé par le contexte dans lequel nous appliquerons le modèle ? Avons-nous pris des données suffisamment nombreuses, diverses, réparties ?\n\n– le biais de disponibilité : fondons-nous nos travaux à partir d’un échantillon de commodité, parce qu’il est disponible ? Sélectionnons-nous ou construisons-nous des variables parce qu’elles sont pratiques et faciles à mesurer ou parce qu’elles sont corrélées et en ligne avec notre objectif ?\n\n– le biais de confirmation : le résultat de notre modèle reflète-t-il la réalité ou simplement nos attentes (biaisées) à son égard ?"
        }
      },
      {
        "json": {
          "pageId": "PAGE090_1757157753085",
          "text": "## Pour conclure\n\nLa formidable opportunité des Big Data réside dans cette capacité à pouvoir mieux comprendre des situations complexes grâce à la data. La technologie a fait tomber beaucoup de barrières pour les volumes et des rapidités de traitement : les défis demeurent pour aligner les équipes et la gouvernance des données afin de produire des datas alignées et des dynamiques de collaboration fructueuses.\n\nNous avons vu dans le chapitre précédent que les fondamentaux de la maîtrise de la donnée ne sont pas complexes. Ils reposent sur une logique plus proche du bon sens que de la science dure. La technologie est accessible sans longue courbe d’apprentissage.\n\nPourquoi ne voit-on pas de culture et de maîtrise data s’instaurer globalement et durablement dans les organisations en dehors des équipes digitales, de Data Science, voire informatiques ? Une des raisons est le facteur humain. En plus des connaissances théoriques et pratiques nécessaires, nous devons emprunter de nouvelles façons de raisonner, de travailler ou de diriger.\n\n------------------------------------------------------------------------\n\n14. SQL nous permet d’accéder à des bases de données et de les manipuler. SQL est devenu une norme de l’American National Standards Institute (ANSI) en 1986 et de l’Organisation internationale de normalisation (ISO) en1987.\n\n15. « Se dit d’un logiciel dont le code source est libre d’accès, réutilisable et modifiable (Linux, par exemple). \\[Recommandation officielle : logiciel libre.\\] » (Larousse)\n\n16. Source des données : kaggle.com\n\n17. Tinker M.A., The Effect of Slanted Text upon the Readability of Print, Journal of Educational Psychology, vol. 45, p. 287-291, 1954. Dans Tinker M.A., Bases for Effective Reading, Lund Press, Minneapolis, p. 136, 1965.\n\n18. Tinker M.A., Bases for Effective Reading, Lund Press, Minneapolis, p. 136, 1965.\n\n19. Tinker M.A. et Paterson D.G., Readability of Mixed Type Forms, Journal of Applied Psychology, vol. 30, p. 631-637, 1946. Dans Tinker M.A., Bases for Effective Reading, Lund Press, Minneapolis, p. 138, 1965.\n\n20. Tinker M.A. et Paterson D.G., Studies of Typographical Factors Influencing Speed of Reading : IX. Reduction in Size of Newspaper Print, Journal of Applied Psychology, vol. 16, p. 525-531, 1932. Dans Tinker M.A., Bases for Effective Reading, Lund Press, Minneapolis, p. 137, 1965.\n\n21. L. Sweeney, Simple Demographics Often Identify People Uniquely. Carnegie Mellon University, Working Paper 3. Pittsburgh, 2000.\n\nCHAPITRE 6  \nRelever le défi humain de la data\n\nLa suppression d’Excel dans les processus d’analyse.\n\nLes projets décisionnels laissent souvent de côté le facteur humain, qui est pourtant à l’origine de la plupart de leurs échecs. Si nous considérons les autres éléments de la data :\n\n– les technologies actuelles sont matures ;\n\n– les développeurs sont légion pour résoudre tous types de problème ;\n\n– les programmes, les réseaux et les systèmes d’exploitation sont fiables (souvenons-nous des écrans bleus Windows et des temps de démarrage interminables) ;\n\n– les interfaces sont de plus en plus ergonomiques ;\n\n– les supports d’utilisation sont adaptables à notre contexte (écrans de grande taille, mobiles, écrans tactiles, etc.).\n\nAujourd’hui, les solutions analytiques, bien implémentées, dans un contexte adapté, fonctionnent. Se plaindre d’une solution qui ne marche pas, c’est souvent se tromper de cible pour ne pas blâmer l’équipe qui l’a choisie ou qui l’utilise de manière mal optimisée.\n\nLa data n’est pas davantage coupable des échecs. La data est là. L’élément bloquant est souvent notre incapacité à la numériser, à la capturer, voire à la traiter efficacement :\n\n– soit nous posons des problèmes beaucoup trop compliqués ou utopiques, et effectivement nous n’aurons jamais la donnée fiable suffisante dans des délais et des coûts raisonnables ;\n\n– soit la donnée n’a pas la qualité suffisante, n’est pas dans des formats exploitables ou a été rendue complexe à centraliser, et, dans ce cas… c’est la faute des humains qui la saisissent. Une machine bien paramétrée ne produit pas de la mauvaise donnée, elle produit ce pour quoi elle a été programmée. Une data « est » : sa fluidité ne dépend que de la chaîne analytique ;\n\n– enfin, dans un contexte normal d’entreprise, la data est toujours dans des volumes maîtrisables par les technologies actuelles.\n\nComme dans la plupart des crashs, des accidents ou des piratages, en analytique, la principale source de problème va être… humaine.\n\nL’histoire de la data est pavée d’échecs\n\nCela n’a pas changé depuis près d’une décennie selon plusieurs sources :  \n– juillet 2019 : VentureBeat AI rapporte que 87 % des projets de science des données ne parviennent jamais à la production ;  \n– janvier 2019 : une enquête NewVantage rapporte que 77 % des entreprises indiquent que l’adoption par les entreprises des initiatives de Big Data et d’IA continue de représenter un défi majeur. Cela signifie que trois quarts des logiciels en cours de construction collectent apparemment de la poussière. Aïe !  \n– janvier 2019 : Gartner affirme que 80 % des processus analytiques ne fourniront pas de résultats commerciaux jusqu’en 2022 et que 80 % des projets d’IA resteront de « l’alchimie, dirigée par des magiciens » jusqu’en 2020 ;  \n– novembre 2017 : Gartner affirme que 60 % des projets \\#bigdata ne parviennent pas à franchir les étapes préliminaires. Oups, le cabinet signifiait 85 % en fait ;  \n– novembre 2017 : CIO.com répertorie sept moyens infaillibles d’échouer lors de l’analyse. « Le plus gros problème dans le processus d’analyse est de ne pas savoir ce que vous recherchez dans les données », déclare Tom Davenport, conseiller principal chez Deloitte Analytics ;  \n– mai 2017 : Cisco rapporte que seulement 26 % des répondants au sondage réussissent avec les initiatives IoT (taux d’échec de 74 %).\n\nBeaucoup d’échecs peuvent être évités en prenant en compte correctement le maillon humain. Dans ce chapitre, nous allons voir quels sont les éléments à intégrer dans nos projets pour prendre en compte cette dimension. Nous allons apprendre à les reconnaître et à les nommer.\n\nNous allons considérer différentes clés pour les résoudre, sachant qu’elles ne seront pas exhaustives et 100 % éprouvées, car tellement dépendantes de notre environnement. Un facteur primordial pour réussir à gérer les aspects humains sera avant tout le facteur temps : celui que nous emploierons pour accompagner, persuader ou influencer nos équipes en nous basant sur toute notre expérience et nos qualités humaines de manager du changement. Ce temps, c’est celui que nous allons gagner par la maîtrise des outils et de la donnée."
        }
      },
      {
        "json": {
          "pageId": "PAGE091_1757157753085",
          "text": "# Le Bad Buzz de la data\n\nNous avons vu jusqu’à présent que les principes techniques et data n’ont pas à s’encombrer de termes complexes, encore moins de buzz. Il n’en est pas de même pour les humains : en voulant masquer notre ignorance ou feindre notre assurance, nous allons être tentés d’habiller le domaine de l’analytique de termes ou d’expressions sans base rationnelle. Ces postures vont alimenter d’autres postures chez nos collègues et créer des situations d’incompréhension, de frustration ou de conflit, comme nous allons le voir dans les exemples ci-après.\n\nLes expressions toutes faites mais toutes vides\n\nLes choses que l’on ne veut pas s’avouer"
        }
      },
      {
        "json": {
          "pageId": "PAGE092_1757157753085",
          "text": "# L’effet Dunning Kruger de l’analyste\n\nDunning et Kruger sont deux psychologues américains qui ont étudié le biais de surconfiance, qui, depuis la publication de leurs travaux en 1999, porte souvent leurs noms.\n\nCet effet suggère que moins nous connaissons un sujet, plus nous avons tendance à avoir des avis tranchés et résolus. En d’autres termes, l’humain surestime souvent son niveau de compétence.\n\nEn analytique, cette attitude est particulièrement problématique, car nous allons :\n\n– aborder des nouveaux problèmes avec trop de certitudes. Nous allons négliger des pistes de questionnement au profit d’hypothèses simplistes venant d’un faisceau de connaissances restreint. Les techniques de Design Thinking ou d’aide à la formulation de questions pertinentes vont aider à mitiger cet effet ;\n\n– sursimpliflier le travail des autres. Par exemple, nous allons atten­dre de la magie de l’informatique ou de la Data Science, sans connaître les subtilités et les difficultés de leur métier. Résumer le travail d’un analyste à « il faut juste éviter le garbage-on garbage-out (mauvaise qualité de data en entrée, mauvaise qualité de donnée en sortie) » ou bien celui d’un data scientist à « tu prends juste beaucoup de données et un algorithme, et on va trouver des réponses incroyables » sont des mythes encore bien présents dans les entreprises ;\n\n– surestimer nos compétences en oubliant nos lacunes. La fierté de démontrer notre savoir-faire data, souvent ésotérique aux yeux des autres, et l’accès à des capacités de traitement quasi sans limites nous confortent dans ce sentiment d’omnipotence. Nous le retrouvons même au niveau des opérateurs tableurs qui vous assureront qu’ils pourront continuer à relever tous les défis de la Data Science sur leur outil.\n\nPour éviter ces risques finalement assez fréquents, quatre recettes simples fonctionnent :\n\n– cultivons la diversité ;\n\n– pratiquons l’art de la question ;\n\n– faisons de la recherche des angles morts de l’analyse un moment d’équipe convivial, amusant. Jouer les Sherlock Holmes en équipe peut aboutir à des séances de brainstorming vraiment productives ;\n\n– lisons, écoutons les chercheurs du domaine, les vrais, pas ceux qui agitent les réseaux sociaux de la data. Le problème est que les bons scientifiques sont aussi de très bons pédagogues et peuvent nous renforcer dans notre Dunning Kruger. Allons plutôt chercher les vidéos et les articles au-dessus de notre zone de confort et savourons tous le chemin qu’ils pourraient nous rester à parcourir.\n\nL’astrophysique : l’anti-Dunning Kruger\n\nJe suis devenu passionné d’astrophysique et d’astrophotographie, et chaque nouvelle lecture d’article sur le sujet me rappelle à quel point je ne sais rien, à quel point l’humanité ne sait pas grand-chose. Nous constatons que des découvertes qui nous paraissent évidentes sont en fait très récentes, comme la notion de galaxies par Edwin Hubble qui remonte tout juste à 1924. Il suffit d’ouvrir un magazine pour se rendre compte de l’abondance des découvertes et des remises en cause en continu du domaine. Il m’arrive souvent de finir une lecture et de comprendre que je n’ai vraiment pas tout saisi. Quand je retourne à la data après ce bain de jouvence cognitive, je me sens mieux armé pour maîtriser mes biais."
        }
      },
      {
        "json": {
          "pageId": "PAGE093_1757157753085",
          "text": "# La courbe de changement digital\n\nLe démarrage de la courbe de changement digital.\n\nInspiré des travaux de la psychologue Elisabeth Kubler Ross sur la courbe de deuil, ce mécanisme va constituer un frein à notre engagement et à celui de nos collègues. Il décrit par étapes les phases par lesquelles nous passons tous à l’annonce d’un changement et comment ces étapes vont constituer un frein à l’adoption des nouvelles dispositions.\n\nÀ ce stade du livre, nous sommes probablement bien avancés dans cette courbe du changement vis-à-vis de la data. Mais qu’en est-il de notre entourage professionnel ?\n\nPrenons pour exemple le parcours que nous suivons à l’annonce de la mise en place de nouveaux processus analytiques qui vont s’accompagner de changements d’outils, de méthodes et de responsabilités.\n\nFigure 14. Courbe de changement"
        }
      },
      {
        "json": {
          "pageId": "PAGE094_1757157753085",
          "text": "## L’annonce\n\nLorsque la nouvelle du changement est annoncée, les décideurs en ont souvent fini avec leur propre courbe de changement : par contre, nous n’avons encore aucune idée de ce qui nous attend. Peut-être avons-nous été sensibilisés lors de réunions préparatoires, mais notre courbe ne commencera que lorsque le changement sera annoncé et irrévocable. Les décideurs doivent être à ce stade absolument assertifs et définitifs dans leur communication : le projet aura lieu.\n\nUne fois ce déclencheur activé, nous allons tous suivre avec une célérité et une intensité variables les étapes de la courbe.\n\nRassurons-nous, l’optimisation des processus ne rime pas avec la réduction des effectifs.\n\nDans un contexte toujours plus complexe et changeant, rares sont les entreprises qui pourraient clamer l’atteinte de l’excellence ultime en analytique. Les progrès continus dans ce domaine ne sont que des pas vers un horizon qui recule sans cesse. Les gains de temps doivent être constamment réinvestis pour progresser vers de nouvelles limites. Dans ce contexte, il est souvent contre-productif d’éliminer des collaborateurs riches d’expérience et de connaissance du métier au motif que certaines de leurs tâches peuvent être automatisées. Ce ne sont pas les machines et les nombres qui apporteront l’innovation, la pensée créative et la culture d’entreprise, mais bien ces collaborateurs libérés de nombreuses tâches manuelles et fastidieuses. De nouvelles disciplines doivent venir compléter nos transformations digitales telles que le Design Thinking ou l’Open Innovation (voir annexe Sea Matilda Bez)."
        }
      },
      {
        "json": {
          "pageId": "PAGE095_1757157753085",
          "text": "## La descente irrationnelle\n\n\\> Le choc\n\nCette étape résulte de la combinaison de la surprise, de la peur soudaine et d’un sentiment de perte de contrôle. Les changements entrevus ou imaginés suite à l’annonce ne sont pas encore bien compris et nous n’avons pas encore le niveau d’information, voire de formation, pour les appréhender sereinement. Nous nous sentons dépassés par notre entourage qui est déjà au fait ou qui fait mine de comprendre. Faute de points de repère, nous sommes à court de toute réaction dans l’instant.\n\nCette phase peut rapidement enchaîner sur les deux étapes suivantes. Elle peut aussi se matérialiser plus longuement par un arrêt de toutes choses en cours.\n\nIl n’y a pas de recette pour gérer le choc, si ce n’est d’être le plus affirmatif et définitif possible dans l’annonce pour ancrer la décision comme étant définitive et ensuite accepter avec empathie ses effets. Cette fondation sera nécessaire pour la gestion des étapes suivantes.\n\n\\> Le déni, le refus\n\nNotre réflexe suivant sera de revenir à ce que nous connaissons. L’issue la plus simple et la mieux connue est de ne rien faire et de rester dans notre zone de confort, ne fût-elle pas si confortable que cela. Nous allons donc tout faire pour empêcher le changement, en commençant par nier son existence.\n\nParmi les techniques souvent employées nous retrouvons :\n\n– « j’étais là avant toi » : l’ancienneté et l’expérience vont servir d’arguments pour dénigrer le changement. Qui sommes-nous pour oser bousculer un statu quo qui marchait si bien ?\n\n– « j’ai mon réseau dans la société et ce n’est pas toi qui vas tout changer » : brandir le spectre du réseau d’opposants est une autre tentative d’intimidation. Si nous persistons dans notre décision, nous serons confrontés à une mutinerie des relations de nos équipiers dont nous n’avons pas idée ;\n\n– « on a déjà essayé et cela ne marche pas » : faire l’amalgame de tous les projets passés pour conclure que notre nouvelle tentative ne marchera pas.\n\nComment réagir ? Nous sommes toujours dans une phase émotionnelle dans laquelle nos points rationnels porteront peu. Nous devrons rester sur des postures d’écoute, de pédagogie, mais de fermeté quant à la décision.\n\n» Avez-vous déjà reconnu ces signes d’opposition exprimés lors de réunions data ? Repérez les croisements de bras, les soupirs, les affirmations que tout va bien, qu’il ne faut rien changer.\n\n\\> La colère ou la panique\n\nCe cocktail d’émotions et de perception est le parfait catalyseur pour la phase qui suit, dernier ressort d’expression quand nous perdons pied : la colère ou la peur panique. Lors de cette phase, ce n’est pas notre raison qui gouverne. Nous nous sentons en danger de perdre ce que nous avons construit, ce qui a contribué à notre routine, à notre succès et à notre reconnaissance.\n\nPour le responsable projet, ce n’est pas une phase gérable. Il faut que colère se passe. N’ajoutons pas à la colère en nous laissant emporter. Après tout, cette colère n’est pas dirigée contre nous mais plus contre notre projet.\n\nLe sourire et Antoine de Saint-Exupéry\n\nMes projets data m’ont permis d’apprendre d’experts sur les sujets de gestion des phases de changement. Deux approches m’ont marqué, tant j’ai payé le prix de les avoir négligées et tant elles sont simples à mettre en place.  \nLa première est de conserver le sourire dans les réunions. J’avais été surpris par un consultant qui avait travaillé avec moi sur des projets data très politiques. Alors que les détracteurs du projet l’attaquaient systématiquement lors des meetings de suivi, faisant usage de mauvaise foi et parfois de malhonnêteté intellectuelle, il conservait un sourire, simple, honnête, naturel. Cela avait le don de ne pas alimenter le conflit et conduisait plus rapidement à un retour au calme. Quand je lui ai demandé comment il faisait pour garder son calme, sa réponse fut : « Ce n’est pas à moi qu’ils en veulent. C’est au projet. Moi, je sais que j’ai fait le boulot professionnellement. » Depuis, je fais en sorte de sourire plus et cela contribue invariablement à désescalader les débuts de conflit !  \nLa seconde approche dérive de la première. Ne mettons pas le projet comme un élément de discorde entre nous et nos interlocuteurs. Dans un face-à-face, l’énergie sera invariablement dirigée contre la personne. Si nous nous tournons tous vers un tableau où le projet est décrit et dirigeons nos énergies et nos critiques vers ce tableau, alors ce sera moins à la personne que nous en voudrons. Ce changement de posture peut désamorcer des discussions houleuses.  \n« Aimer, ce n’est pas se regarder l’un l’autre, c’est regarder ensemble dans la même direction. »  \nAntoine de Saint-Exupéry, artiste, aviateur, écrivain (1900-1944)"
        }
      },
      {
        "json": {
          "pageId": "PAGE096_1757157753085",
          "text": "## La transition émotionnelle\n\nLes phases précédentes, défensives et offensives, n’ont pas abouti à faire dérailler le projet. L’initiative progresse. La courbe de changement digital va s’aplatir et entamer deux étapes pendant lesquelles nous allons digérer le changement.\n\n\\> La dépression\n\nL’inéluctable est devant nous. Nous avons un sentiment de défaite, mais également de peur de l’inconnu. Nous allons avoir besoin de nous recentrer, de trouver de nouveaux repères.\n\nPour l’équipe en charge, c’est le début d’une reprise de contact avec des bases plus rationnelles. L’empathie et l’écoute vont être clé pour rassurer nos collaborateurs : un projet data est là pour valoriser nos expériences, pas pour les détruire.\n\n\\> La justification\n\nLe sens du projet va être rationalisé. Une fois le spectre des dangers immédiats évacués (perte de responsabilité, inutilité du rôle), après l’acceptation de la perte programmée de sa feuille Excel ou de ses processus tant chéris, la phase de reconstruction mentale autour du nouveau paradigme de travail commence.\n\nÀ la lumière des promesses d’une analytique bien huilée, les processus actuels apparaissent enfin tels qu’ils sont : douloureux et chronophages. La promesse d’engagements plus responsabilisants et de plus de temps pour soi va contraster avec les tâches quotidiennes ennuyeuses et débilitantes.\n\nSans peindre un avenir trop simple et trop rose, car les initiatives analytiques sont une longue suite d’échecs tendant vers des succès, conforter la raison d’être de ces changements avec sincérité va permettre de prendre ce tournant et d’en sortir en pleine accélération."
        }
      },
      {
        "json": {
          "pageId": "PAGE097_1757157753085",
          "text": "## Le retour au rationnel\n\n\\> La négociation\n\nLe sens du projet va apparaître dans les esprits et nous devons être prêts à nous faire pousser par cette vague d’adhésion.\n\nC’est toute l’abnégation de l’équipe qui rentre en jeu car, pour nous, il s’agit de reprendre la main sur un domaine que nous croyions perdu. Il faut s’effacer pour que les évidences du projet deviennent les nôtres. L’initiative, la demande de participation, de prise en charge de tests ou de définitions de processus doivent à ce stade être sciemment décentralisés.\n\nFaire place libre pour cette motivation est aussi une des clés du succès : qui mieux que chacun d’entre nous pourra optimiser l’application de ces nouvelles approches à son propre domaine ? Si des bases de processus claires ont été définies, nous pouvons les mettre en œuvre au mieux pour nos besoins personnels.\n\n\\> L’engagement\n\nNous y sommes. Peut-être que les décideurs auraient pu accélérer l’adhésion par une communication plus large et plus en avance de phase. Nous nous rendons compte que ce temps non technique et non consacré à la data est absolument nécessaire pour arriver à un résultat. L’engouement des décideurs pour avancer vite et apporter des victoires rapides (quick wins) peut se payer cher au départ d’un projet s’ils n’embarquent pas les équipes.\n\nIl faudra s’armer de patience car, si certains d’entre nous mettent juste quelques minutes, ou quelques heures, pour d’autres ce sera des jours, voire des semaines ou des mois… voire jamais pour accepter le changement.\n\nLa norme du 3-4-3 encore appelée EAO (Enthousiastes, Attentistes, Opposés)\n\nJuste fondée sur l’observation, cette norme peut nous permettre de décoder les postures de chacun de manière simple, sans être toutefois trop simpliste, et donc d’autoévaluer le niveau d’acceptation de notre projet.  \nÀ chaque démarrage de projet data, une minorité sera prête à nous suivre. Avec un minimum de questions, en reposant sur la confiance, ces supporteurs de la première heure vont jouer un rôle primordial :  \n– ils vont nous économiser de l’énergie en management et en persuasion ;  \n– ils vont accepter les premières erreurs et les échecs inhérents aux projets qui commencent ;  \n– ils vont nous apporter nos premières victoires.  \nCe sont les enthousiastes. Sur dix équipiers, souvent deux, trois, voire quatre, s’engagent avec nous sans hésiter.  \nÀ l’opposé du spectre, nous retrouverons invariablement les opposés. Ils sont entre deux ou trois à refuser tout changement ou toute concession. Nous commettons souvent la même erreur à leur encon­tre : la surfocalisalisation. En effet, ils vont prendre beaucoup de notre temps et de notre énergie pour peu de résultats. Ils sont souvent peu enclins à évoluer. Plus ils auront d’attention, plus ils seront écoutés. Dans les étapes initiales, ils risquent de casser des dynamiques fragiles. S’il faut écouter toutes les voix, il faut aussi savoir faire preuve de pragmatisme et penser au projet et à ceux qui comptent dessus.  \nMon expérience m’a appris que, malheureusement, beaucoup de ces batailles pour convaincre les opposés sont vaines à court terme. L’éducation, la patience, le coaching ou l’impulsion très forte d’un leader devront prendre le relais.  \nEnfin, au milieu, nous avons le groupe de trois ou quatre observateurs indécis qui, soit par manque d’informations soit par prudence, attendent de voir comment le projet va prendre forme. Si nos trois enthousiastes sont notre relais et démontrent des résultats, alors ce groupe intermédiaire sera tenté de suivre et, de trois à quatre supporters, nous passerons à six à huit engagés, et nous aurons commencé une adhésion durable.  \nL’adhésion à 100 % est un mirage que j’ai trop longtemps cherché. Une approche plus réaliste type 3-4-3 m’aurait évité bien des soucis, des nuits blanches et des conflits.  \nUne question m’est souvent posée : que faire des irréductibles ? Accompagnement, formation ou positionnement sur des nouvelles fonctions moins exigeantes en digital doivent faire partie des options. L’erreur à ne pas commettre, néanmoins, ne se situe pas dans le déroulé de ces plans d’aide, mais dans le recrutement des prochains collaborateurs. En effet, si nous nous devons d’aider nos équipiers, nous sommes impardonnables si nous continuons à utiliser les critères de recrutement des décennies précédentes. C’est à nous de travailler avec nos ressources humaines pour que les valeurs d’agilité, d’acceptation du changement et d’ouverture aux nouvelles approches data fassent partie des critères de recrutement.\n\nL’intégration de ces étapes du changement va servir de guide pour mieux connaître le niveau d’engagement des équipes et adopter un rythme en harmonie avec ce que l’organisation peut suivre."
        }
      },
      {
        "json": {
          "pageId": "PAGE098_1757157753085",
          "text": "# Le YBG-IBG (JSP-TSP en français)\n\nImaginons-nous discuter avec un collègue ou notre patron. Animés par la passion de notre projet, nous évoquons autour d’un café les formidables opportunités à venir. Alors que nous débordons d’enthousiasme, notre interlocuteur nous coupe en plein élan : Pourquoi te mettre tout ce travail sur le dos ? Pourquoi prendre tant de risques ? Entre toi et moi, dans 18 mois, Tu Seras Parti et Je Serai Parti (You’ll Be Gone and I’ll Be Gone).\n\nAujourd’hui, nous ne faisons plus carrière : l’impact d’initiatives même moyen terme a peu de chance d’être apprécié pendant que nous sommes en fonction. De plus, les mesures de performance sont souvent faites au trimestre par des dirigeants qui eux aussi changent de plus en plus vite. Des projets structurants, inscrits dans la durée ont peu de chance d’être reconnus. Pire encore, avec les pivots opérationnels réguliers opérés à chaque changement de leader, il y aura de fortes chances que nos choix d’hier ne soient plus dans la ligne du parti, voire non compris par la nouvelle garde."
        }
      },
      {
        "json": {
          "pageId": "PAGE099_1757157753085",
          "text": "# L’obstacle du management"
        }
      },
      {
        "json": {
          "pageId": "PAGE100_1757157753085",
          "text": "## Les motivations\n\nÀ la gestion court terme d’objectifs, à la rotation des postes évoquée plus haut va s’ajouter un biais important dans l’attribution des bonus et autres reconnaissances.\n\nLes processus analytiques optimisés vont permettre de gagner beaucoup de temps qui va pouvoir être investi en conversations exploratoires, en discussions de résolution de problèmes, en GEMBA (voir chapitre 2, Comment poser la bonne question), en tests de nouvelles options, voire en temps pour soi. Malheureusement, aux yeux d’un management sensible à l’effort et au temps passé au bureau, ce temps libre et serein ne vaudra jamais des heures en panique, des journées sans fin ou des week-ends au bureau.\n\nLa capacité d’analyse, de simulation et de réflexion va aussi nous permettre d’anticiper ou d’aborder les constantes évolutions de notre environnement. Nouvelles normes comptables, nouvelles réglementations, nouvelles situations à mesurer : si nous maîtrisons la donnée, son traitement et ses flux grâce aux outils et aux méthodes simples décrites plus haut dans ce livre, nous pourrons concentrer nos efforts sur des résolutions en équipe avec de bonnes chances de réussite. De ce fait, nous aurons moins souvent l’occasion de nous présenter en pompier, héros du jour, qui vient sauver une situation perdue, au prix d’un travail acharné. Nous rendons nos tâches plus simples, plus faciles à gérer, et ainsi moins sujettes à l’admiration et surtout à la récompense.\n\n» Qu’allez-vous choisir ? Un modèle de récompense éprouvé, qui marche à coup sûr, ou prendre le risque de livrer des projets innovants qui ne seront pas reconnus à leur juste valeur ?"
        }
      },
      {
        "json": {
          "pageId": "PAGE101_1757157753085",
          "text": "## La transparence\n\nLa transparence n’est pas forcément ce que nous voulons vraiment. C’est une vertu appréciable chez les autres. Rendre l’information fluide et donner à chacun la possibilité d’analyse détaillée est louable, mais cela conduit aussi à changer les rapports de force entre employé et dirigeant. L’omniscience théorique du chef, qui pouvait éventuellement se justifier par une expérience et une formation prestigieuse, se retrouve à la merci de la capacité d’analyse d’une situation d’un collaborateur.\n\n» Allez-vous avoir envie de donner ce pouvoir à l’équipe qui vous entoure, pour qu’ils risquent de voir que le roi est nu ?"
        }
      },
      {
        "json": {
          "pageId": "PAGE102_1757157753085",
          "text": "## Responsabilité et culpabilité\n\nLa montée en compétences de collaborateurs et la création de chaînes d’informations fluides vont également remettre en cause un mécanisme de défense efficace jusqu’à présent : « le responsable mais pas coupable ». Quand la responsabilité qui va avec le titre et le salaire peut être déconnectée de la culpabilité, il est plus facile de se défendre. En cas de problème, nous pourrons arguer que nous ne pouvions pas connaître les détails de ce qui se passait ou que l’on ne nous a pas tout dit. Nous avons une barrière de protection qui fonctionne pour tout.\n\nUne analytique et un reporting efficaces réduisent considérablement ce mécanisme de défense. Nous allons être constamment avertis des événements qui comptent, des problèmes qui surviennent ou de signaux avant-coureurs de crise : nous n’aurons plus d’excuse de ne pas savoir et ne pas avoir agi en conséquence. Nous serons alors responsables et potentiellement coupables.\n\nL’affaire du sang contaminé : responsable mais pas coupable\n\nLe 4 novembre 1991, Georgina Dufoix déclare sur TF1 : « Je me sens profondément responsable ; pour autant, je ne me sens pas coupable, parce que vraiment, à l’époque, on a pris des décisions dans un certain contexte, qui étaient pour nous des décisions qui nous paraissaient justes. »  \nCette déclaration est faite dans le cadre de l'affaire du sang contaminé, un scandale sanitaire, politique et financier qui a touché la France dans les années 1980 à la suite d'infections au VIH par transfusion sanguine. Les révélations qui sont faites sur le comportement des décideurs médicaux, administratifs et politiques font état de graves négligences, de mesures inefficaces, de retard de prise de décision et de la défaillance des milieux impliqués. En conséquence, plus de 550 personnes, hémophiles et patients hospitalisés, ont été contaminées par le VIH.  \nLes protagonistes de l’affaire se défendent notamment en invoquant les incertitudes médicales au début de l’épidémie. Ils plaident la difficulté d’analyser une situation et d’aligner les décisions à l’époque du drame. Si le 20 Décembre 1991, la responsabilité de l'État est reconnu pour « ne pas avoir autoritairement et sans délai mis fin à la distribution des produits confinés » indiquant que « la révélation de catastrophe sanitaire annoncée commandait qu'il soit mis fin autoritairement et sans délais à la distribution de produits sanguins conta­minés. », au final aucun des acteurs de cette affaire ne sera jugé coupable au motif que les connaissances scientifiques de l’époque ne permettaient pas de prendre d’autres décisions."
        }
      },
      {
        "json": {
          "pageId": "PAGE103_1757157753085",
          "text": "# Les dynamiques d’équipe"
        }
      },
      {
        "json": {
          "pageId": "PAGE104_1757157753085",
          "text": "## Les visages de l’opposition\n\n\\> Geoffrey Effraie\n\nComme nous l’avons vu dans la courbe du changement digital, le toboggan émotionnel n’épargnera personne. Pour Geoffrey, c’est la peur de tout perdre qui va primer. En voyant ses outils, ses processus ou ses secrets disparaître avec les nouvelles solutions, il va percevoir les menaces de perdre ses responsabilités, son poste, voire son aura.\n\n\\> Claire Excel\n\nLes tableurs restent LA solution décisionnelle la plus utilisée. Pour beaucoup d’entre nous, c’est un outil que nous avons dû utiliser faute d’alternatives disponibles pour analyser la donnée et compenser sa mauvaise qualité par des modifications de dernière minute. Mais c’est aussi un faire-valoir de poids, tant il permet de produire de tableaux et de graphiques raffinés. Grâce à ses macros, c’est aussi le terrain de démonstration de force : on se démarque avec du code souvent ésotérique aux yeux de l’équipe, qui produit des animations de cellules clairement indicatives d’un niveau de complexité de programmation avancé.\n\nImposer des outils d’analyse et des processus intégrés de préparation de données va rendre obsolète les approches tableurs jus­qu’alors tant reconnues. Pour Claire Excel, c’est l’annonce de la fin d’une ère de reconnaissance et d’admiration. Notre projet ne pèsera pas bien lourd face à ce risque de perte de statut.\n\n→ Je ne l’ai plus jamais regardé de la même manière\n\nPendant mes années parmi les grands acteurs de la Silicon Valley, j’ai été amené à travailler avec de brillants jeunes diplômés, frais émoulus des plus grandes universités américaines. L’attrait de la « Baie » nous permettait de recruter les meilleurs, le « top 1 % » des promotions. Parmi eux se trouvait Claire, une jeune femme aussi brillante que sympathique avec toute l’équipe. Elle s’était imposée par sa maestria sous tableur, au point d’avoir obtenu des PC ultraperformants pour faire tourner ses macros et ses calculs dans des temps raisonnables. Son nom était devenu la référence en la matière et nous avions cessé d’aller chercher sur l’Internet des réponses à des problèmes Excel : il suffisait d’aller demander à Claire. Elle ne faisait pas partie de mon groupe data, elle était nouvellement arrivée dans les équipes finance que je supportais.\n\nCe soir-là, je finissais une longue journée et je m’apprêtais à partir. Claire semblait désespérée face à ses quatre (oui, quatre) écrans de lignes et de colonnes. Un rapport était dû le soir même et plus rien ne marchait. Je décidais de lui venir en aide : au bord de la crise de nerfs, elle ne refusa pas ce support. N’ayant pas le temps de me replonger dans la complexité de son document, je choisis de reculer pour mieux sauter : j’extrayais l’ensemble des données en un jeu de données tabulaires. Je reproduisis ensemble les « recherchev » sous forme de jointures propres. J’appliquais enfin dans ma base de données les précalculs nécessaires. En une dizaine de minutes à remettre d’aplomb son processus, j’avais obtenu une donnée raffinée capable de fournir non seulement les tableaux attendus, et ce, avec plus d’agilité. La rapidité des calculs était sans commune mesure : nous étions passés de dix minutes à quelques secondes.\n\nJe me tournai vers Claire et lui proposai de parler à son superviseur : lui dire qu’elle n’avait pas besoin de finir ce soir dans la précipitation, qu’il valait mieux qu’elle reprenne les choses correctement avec une approche plus orientée data et qu’elle présente ses résultats et ses progrès le lendemain.\n\nAvec un sourire rempli d’empathie et de gentillesse, elle me répondit d’une voix assurée : « Merci Gauthier, c’est vraiment une super approche, mais, tu sais, j’adore mon Excel. » Je ne l’ai plus jamais regardée de la même manière…\n\n\\> Larry Big Data\n\nLarry a les yeux qui brillent : les nouvelles technologies, les jeux de données massifs sont autant de défis qu’il veut enchaîner. Peu importe si les affaires courantes ne sont pas traitées : les succès apparents des projets spectaculaires que peu de collègues comprennent masqueront tout cela.\n\nVenir avec des initiatives pragmatiques et ciblées, qui ne brilleront pas par ce qu’elles sont, mais par ce qu’elles livrent, ne va pas être aligné avec les choix politiques des Larry.\n\n\\> Rose Royce\n\nRose sait que tout échec peut coûter cher en politique. Elle connaît les risques des projets data. Elle connaît les taux d’échec impressionnants. Plutôt que de se former et de monter en compétences, elle va choisir de prendre une assurance : opter pour une solution chère et connue dont on ne pourra pas lui reprocher le choix. Si sa société est réputée, elle sait aussi que consultants et éditeurs feront tout pour lui éviter un échec. Enfin, une implémentation d’un logiciel « Rolls Royce » est un fait de guerre qui aura de la valeur dans un CV.\n\nCes choix fonctionnent avec des managers peu au fait des options ou des collègues partageant la même vision. Ils peuvent conduire à acquérir des technologies beaucoup trop compliquées pour les équipes en place. Ils peuvent créer des approches beaucoup trop lourdes pour l’organisation. Pire encore, parce que le nom ne fait pas tout, ces options peuvent même ne pas remplir des objectifs clés du projet et aboutir à des fiascos monumentaux.\n\nDans le milieu informatique, il y a un viel adage qui a la vie dure : « Quoi qu’il arrive, on ne se fait jamais renvoyer si on choisit SAP, Salesforce ou Oracle. »\n\n\\> Anton Python\n\nAnthon a délaissé Excel depuis longtemps : tout est codage et algorithme, pratiques qu’il maîtrise parfaitement.\n\nLe choix de standardiser, de démocratiser et de pérenniser les processus data ne peut s’accommoder de la prolifération de bouts de codes aux mains d’une personne. Si le Data Wrangling par le langage python est très performant, les risques présentés par la maintenance de ces pages de codes peuvent devenir délicats à gérer.\n\nMettre en place des traitements centraux de la donnée, confier leur développement à des véritables codeurs qui créent des programmes documentés, optimisés et maintenus va constituer une menace pour les Anthon, qui risquent de perdre leurs prérogatives de codeurs.\n\n» Regardez autour de vous pour identifier les rôles que chacun tient ? En avez-vous identifié d’autres ?\n\nÀ la décharge de cette équipe, la proposition de nouvelles pratiques et technologies sonne souvent comme une agression. Ima­ginons un instant que le travail tel que nous l’avons accompli depuis des années soit remis en cause en l’espace d’une annonce lors d’une réunion à laquelle nous n’étions pas préparés. Qu’allons-nous entendre ? « Nous allons tous faire un nouveau projet qui rendra tout ce que l’on fait meilleur » ou bien « le travail que vous avez fait jusqu’à présent ne vaut rien et nous allons vous en donner un nouveau, celui que (moi) j’ai défini, et vous allez voir, il sera nettement mieux ».\n\nVous comprenez que cette perception ne fera pas apparaître vos projets sous des auspices favorables.\n\nLes Pionniers portent invariablement les blessures issues de ces confrontations. On me demande souvent si j’ai compté les coups dans le dos reçus lors de mes projets data. Je suis passé à autre chose, mais j’explique souvent que ce sont des coups de face auxquels il faut s’attendre, tant l’anxiété des collaborateurs peut être forte, même si nous avons les meilleures intentions."
        }
      },
      {
        "json": {
          "pageId": "PAGE105_1757157753085",
          "text": "## Nos superpouvoirs\n\nL’environnement humain aura plus de raisons de s’opposer aux projets data que de les soutenir. Avons-nous des raisons de ne pas nous sentir à la hauteur des exigences des nouveaux enjeux et de nous sentir dépassés ?\n\nLes défis du digital vont se reposer sur la compréhension de nouvelles techniques qui peuvent être apprises. Ils vont aussi reposer sur de nouvelles attitudes qui vont nous permettre d’embrasser le changement. Nous avons tous dans notre humanité ces capacités et il nous suffit d’aller les puiser. Elles pourront être affûtées avec le temps, mais elles ne sont pas liées à une notion de quotient intellectuel ou de niveau d’études.\n\nPour réussir en analytique, nous devrons nous reposer sur quatre super pouvoirs :\n\n– le pouvoir de dire « je ne sais pas » : cela devrait être le point de départ de toute démarche analytique nouvelle. Ouvrir le champ des possibles va permettre d’éviter les biais et les réponses routinières ;\n\n– le pouvoir de dire « pourquoi » : une réponse en analytique doit ouvrir une nouvelle question. Si une fois un problème résolu nous nous arrêtons là, alors bâtir des systèmes et des processus n’a pas de sens. C’est cette dynamique qui pousse au progrès et à l’innovation ;\n\n– le pouvoir de savoir dire « j’ai échoué » : c’est un lieu commun que nous oublions en entreprise : il n’y a point de succès sans une longue suite de petits échecs. Oser échouer est une qualité pour avancer. Il faut juste savoir calibrer ses échecs et éviter les coups de poker trop gros ;\n\nAvant-poste Silicon Valley\n\nPour de nombreuses entreprises, avoir son centre de recherche ou d’innovation en Silicon Valley est un must. Quand vient le choix de qui y envoyer, la sélection se fait logiquement sur les salariés les plus méritants. Mais quelle grille de lecture est utilisée pour ce choix ? Le salarié dont le chemin de carrière a été parfait est-il le meilleur pour le poste ? Grenouiller, gravir les échelons, savoir agir politiquement sont des qualités essentielles au siège social : elles deviennent de véritables obstacles dans un milieu d’innovation et de prise de risque. Ne faudrait-il pas privilégier ceux qui ont essayé beaucoup de choses, échoué, appris, osé ?\n\n– le pouvoir de « vouloir faire » : ce dernier pouvoir illustre la motivation à apprendre pour comprendre et maîtriser. L’analytique d’aujourd’hui ne fait que reproduire des mécanismes ancestraux de collecte de données, de raisonnement et de prise de décision. Les technologies qui soutiennent ces processus ne sont pas complexes et peuvent être comprises, quel que soit le niveau auquel nous souhaitons les utiliser. Nous sommes tous en capacité de pouvoir faire. Acquérir cette confiance va nous permettre d’oser nous engager sur les projets analytiques.\n\n→ Les principes fondateurs du leadership\n\nCes principes fondateurs de la UC Berkeley Haas School of Business définissent les qualités recherchées à la Business School. On retrouve parfaitement, dans l’état d’esprit managérial recherché, les qualités d’un bon analyste.\n\nEspérons que ces états d’esprit se rejoignent dans les organisations de demain pour éviter les antagonismes évoqués dans les paragraphes précédents."
        }
      },
      {
        "json": {
          "pageId": "PAGE106_1757157753085",
          "text": "## La force de notre humanité et de la diversité\n\nL’ultrafocalisation de l’opinion publique, des médias, et des éditeurs et des fabriquants sur les aspects technologiques a fait oublier l’importance clé de l’humain.\n\nLa machine n’innove pas, la machine doit apprendre de l’humain avant d’apprendre elle-même. Si nous ne jouons pas notre rôle créatif et éthique, rien de nouveau ou de bon sortira des algorithmes.\n\n« Les ordinateurs sont inutiles. Ils ne savent que donner des réponses. » Picasso\n\nLe moteur du progrès en analytique peut être à court terme la capacité de traitement ou de mesure de données, mais, sans but intelligent, nous savons que faire de la data pour faire de la data n’a aucun sens. C’est l’apport de l’humain qui va donner son sens à l’analytique : à la fois sa direction et sa signification.\n\nNotre responsabilité est finalement assez lourde dans la programmation de ces machines et dans les résultats qu’elles vont fournir. Dans un monde connecté et complexe, il devient de plus en plus délicat de prendre des décisions pertinentes, justes éthiquement et respectueuses. Nous ne pouvons pas nous seuls porter cette responsabilité. Une équipe d’analystes non inclusive de la diversité de notre société n’est pas équipée pour prendre ces décisions qui influenceront les algorithmes et les interprétations des résultats. Il nous faudra les voix les plus diverses pour non seulement prendre en compte la complexité et la diversité de nos problèmes, mais également pour nous assurer que nos réponses ne soient pas biaisées et discriminantes. Nous n’avons pas la vérité innée : une équipe diverse nous permettra d’apporter une certaine raison dans nos quêtes et nos décisions.\n\nLa diversité est la clé d’une analytique holistique, éthique et non biaisée."
        }
      },
      {
        "json": {
          "pageId": "PAGE107_1757157753085",
          "text": "## Le vrai visage du data scientist\n\nLa fonction de data scientist, décrétée « métier le plus sexy22 » par la célèbre Harvard Business Review a trop souvent été limitée à sa nature mathématique, statistique et Big Data. Cette notion se fonde sur la définition de DJ Patil, data scientist américain, qui définissait un rapport clair avec la notion de sciences.\n\nNous ne pouvons pas tous devenir ce data scientist-là : les compétences en sciences et l’affinité avec les nombres et les formules mathématiques ne sont pas pour tous (et heureusement).\n\nRapidement, les entreprises se sont rendu compte qu’elles avaient besoin d’un autre type de profil, un profil qui aurait peut-être moins de compétences mathématiques, mais plus de connaissances métier, de compétences data, de capacité à « hacker » des problèmes, de qualité de présentateur et de savoir être.\n\nIl est rare que ces qualités soient réunies en une seule personne, et celle-ci serait certainement onéreuse à recruter. En plus, quand bien même l’organisation pourrait attirer de tels talents, la difficulté est de les retenir, tant ils sont convoités et surtout délicats à satisfaire en termes de poste et de responsabilité.\n\nUne approche alternative à chercher ce mouton à cinq pattes va consister à assembler ces qualités au sein d’une équipe avec une diversité de profils. Ce data scientist virtuel pourra être construit par la formation des talents existants suivant les affinités de chacun : certains trouveront un intérêt dans la technologie, d’autres dans les datas, dans l’investigation ou dans les algorithmes et le codage."
        }
      },
      {
        "json": {
          "pageId": "PAGE108_1757157753085",
          "text": "## Les nouveaux champs de développement\n\nEn comprenant que la data n’est pas une fin en soi, et que c’est la libération de nos potentiels de réflexion et d’action qui va compter, nous allons devoir apprendre à dompter de nouveaux mécanismes de pensée et de comportement. Nous avons évoqué plus haut les approches de Design Thinking. Nous verrons comment développer notre potentiel avec l’Open Innovation (voir annexe Matilda Bez).\n\n------------------------------------------------------------------------\n\n22. Davenport T.H. et Patil D.J., Data Scientist : The Sexiest Job of the 21st Century, Harvard Business review, octobre 2012.\n\nCHAPITRE 7  \nConstruire des processus efficaces, légers et durables\n\nL’annonce de la reprise du processus de reporting en début de mois."
        }
      },
      {
        "json": {
          "pageId": "PAGE109_1757157753085",
          "text": "# Le défi des processus lean\n\nLean veut dire léger, fin, élégant. Ce terme est particulièrement adapté pour décrire des processus analytiques performants. En effet, la qualité d’un processus va résulter de plusieurs paramètres dont l’harmonisation va compter plus que la maximisation de certains en particulier.\n\nNous avons vu dans un exemple précédent comment une obsession pour la digitalisation d’un processus tournait au désastre opérationnel par une concentration sur les seuls aspects technologiques (voir chapitre 4, Quelle est l’utilité de la technologie en data ?). Nous avons également constaté l’intégration de solutions « Rolls Royce » qui, tout en étant les meilleures du marché, se montrent particulièrement inefficaces au sein de chaînes de traitements mal adaptées pour ces gros outils. De même que la concentration sur l’éradication d’intervention humaine peut engendrer des complexités techniques qui créent plus de risque qu’elles n’en résolvent, à vouloir laisser à la machine le traitement automatisé de tous les cas possibles, on crée des usines à gaz délicates à auditer et à maintenir, là où l’humain pourrait juste apporter un peu de jugeote.\n\nL’optimisation d’un processus est un exercice qui doit se concentrer sur le résultat global du processus et non ses maillons, car le premier sera toujours limité par la performance, la qualité ou la fiabilité des derniers.\n\n→ Quel processus va finir en premier ?\n\n1\\. Saisie manuelle 5 minutes – Contrôle 20 secondes – Analyses Excel 10 minutes – Reporting 2 minutes.\n\n2\\. Correction des données source 10 minutes – ETL 20 secondes – Analyses BI 5 minutes – Reporting 2 minutes.\n\nLes deux processus finiront en même temps, même si le second est quasiment entièrement digital. Dans cet exemple, le problème est a priori la complexité de la capture de l’information, qui, lorsqu’elle est automatisée, doit se faire sur de la donnée très bien calibrée. Cette phase peut nécessiter un gros investissement temps, alors qu’une capture manuelle peut intégrer cette complexité naturellement et rapidement. L’arbitrage se fera certainement sur la fiabilité d’une saisie manuelle et de son contrôle, comparée à celle d’une préparation manuelle des données avant un ETL normalement 100 % fiable.\n\nL’optimisation de maillons indépendants du reste de la chaîne analytique est tentante, car elle permet à une équipe de livrer un résultat visible et récompensable plus rapidement. Elle ne s’encombre pas des tenants et des aboutissants du processus et surtout évite les politiques. Le data pionnier devra systématiquement arbitrer entre ces victoires rapides pas toujours durables et les initiatives à long terme moins motivantes.\n\nL’utilisation de l’IA pour une optimisation holistique des processus  \n\nWendy Pfeiffer, CIO de Nutanix, élue directrice informatique de l’année en 2019 à Berkeley, travaille de manière très innovante sur le sujet de l’optimisation des processus.  \nJe veux regarder au-delà du simple gain de temps ou de la complexité. Je veux comprendre aussi comment le processus s’insère dans la journée d’un collaborateur pour maintenir un épanouissement et une contribution de valeur de sa part. Le constat est le suivant : réduire une journée à la supervision d’une succession de tâches automatisées n’est pas motivant, voire débilitant. Accélérer tous les processus permet certes de gagner du temps, mais supprime le repos masqué contenu dans certaines tâches : en effet, certaines opérations nous donnent le loisir de prendre du recul, d’entretenir une conversation ou même de « débrancher ». Si nous nous débarrassons de tous ces temps « faibles » ou lents de notre journée, n’embarquons-nous pas notre cerveau dans une course effrénée peu propice à la création ? Je crois à la valeur des enchaînements de tâches qui créent des équilibres dans la journée ou dans la semaine. C’est en variant nos activités et nos rythmes de pensée que nous pouvons voir apparaître de nouvelles choses. C’est en alternant les pensées rapides, instinctives, émotionnelles avec les pensées lentes et plus réfléchies que nous nous donnons la chance de ne pas sombrer dans les biais cognitifs propres à chaque mode de pensée23. Mais comment trouver ces équilibres ? Comment appréhender les métriques permettant de mesurer cette harmonie ? S’il est facile de chronométrer une tâche ou de compter des étapes, comment pouvons-nous capturer le côté profondément humain dans les dynamiques de leur exécution et de leur enchaînement ? Comment ne pas perdre le plus beau potentiel de valeur ajoutée, de créativité et d’innovation qu’est notre contribution en tant que personne ? Afin d’identifier ces mécaniques subtiles, je travaille avec des solutions d’IA qui vont être capables d’identifier ces signaux faibles et d’aider à prendre des décisions plus fines que celles à partir du simple « emporte-pièce » de réduction de temps et de gestes, déjà bien éprouvé par le passé. Les algorithmes vont nous aider à traiter une donnée beaucoup plus variée et représentative d’un contexte de travail, pour trouver l’optimisation entre la productivité pure et la culture de la valeur de chaque individu (voir chapitre 6, La force de notre humanité et de la diversité)."
        }
      },
      {
        "json": {
          "pageId": "PAGE110_1757157753085",
          "text": "# Chasser les processus Creeper\n\nLe terme Creeper fait référence à la plante grimpante ou à la liane qui s’enchevêtre autour d’un corps, ou encore aux créatures qui rampent lentement. Ce sont les processus qui, par leur complexité, leur sensibilité à l’erreur ou leur aspect manuel, nous minent au quotidien. Ils requièrent une concentration inutile et consomment beaucoup de notre temps et de notre énergie.\n\nPire encore, ces processus contraignent les collaborateurs de notre environnement à travailler avec une complexité que nous leur imposons et à eux-mêmes développer des usines à gaz pour s’interfacer avec les mécaniques creeper que nous créons. Progressivement, notre désinvolture dans la création de processus « lianes » peut impacter la performance d’organisation tout entière.\n\nUn exemple classique est celui de la template Excel. Cette pierre d’angle de la majorité des processus de reporting ou budgétaires est souvent créée avec l’objectif d’émuler une application de saisie, de contrôle de donnée, d’analytique et de reporting. Ces créations sont gorgées de formules, de colonnes et d’onglets cachés, de protections et de macros. Elles sont également conçues avec une certaine idée de l’expérience utilisateur. Si elles font la fierté de leurs concepteurs, elles sont bien souvent énergivores et chronophages pour les organisations :\n\n– leur maintenance est complexe et souvent maîtrisée par une seule personne ;\n\n– la saisie des données par les utilisateurs est complexe, répond à beaucoup de règles et se fait de manière manuelle, ligne à ligne ;\n\n– elles ne permettent pas d’import direct et massif de données : il faut passer par la saisie ;\n\n– les capacités d’analyse des saisies sont limitées. La template impose des pivots et des graphiques créés en central qui ne sont pas forcément concentrés sur les points pertinents en local ;\n\n– à vouloir accommoder dans un seul document collecte, stockage et reporting, nous abandonnons les principes FAIR de la donnée. Celle-ci est collectée et stockée de manière compliquée et rarement tabulaire, qui nécessite des mécanismes d’extraction et de consolidation complexes en contradiction avec les bases d’une donnée fluide.\n\nTout cela pourrait être si simple si la donnée était collectée sous forme de table, avec en amont et en aval de la chaîne des analystes capables d’auditer et d’analyser ces données suivant les méthodes que nous avons vues plus haut.\n\n→ La fameuse template des cours de change\n\nC’était un monument pour tous les départements finance de ce groupe mondial : le fichier Excel des cours de référence. Mis à jour chaque mois, ce tableau cosmétisé et optimisé pour une impression irréprochable était envoyé aux trente filiales. À première vue, cette centralisation de la diffusion de la donnée permettait son contrôle et son uniformité globale. Mais cette approche déclenchait des heures de travail dans chacune des trente équipes finance locales. En effet, une fois le document reçu, ces taux devaient être renseignés dans les systèmes locaux pour prendre effet. Mensuellement, c’était la tâche de trente analystes de saisir ligne à ligne ces chiffres, un doigt sur la feuille, une main au clavier. Avec les dix minutes de repointage et les corrections d’erreurs, c’était 30 x 45 = 1 350 minutes, soit plus de 22 heures qui étaient dépensées chaque mois pour une tâche sans aucune valeur ajoutée.\n\nLa simple mise à disposition d’un fichier plat sur le réseau, suivie d’un import ou ETL dans les systèmes locaux, aurait été quasi instantanée et sans risque d’erreur de saisie. Pourtant, cette mécanique de diffusion et de ressaisie a perduré pendant des années.\n\n» Prenez le temps de réfléchir à ces processus Creeper qui ralentissent votre quotidien. Décomposez-les en actions simples, appliquez les principes FAIR de la donnée et appuyez-vous sur des outils adaptés. Des dizaines d’heures d’économie vous attendent."
        }
      },
      {
        "json": {
          "pageId": "PAGE111_1757157753085",
          "text": "# La valeur ajoutée des étapes du processus analytique\n\nToutes les phases d’un processus analytique ne se valent pas. De la question à l’action, vous pouvez découper votre chaîne en neuf grandes étapes, indépendantes de votre industrie ou de votre technologie."
        }
      },
      {
        "json": {
          "pageId": "PAGE112_1757157753085",
          "text": "## L’optimisation des neuf étapes d’un processus analytique\n\n\\> Neuf étapes vers le data nirvana\n\nAfin d’optimiser un processus analytique, vous devez en comprendre ses phases clés. Lors de la revue des systèmes et de la chaîne de traitement de la donnée, vous avez déjà constaté l’existence d’étapes invariables.\n\nFigure 15. Les neuf étapes du processus analytique\n\nComprendre la raison d’être de chacune de ces phases va vous permettre ensuite d’appliquer vos optimisations.\n\nRéfléchir\n\nNous avons vu que les organisations nous poussent à l’action et au mouvement comme synonymes de travail. Dans le contexte de l’analytique, vous ne pouvez pas vous ruer sur la construction de rapports sans au préalable savoir ce que vous cherchez. En faisant cela, vous entrez dans la logique « faire et défaire, c’est toujours travailler » qui peut être coûteuse en temps et en moyen, et qui va vous détourner d’une réflexion posée et pragmatique. Cette phase de réflexion est l’alpha de tout processus décisionnel. Elle va aboutir à la question pertinente qui vous servira de guide tout au long de la chaîne d’analyse.\n\nUne fois la question posée de manière pertinente, le processus peut continuer.\n\nIdentifier\n\nCette phase consiste à identifier les données nécessaires à une analyse complète et juste. Le choix des informations que vous allez collecter va influencer non seulement la largeur et la profondeur de votre analyse, mais aussi sa pertinence. En vous ruant sur des indicateurs mal choisis, vous pouvez obtenir des informations fausses ou, pire encore, insidieusement biaisées.\n\nCapturer\n\nL’objectif est de collecter de manière efficace et durable. Vous allez devoir avant tout chose sécuriser des sources de données numérisées et fluides. Si vos sources sont sur des formats papier, il faudra les mettre au format électronique (sous peine de consacrer des heures de ressaisie manuelle). Une fois cette phase essentielle réalisée, vous allez créer les processus de collecte qui vont alimenter votre chaîne d’analyse. Il s’agit à ce stade de créer des mécaniques capa­bles de récupérer votre information rapidement, de manière répétée si besoin et peu sensible aux fluctuations de volumes.\n\nPréparer\n\nC’est 80 % du travail du Pionnier de la data et de tout analyste en général. Les phases de profilage, de nettoyage, d’alignement, de modélisation, de précalcul sont essentielles pour rendre vos jeux de données propices à des analyses rapides, justes et pertinentes (voir chapitre 5, Tirer le meilleur de sa donnée avec une bonne préparation).\n\nStocker\n\nLe stockage va consister en la mise à disposition de votre donnée raffinée pour vos propres analyses ou pour des communautés plus larges d’analystes. Vous allez choisir les modes de stockage les plus adaptés à vos contraintes de volume, de rapidité et de complexité de calculs ou d’agilité de maintenance.\n\nAnalyser\n\nSur la base d’une donnée organisée, vous pouvez vous concentrer sur la recherche des réponses ou la validation de vos hypothèses. L’ajout de dernières retouches de calculs ou d’agrégation va vous permettre de détecter en tableaux ou en graphiques dynamiques ce que vous cherchez.\n\nPrésenter\n\nUne fois les éléments clés identifiés pour formuler votre réponse, vous allez devoir créer le média qui correspondra le mieux à votre audience. Les options sont multiples et ce qui doit vous guider, c’est l’impact. Sans une prise de décision ou une action, les six étapes précédentes n’auront servi à rien.\n\nCommuniquer\n\nIl s’agit de porter le message à votre audience et de le livrer de la manière la plus concise possible. Ce qui compte, c’est l’étape qui va suivre.\n\nAgir\n\nNous y sommes enfin. La promesse de « la visibilité pour l’action » (insight to action) enfin réalisée, le vrai travail commence. Votre expérience, votre expertise, voire votre intuition, qui avaient été clés à la première étape, se retrouvent de nouveau au cœur de cette phase. Votre contribution en tant que professionnel réfléchi, créatif ou inspiré va vous permettre de prendre les (bonnes) décisions et de mettre en œuvre les bonnes actions.\n\n\\> Les appuis techniques et méthodologiques à chaque étape\n\nMaintenant que vous avez détaillé les étapes de production de vos analyses, comme sur une chaîne de production industrielle, vous allez pouvoir réfléchir aux optimisations de chacune d’elles, en gardant en tête que votre processus ira toujours à la vitesse de son maillon le plus lent.\n\nLa priorité est de classer les tâches selon le niveau de valeur que vous apportez en tant qu’humain. Dans la figure 15, les étapes qui nécessitent votre contribution sont identifiées par les blocs avec les petites silhouettes.\n\nPour l’ensemble des phases, vous pouvez augmenter vos capa­cités en vous appuyant sur la chaîne technique décisionnelle et les méthodes décrites plus haut.\n\nOù apportons-nous une vraie valeur ajoutée ?\n\nParmi les phases qui nécessitent notre engagement, nous retrouvons la réflexion, ainsi que la décision et l’action. Même si nous automatisons le processus de décision et les actions qui suivent, n’oublions pas qu’à la base la machine devra être programmée pour suivre nos directives. Si nous négligeons la logique que nous codons dans la machine ou bien si la donnée que nous apportons aux algorithmes est biaisée, alors les décisions et les actions seront mauvaises.\n\nLa manière avec laquelle nous allons préparer nos datas est aussi clé pour la qualité de notre analyse.\n\nL’analyse à suivre est évidemment là où nous allons exprimer pleinement notre esprit critique.\n\nEnfin, pour la présentation de nos résultats, ce sont plus nos capacités de communicant ou de pédagogue qui vont être mises en tension.\n\nDes gains de performance seront immédiatement effectifs par la simple application de briques technologiques ou méthodologiques décisionnelles (voir chapitre 4, Comprendre les solutions BI ou décisionnelles) :\n\n– le Design Thinking va vous aider à formuler la bonne question en prenant en compte le maximum d’options et en limitant vos biais cognitifs ;\n\n– le Design Thinking encore va vous guider pour définir les meilleures options pour vos décisions et vos actions ;\n\n– la réflexion large, la collaboration inside out et outside in proposées par les approches Open Innovation va vous procurer l’inspiration et les idées dont vous allez avoir besoin pour mettre en pratique au mieux les conclusions de vos analyses ;\n\n– l’inclusion de la diversité dans vos équipes va vous aider à formuler vos questions de manière plus juste et moins influencée par vos biais culturels et sociaux ;\n\n– la technologie va également décupler votre puissance de feu :\n\n• pour la préparation et la modélisation de données, vous allez disposer des fonctions de profilage de vos solutions analytiques ou de certains langages tels que le Python ou R que vous pourrez apprendre progressivement pour devenir la Data Avant-Garde ;\n\n• pour l’analyse, les outils de Business Intelligence qui vont vous permettre de travailler sur des millions de lignes en quelques clics de souris ;\n\n• pour la présentation claire et impactante de vos conclusions, les solutions de visualisation avancées, souvent incluses dans les applications de Business Intelligence, vont vous permettre de visualiser de façon simple des contextes complexes ;\n\n• le Python ou le R vont venir briser les plafonds de volume et de complexité qui vont arriver aux limites des solutions traditionnelles. Soyez néanmoins rassuré, les solutions décisionnelles viennent sans difficulté à bout de plusieurs millions de lignes sur un ordinateur standard ;\n\n• la préparation de données va être prise en charge par des ETL agiles. Vous pourrez faire appel aux ETL couramment utilisés dans l’entreprise, s’ils sont accessibles et utilisables par des utilisateurs métier, aux ETL orientés utilisateurs tels qu’Alteryx, aux ETL intégrés dans les outils de Business Intelligence comme dans Pyramid Analytics, PowerBI ou Tableau Prep. Vous pourrez également automatiser vos traitements manuels par l’application de mini-programmes codés en SQL, Python ou R ;\n\n– une autre partie de la préparation, la modélisation de vos données, va pouvoir aussi bénéficier de solutions techniques. Si certains systèmes de bases de données ou leurs outils de gestion permettent la mise en œuvre rapide de modèles grâce à des interfaces graphiques, la plupart des outils BI intègrent la possibilité de développer en local des modèles orientés métier, dans l’attente d’une industrialisation par les services informatiques ;\n\n– enfin, pour l’homogénéisation, la normalisation et la mise en conformité de vos données, dernière facette de la préparation, l’ETL accompagné d’une solution de MDM ou de gestion de qualité de données va pouvoir prendre en charge une grosse partie des transformations de masse.\n\n» Prenez un peu de recul sur vos processus actuels et considérez combien de temps vous pouvez gagner.\n\nOù n’apportons-nous aucun réel avantage ?\n\nLa capture et le transport de la donnée de nos bases sources à nos bases décisionnelles (fichiers texte ou bases de données) sont une tâche mécanique que la machine pourra faire beaucoup mieux que nous.\n\nLe stockage de nos données, leur archivage, leur mise en commun ne sont pas non plus un bon investissement de notre temps.\n\nEnfin, la transmission d’un résultat, qui ne doit pas être confondue avec la discussion qui suit la réception du message, relève également d’une action assez simple et répétitive. Au sein d’une entreprise connectée, passer des heures à envoyer des rapports n’est plus acceptable.\n\nLa bonne nouvelle est que nous avons dans la pile des systèmes décisionnels des gisements d’automatisation et de productivité énormes :\n\n– vous l’avez certainement deviné, l’ETL va encore jouer un grand rôle dans la phase de capture de données. Il va vous permettre de récupérer des volumes massifs, aux heures choisies, de systèmes sources à des systèmes cibles en conservant l’intégrité de vos données ;\n\n– la base de données est bien évidemment l’outil parfait pour centraliser, conserver et mettre à disposition vos informations ;\n\n– enfin, les solutions de Business Intelligence intègrent aujourd’hui de nombreuses options qui vont éviter les longues listes de distribution d’e-mail :\n\n• l’accès à des rapports et à des tableaux de bord en ligne ;\n\n• la mise à disposition des classeurs d’analyses en tant que tels pour donner non seulement les résultats des analyses, mais aussi leurs détails ainsi que la possibilité de rebâtir si besoin des tableaux complémentaires. Les classeurs PowerBI ou Tableaux contiennent par exemple la donnée et les analyses, et permettent à une tierce partie de retravailler si besoin sur les chiffres ;\n\n• et si l’audience est toujours en attente de documents attachés à des e-mails, nous pourrons utiliser les fonctionnalités de diffusion automatisées de rapports filtrés des outils de BI sur le périmètre de responsabilités de chacun.\n\nDes gains de temps immédiats\n\nMis à part pour les phases de réflexion et d’action où l’arbitrage vitesse/qualité varie en fonction des urgences et des risques à gérer, imaginons les gains de temps que ces approches peuvent apporter à court terme. Ces chiffres reflètent de manière conservatrice mon expérience au contact des équipes que je forme.\n\nTableau 9. Temps comparés entre un processus tableur et un processus s’appuyant sur des outils et des pratiques analytiques FAIR\n\nEn général, les cas que je rencontre sont encore plus exemplaires.\n\n→ Le facteur 10\n\n– une société pharmaceutique, qui a formé des pionniers comme vous, a en l’espace de deux mois réduit de 10 heures des processus analytiques qui prenaient chaque mois 12,5 heures par la simple application des approches décrites dans ce chapitre, sur la base des compétences acquises dans cet ouvrage ;\n\n– un groupe automobile a transformé un processus d’une journée en un processus d’une heure en appliquant simplement les bonnes technologies sur une donnée rendue tabulaire ;"
        }
      },
      {
        "json": {
          "pageId": "PAGE113_1757157753085",
          "text": "– sur mes postes de contrôleurs, nous mesurions une division par dix des temps de traitement de processus d’analyse tableur, par l’application des approches FAIR, des bonnes technologies et par le courage de changer.\n\n→ La règle des 20-20\n\nÀ la suite d’un atelier dans une institution bancaire, j’ai observé une réduction de soixante fois le temps d’un processus analytique, passant de 20 heures par mois à 20 minutes. Imaginez que c’est comme participer à un marathon de 42,2 km et obtenir un avantage de 41,5 km sur ses pairs (ceux qui ont des processus manuels et basés sur des feuilles de calcul). Même le meilleur athlète ne pourra jamais nous battre. Avec une telle avance, nous aurons le temps de réfléchir à notre foulée, à notre respiration, de faire des selfies et de profiter de l’ambiance de la foule de supporters. Non seulement nous gagnerons à chaque fois avec une avance écrasante, mais nous aurions en plus le loisir de travailler à peaufiner notre avantage compétitif sur les prochaines compétitions. Cela semble presque injuste, n’est-ce pas ?\n\nDans le cas en question, Virginia, notre héroïne, était chargée de superviser la qualité des rapprochements bancaires de trente filiales du groupe à travers le monde. La diversité des systèmes et l’absence de normalisation des processus ne permettaient pas d’effectuer automatiquement un rapprochement complet en central. Même si elle était consciente que l’analyse n’apportait pas beaucoup d’informations ou de feedback opérationnel, elle devait la fournir chaque mois.\n\nPour sortir de cette routine peu valorisante, ce que Virginia a mis en place n’a rien de sorcier. Comme beaucoup d’analystes, elle n’avait jamais suivi de formation aux bases de la data. Elle a choisi d’acquérir les compétences « Pionnier » et a surmonté son appréhension du changement ainsi que son attachement aux tableurs qui faisaient partie de son quotidien.\n\nMais comment a-t-elle fait pour réaliser cette spectaculaire optimisation ? Comment a-t-elle pu passer de 20 heures de production de rapports systématiquement en retard à des analyses proactives établies en 20 minutes ?\n\nElle a transformé trente feuilles de calcul avec des onglets mensuels en une seule table de données, avec des dates et des identifiants bien définis. Elle a mis en place une collecte automatisée des données lorsque cela était possible ou a exploité les échanges de fichiers plats lorsque les interfaces étaient trop complexes à programmer. Elle a conçu des rapports automatisés qui pouvaient ensuite être envoyés non seulement à la direction de manière agrégée, mais aussi à chaque filiale comme aide à l’exploitation et à la décision. Elle a commencé à sécuriser d’autres données en rapport avec les entrées et les sorties des comptes bancaires, telles que les pays d’origine et de destination ou les personnes émettrices en interne. Elle a naturellement commencé à poser de nouvelles questions sur les risques, la fraude et la conformité, et a apporté une véritable valeur à son travail.\n\nEn quelques semaines à peine, elle avait déjà repoussé les limites de ses analyses et exprimait déjà le besoin d’appliquer des algorithmes de classification à son trésor de données.\n\nVirginia a découvert que les données sont comme n’importe quel matériau. Elles sont collectées sous différentes formes et doivent être transformées, assemblées et livrées à un destinataire. Elle a appliqué les principes de base qu’il faut connaître pour traiter les données correctement et avec efficacité, tels que :\n\n– la structure de la chaîne d’approvisionnement des données, du producteur au consommateur ;\n\n– la notion de tables, de clés et de jointure ;\n\n– les concepts de faits, de dimensions et de données de base ;\n\n– les bases de la gestion des données de base, de la transformation et de la qualité des données ;\n\n– l’usage des ETL pour capturer les bonnes données et fournir une préparation automatisée ;\n\n– les bases de données relationnelles et la modélisation des données ;\n\n– la gestion des Master Data.\n\nElle s’est ouverte avec courage à de nouvelles solutions. Jusqu’à présent, les feuilles de calcul avaient été son seul atout pour tous les défis analytiques. Elle a démystifié le battage médiatique autour du Big Data et a clairement identifié ce dont elle avait besoin d’un point de vue métier.\n\nElle est devenue une force de proposition pour les projets analytiques, car elle arrivait non seulement avec des questions métier claires, mais aussi avec des données classées et connectées qu’elle avait accumulées pour son analyse principale.\n\nVirginia avait entendu parler de la règle des 80/20 entre le traitement et l’analyse des données. Elle venait d’en créer une nouvelle : la règle des 20-20, où 20 heures d’analyses manuelles fastidieuses se transforment en 20 minutes d’informations à forte valeur ajoutée !\n\nRespectons-nous\n\nUn déclencheur de rationalisation de processus peut être aussi de poser un nouveau regard sur l’exécution de nos tâches quotidiennes en analytique.\n\nEn concentrant notre temps sur nos tableurs et nos processus manuels, la majorité de nos actions va consister à enchaîner des tâches peu valorisantes et passionnantes. Comme me l’avait décrit un confrère, non sans une pointe d’humour, une fonction d’analyste se résume assez bien par :\n\n– des copier-coller de formules et de données. Cette discipline est souvent le sport national en analytique. Cette aérobic pour nos phalanges n’est pas la meilleure utilisation de nos talents ;\n\n– des calculs relevant du cours élémentaire qu’avec application nous recopions tous les jours au travers de nos jeux de données ;\n\n– du coloriage glorifié représenté par nos longues séances de cosmétisation de tableaux et de graphiques, comme à l’époque de nos ateliers de peinture et de dessin à la maternelle.\n\nEst-ce raisonnable après des années d’études ou d’expérience d’en arriver là ? Aussi satisfaisantes ou rassurantes puissent être ces tâches, il y a de fortes chances qu’elles ne soient pas en phase avec notre fonction, notre salaire et notre amour-propre.\n\nSommes-nous prêts à quitter ce confort sclérosant et auto­limitant ?\n\nOsons le repositionnement homme/machine\n\nL’application de technologie à une donnée fluide n’a pas d’effet magique automatique. Un outil et une donnée FAIR ne feront gagner du temps que s’ils sont positionnés et utilisés correctement.\n\nSouvent, nos processus ont été bâtis avec des contraintes et des connaissances différentes de celles d’aujourd’hui. Si nous n’avions pas accès à des solutions pour une meilleure performance ou n’avions pas de motivation à travailler plus vite, alors rien ne nous contraignait à penser à la création de processus performants. La durée et la complexité d’un processus peuvent être considérées comme une fatalité immuable. De surcroît, l’habitude, les reprises de procédures à chaque nouvel arrivant sur le poste ajoutent peu à peu les couches de complexité et gomment progressivement le rationnel de certaines étapes de nos processus.\n\nL’implémentation d’une nouvelle solution ne peut être optimale si les fondamentaux du processus qu’elle couvre ne sont pas revus. La difficulté est que, lorsqu’une portion de chaîne d’analyse est modernisée, il faudra souvent ajuster :\n\n– le processus concerné par la nouvelle solution pour profiter pleinement de ces fonctionnalités plus performantes et également des nouvelles opportunités apportées, que nous n’avions jamais considérées auparavant ;\n\n– les processus amont et aval qui connectent avec le processus en question. En effet, si notre portion de processus devenue lean doit se connecter sur des processus manuels et à haut risque d’erreur, alors l’investissement n’aura qu’une portée limitée sur l’ensemble du processus.\n\nUne vraie optimisation de processus s’étend souvent bien au-delà de son point de démarrage, ce qui rend ces initiatives délicates, si nous voulons les mener de manière holistique. Non seulement nous allons devoir apprendre à travailler différemment, mais nos partenaires internes vont devoir aussi évoluer.\n\nL’âne et le camion\n\n  \nImaginons que nous sommes un fermier des années 1910 de Menlo Park et que, chaque semaine, nous nous rendons avec notre âne à San Francisco, 45 km au nord, pour vendre nos récoltes maraîchères. C’est une habitude, une tâche que nous, nos parents et nos grands-parents avons effectuée depuis des décennies.  \nUn jour, notre âne se faisant vieux, nous décidons d’utiliser l’automobile de notre voisin. Nous la rendons quelques heures après, sans qu’elle ait bougé. Cette machine est sans intérêt : elle n’avance pas avec la carotte, elle est très inconfortable quand on est assis sur le toit et les paniers d’osier n’ont pas d’endroit pour s’accrocher sur les côtés du véhicule. En plus, elle ne rentre pas dans l’étable !\n\nSi nous ne changeons pas notre processus, ainsi que nos habitudes amont et aval, nous n’allons pas pouvoir tirer le meilleur parti de cette innovation technologique."
        }
      },
      {
        "json": {
          "pageId": "PAGE114_1757157753085",
          "text": "# L’accélération par le RPA (Robot Process Automation)\n\nCette pratique est devenue très à la mode depuis le début des années 2010. Elle consiste à mettre des robots pour automatiser les tâches administratives et analytiques.\n\nLes robots RPA vont réaliser de nombreuses actions simples à la place des utilisateurs. Ils vont pouvoir se connecter aux différentes sources de données ou applications, transférer des données ou des fichiers, et réaliser des opérations manuelles au sein de nos logiciels bureautiques tels que des copier/coller, des sauvegardes, des saisies des formulaires, etc.\n\nCette approche est très séduisante et très efficace, mais elle peut aussi se révéler avoir l’effet d’un cautère sur une jambe de bois, et être un pis-aller qui ne résout pas le problème fondamental du processus alambiqué que nous cherchons à améliorer.\n\nSi une chaîne analytique contient suffisamment de copier-coller et de manipulations de fichiers pour justifier un projet de RPA, c’est peut-être qu’il y a déjà trop de copier-coller et de manipulations au départ. Essayer de les confier à des robots logiciels va s’avérer une tâche délicate de programmation pour réaliser toutes les tâches manuelles et prendre en compte les subtilités des données.\n\nUne remise à plat du processus avec une approche telle que celle qu’appliquent les Pionniers peut largement suffire à atteindre les objectifs de performance souhaités. Par contre, si l’organisation n’a pas la capacité d’adaptation de ces processus et encore moins de ceux amont et aval, alors, ce pansement RPA sera la solution.\n\n→ La fameuse template des cours de change (suite)\n\nDevant l’ampleur du temps passé à ressaisir les données chaque mois, il fut décidé de mettre en place du RPA.\n\nPendant deux mois, une équipe de consultants et d’employés planchèrent sur l’automatisation du processus en développant un petit robot (littéralement affiché, dansant à l’écran lors de sa mise en action) capable d’ouvrir le fichier Excel, de choisir le bon onglet, de repérer les bonnes colonnes, d’ouvrir l’application cible et ensuite d’y copier-coller pendant une quarantaine de secondes les taux.\n\nLe coût du projet s’est monté pour la phase 1 à 200 000 euros. Il a toutefois rapidement montré des limites. La stabilité du processus n’était pas garantie : des machines locales n’arrivaient pas à supporter l’enchaînement. Certaines versions de systèmes d’exploitation n’étaient pas compatibles avec le robot. De plus, les fonctionnalités ne s’intégraient pas avec tous les systèmes du groupe.\n\nFinalement, les gains de temps ne se sont appliqués qu’à un tiers des filiales et chaque modification de la donnée centrale impliquait une reprogrammation coûteuse.\n\nUne refonte du processus n’aurait-elle pas été plus simple et moins onéreuse ?"
        }
      },
      {
        "json": {
          "pageId": "PAGE115_1757157753085",
          "text": "# L’optimisation des processus analytiques"
        }
      },
      {
        "json": {
          "pageId": "PAGE116_1757157753085",
          "text": "## L’excellence est dans la répétition\n\n« L’excellence est un art que l’on n’atteint que par l’exercice constant. Nous sommes ce que nous faisons de manière répétée. L’excellence n’est donc pas une action mais une habitude. » Aristote.\n\n\\> Répéter pour apprendre\n\nL’excellence d’un processus ne se décrète pas sur un diaporama ou un guide de procédure. Il n’est pas garanti pas l’observation stricte des bonnes pratiques. Ancré dans des principes évidemment sains et orienté par des objectifs clairs, un processus va devoir faire l’objet d’une amélioration continue. Il devra tenir compte des imprévus, des nouvelles découvertes, des nouvelles solutions que nous ajoutons ou des évolutions de besoin ou de priorité. Définir ex nihilo et une fois pour toutes des processus analytiques est souvent une gageure, devant la complexité et la diversité des éléments humains, techniques et data qui pourront survenir par la suite.\n\nComme pour les implémentations de solutions techniques, commencer petit, procéder par itération et ajuster en faisant est une recette infaillible pour définir et mettre un processus sur le rail de manière durable.\n\n\\> Répéter pour optimiser\n\nUne fois lancé, il sera nécessaire de conserver une récurrence suffisante pour ne pas perdre la main. Un processus réalisé trop rarement va être difficilement optimisable. Si cette tâche a véritablement une valeur ajoutée, il ne faut pas hésiter à en augmenter la fréquence. Cette accélération des rythmes de publication va permettre de démarrer des cercles vertueux avec :\n\n– l’acquisition de réflexes de traitement. À la manière d’un sportif, plus vous allez répéter les gestes et plus vous allez les réaliser vite et sans effort ;\n\n– la maîtrise des données entrantes avec une période plus courte à analyser à chaque itération. Revenir sur trois ou six mois de données sera plus délicat que revenir sur les trente derniers jours ;\n\n– la correction des données ou des traitements plus fréquents. Vous allez progressivement identifier les erreurs qui se nichent dans vos processus et, itération après itération, vous allez pouvoir les corriger, tendant ainsi vers le zéro défaut.\n\nVous allez alors arriver au paradoxe des processus analytiques lean : un processus effectué mensuellement peut finalement prendre moins de temps qu’un processus trimestriel. Une fois la contrainte initiale de la fréquence passée, qui va être coûteuse en temps et en effort, vous allez mettre en route un moteur d’optimisation et entrer dans une phase de gain croissant et surtout de visibilité accrue sur votre activité.\n\n\\> Répéter pour se tromper\n\nÀ moins d’appartenir au cercle restreint des individus qui ne se trompent jamais ou qui font bien du premier coup, votre parcours analytique sera immanquablement un parcours d’erreurs et de corrections. Se tromper et avoir à recommencer est frustrant de prime abord, mais c’est aussi une opportunité pour répéter une tâche et réfléchir à son exécution optimale. Si nous nous trompons dix fois, nous ferons le geste dix fois et, tout en apprenant à le faire naturellement, peut-être nous poserons-nous la question de comment le faire de manière plus simple, confortable, en un mot, de manière optimisée.\n\n→ Prendre le coup de main\n\nLors d’une présentation logicielle, cette directrice financière regarde faire le démonstrateur. Faute de pouvoir travailler sur sa machine habituelle, le processus présenté « bugue » à plusieurs reprises. Puis ce sont des problèmes réseau qui viennent arrêter le déroulement de la session.\n\nConfus, navré, le commercial se confond en excuses. Il vient de recommencer la démo pour la nième fois, progressant chaque fois un peu plus, mais n’arrivant pas au bout, pour des raisons techniques indépendantes. À sa surprise, la directrice financière lui répond : « Ne vous inquiétez pas, en vous regardant faire, je commence à prendre le coup de ce qu’il faut faire. Je vois exactement comment la solution va s’insérer dans mes processus. »"
        }
      },
      {
        "json": {
          "pageId": "PAGE117_1757157753085",
          "text": "## L’organisation des contrôles et les check-lists\n\nAvec l’automatisation des processus analytiques, nous allons progressivement avoir moins de visibilité détaillée sur ce qu’il se passe à chaque étape. En théorie, ce n’est plus un problème de ne plus voir chaque ligne et chaque colonne de chaque jeu de données si nous savons que leur traitement sera fait par la machine, sans erreur.\n\nCe qui demeure à contrôler est néanmoins :\n\n– ce qui arrive à l’entrée du processus et ce qui en sort par comptage ou sommation des lignes entrantes et sortantes ;\n\n– si les données maîtres et les données de référence sur lesquelles vont s’appuyer les préparations de données sont correctes et complètes par profilage ;\n\n– si les calculs et les algorithmes ne contiennent pas d’erreur ;\n\n– si les résultats des traitements sont cohérents par profilage et test aléatoire.\n\nDans le prolongement de votre logique d’efficacité, vous allez vous appuyer sur les rapports d’exécution des différentes solutions de vos chaînes analytiques :\n\n– les rapports d’ETL vont vous apporter une information précieuse sur les volumétries captées et chargées ;\n\n– des rapports analytiques automatisés vont vous permettre de compter et de valider les formats de vos données pour éviter l’oubli d’information qui pourrait ne pas avoir été remontée dans les systèmes ;\n\n– des revues analytiques vont suivre la cohérence des données maîtres ou de référence d’une période sur l’autre : lesquelles sont les nouvelles, lesquelles ont disparu, quelle est leur valeur par rapport aux périodes précédentes, etc. Des rapports bâtis une fois pour toutes pourront vous donner cette vue instantanément ;\n\n– les résultats de tests de jeux de données, recommandés principalement dans les premières itérations du processus. Vous allez valider le processus en suivant une donnée de bout en bout et en validant transformations et calculs. La sélection de ces données test pourra se faire de manière aléatoire ou être concentrée sur des données sensibles.\n\nIl est à noter que, compte tenu de la puissance de calcul des outils actuels, vous allez pouvoir faire porter ces contrôles sur 100 % des données. Vous pourrez vous référer au chapitre 5 sur le profilage pour plus de détail sur les techniques d’analyse de la qualité de données.\n\nLes check-lists sont le média idéal pour centraliser ces contrôles et valider que :\n\n– les tâches ont bien été réalisées par les machines ;\n\n– les rapports d’exécution (logs) ont été contrôlés sur leurs points clés ;\n\n– les rapports d’analyse de profilage ont été revus et visés par un opérateur.\n\nEn quelques minutes, vous aurez validé la qualité des données entrantes, l’exécution des traitements de préparation et la cohérence des paramètres utilisés pour ces derniers.\n\nPar itération, vous allez progressivement raffiner vos contrôles ou ajouter des tests et, en quelques itérations, vous serez en mesure d’avoir l’assurance de la qualité de votre processus et surtout de démontrer que vous avez mis en place toutes les diligences. En plus, valider une check-list, c’est aussi matérialiser que les diligences nécessaires ont été faites.\n\nIl restera bien sûr la validation des données de fait, ces chiffres saisis par les opérateurs ou remontés par d’autres équipes. C’est ce que la phase d’analyse permettra de faire. Avec les heures gagnées par l’automatisation des contrôles, vous pourrez enfin vous y consacrer de manière plus approfondie."
        }
      },
      {
        "json": {
          "pageId": "PAGE118_1757157753085",
          "text": "# Extraire les trésors de l’intelligence collective"
        }
      },
      {
        "json": {
          "pageId": "PAGE119_1757157753085",
          "text": "## La dynamique de collaboration\n\nLes processus d’analyse ont tous leur dose de pression, d’urgence, de concentration. Si nous les ajoutons à nos biais, à nos erreurs, à nos omissions et à nos prédispositions pour certaines tâches analytiques plutôt que d’autres, nous avons un cocktail assez propice aux problèmes.\n\nCes risques croissent d’ailleurs significativement lorsque les processus sont complexes et manuels, comme peuvent l’être des processus tableur.\n\nAfin de pallier ce problème, vous pouvez :\n\n– vous reposer sur vos qualités de concentration, d’attention et de minutie ;\n\n– passer plus de temps, travailler plus lentement et vérifier scrupuleusement les détails de chaque traitement.\n\nCes deux attitudes vont permettre de grandement limiter les risques d’erreur, mais elles vont le faire au détriment de la performance du processus et surtout de la réflexion stratégique autour du problème. Si nous sommes constamment dans les détails, comment pouvons-nous réfléchir de manière globale ou innovante face à un problème ? De plus, nous n’avons pas tous les mêmes prédispositions pour l’attention aux détails.\n\nPlus que s’en remettre à sa seule expérience pour assurer toutes les facettes de compétences stratégiques et tactiques, s’appuyer sur une diversité de collaborateurs va permettre d’assembler le portefeuille de compétences requis pour des analyses pertinentes.\n\nEssayez de travailler en binômes, voire en trinômes, pour traiter chaque aspect du processus analytique de la meilleure façon. Choisissez de vous associer avec des collègues dont les qualités et les centres d’intérêt sont complémentaires aux vôtres. Ce sera de surcroît un parfait antidote pour les effets Dunning Kruger (voir chapitre 6, L’effet Dunning Kruger de l’analyste).\n\nSi vous avez une préférence pour la réflexion plus globale, associez-vous avec des talents qui ont une passion pour les détails. Si vous êtes plus attiré pour la résolution de problèmes techniques et l’optimisation de processus, travaillez avec des professionnels ancrés dans la réalité du terrain.\n\nCette approche s’éloigne des stratégies de recrutement des années 2010, où des groupes se concentraient uniquement sur le recrutement de MBA, grandes écoles pour toutes les tâches analytiques. Les équipes se retrouvaient alors avec des armées de jeunes talents, âgés de 21 à 25 ans, sans grande expérience opérationnelle, issus des mêmes environnements sociaux et culturels, hommes, caucasiens pour la plupart. Si, sur le papier, la surenchère sur l’excellence académique peut sembler légitime, dans le contexte analytique, elle est assez contre-productive.\n\nAvec uniquement des analystes surdiplômés fondus du même moule, nous nous retrouvons avec les mêmes façons de penser, les mêmes ambitions concurrentielles, le délaissement des tâches moins nobles et pourtant clés (telles que la préparation de données) ou le pointage de détail. Cela engendre des processus analytiques déséquilibrés, avec des trous sur les phases que ces équipes ne daignent pas traiter.\n\nLa fiction dépasse la réalité ?\n\nS’il nous fallait uniquement les meilleurs des meilleurs pour réussir, comment ont fait L’Agence tous risques, Scoubidou, Le Club des 5, L’équipe de France victorieuse de la coupe du monde de 1998, le village d’Asterix, Tintin et ses amis, Harry Potter pour venir à bout de toutes leurs aventures ?\n\nN’est-ce pas l’alchimie de la diversité des personnages qui permet au groupe de prévaloir ?\n\n» Recherchez dans vos films, vos livres préférés ou vos expériences personnelles quand se sont manifestées ces dynamiques de collaborations inclusives ?\n\nLe courage que nous avons évoqué au chapitre 6 sur les facteurs humains va devoir être employé pour s’associer avec des individus « différents ». Nous allons devoir oser exposer nos résultats, nos logiques, nos processus, nos erreurs à des vues différentes. Cela va nous sembler vexant d’être pris en défaut sur une faute d’étourderie ou une erreur de calcul. Mais, au lieu de prendre cet exercice comme un jugement de notre travail, considérons-le comme une collaboration pour améliorer un processus analytique. Coupons le cordon ombilical qui nous unit à notre reporting pour nous concentrer sur sa finalité et non sa forme.\n\n→ Merci Marie\n\nJ’ai découvert l’importance de la diversité en analytique avec Marie, une de mes premières collaboratrices.\n\nAvec plus de trente ans d’ancienneté dans la société, Marie avait acquis une connaissance des rouages de l’organisation sans pareil. Elle avait aussi un sens aigu du détail et une patience inépuisable pour certaines tâches de précision. Ces qualités, que j’ai travaillées avec le temps, notamment avec l’astrophotographie, me manquaient beaucoup à l’époque. De plus, nous étions en pleine reconstruction (révolution) de notre analytique et en situation de crise : mon esprit était accaparé par des défis techniques, opérationnels, politiques des nouveaux choix en Business Intelligence.\n\nPendant des années, nous avons peaufiné nos synergies. Nos progrès ont permis le bouclage de projets reconnus dans plusieurs grands médias dont le Financial Times. Ce fut pour moi un déclic.\n\nNous considérions nos analyses et nos reportings récurrents comme des outils de réflexion et non des fins en soi. Nous travaillions dans un but commun, sans revendication de propriété d’un rapport et nous n’avions aucun problème à se faire corriger l’un par l’autre. Nous avions conscience de nos points faibles respectifs et n’étions jamais dans le jugement de l’un ou de l’autre : nous faisions chacun du mieux possible.\n\nCombien de fois Marie m’a-t-elle pris en défaut, corrigé des erreurs, ou challengé un résonnement ! Combien de fois ai-je gentiment repris Marie pour des tâches mal optimisées, ou l’ai-je engagée sur des évolutions de processus ?\n\nNous savions nous sortir l’un l’autre de nos zones de confort et nous nous rappelions régulièrement à l’ordre pour corriger nos faiblesses.\n\nNotre faculté de remise en cause va aussi être testée par ces collaborations. La récurrence des processus analytiques contribue à notre excellence, mais aussi à notre complaisance. À force de faire des choses, nous perdons du recul, la routine s’installe et notre jugement ou notre attention s’altèrent. Les collaborateurs que nous invitons dans notre processus apporteront naturellement le regard critique qui s’estompe progressivement de notre côté.\n\n→ Quel est le point commun entre un cockpit d’avion de ligne, une salle de chirurgie et… une équipe d’analystes performante ?\n\nL’aviation et la médecine ont rapidement reconnu les avantages de collaboration saine au sein d’équipes de travail. Les conséquences dramatiques d’erreurs ont certainement poussé à réfléchir et à formaliser des protocoles dans ce domaine. Même si un commandant ou un chirurgien dirige, il est crucial que leurs équipes puissent pointer des omissions, des erreurs ou présenter des idées.\n\nLe plus gros crash de l’aviation civile, dans les îles Canaries, illustre les risques d’une relation hiérarchique trop forte qui neutralise la conversation et la critique. Dans ce drame, ce sont deux Boeing 747 qui se sont percutés sur la piste, tuant 585 personnes. À la suite d’une série d’événements improbables qui s’enchaînent, les deux avions se retrouvent sur un aéroport sous-dimensionné et sous-équipé. Le commandant du premier Boeing, fort de caractère, pressé par le temps et excédé par la situation qui les a amenés là (je vous recommande la lecture des détails de ces événements dans les nombreux articles sur le sujet) va choisir d’engager un décollage en plein brouillard, sans visibilité sur le bout de la piste. Agacé, à bout, il va le faire sans respecter les protocoles de confirmation de décollage. Son copilote, qui note le problème ne va toutefois pas oser suffisamment fermement faire part de son doute sur l’autorisation de décollage. Entre la communication radio dégradée avec une tour de contrôle dépassée par la situation et la figure autoritaire du commandant, alors figure de premier plan dans la communication de la compagnie aérienne, le copilote ne va pas contester la décision de mettre les gaz. Le Boeing, rechargé en kérosène, finira sa course en frappant de plein fouet l’autre Boeing toujours en manœuvre à l’autre bout de la piste.\n\nCes drames, liés à la confusion entre hiérarchie, compétence et raison sont aussi malheureux en médecine comme lors de cet incident sur une patiente dans un hôpital anglais. La victime décédera des suites d’une anesthésie suivie d’un arrêt de ses fonctions respiratoires, à peine la chirurgie commencée. Dès le problème constaté, le chirurgien et l’anesthésiste avaient pourtant déroulé les protocoles nécessaires. La patiente ne réagissant pas, ils avaient fait appel à des confrères experts présents dans les locaux. Les tentatives d’intubation n’aboutissaient pas et la tension était montée rapidement. Une équipe des meilleurs spécialistes s’affairait encore autour de la patiente quand elle a rendu son dernier souffle. Aucun d’entre eux n’aura fait attention à l’infirmier trop timide pour s’imposer, qui avait préparé ce qui aurait certainement sauvé la victime : un kit de trachéotomie.\n\nLa majorité de nos analyses ne mettent pas entre nos mains la vie de patients ou de passagers, mais nous pouvons beaucoup appren­dre de ces disciplines et de leur gestion de processus.\n\nApprenons à encourager la collaboration bienveillante. Osons exposer nos faiblesses pour mieux les compenser avec l’aide de nos collaborateurs."
        }
      },
      {
        "json": {
          "pageId": "PAGE120_1757157753085",
          "text": "## Le cercle vertueux Analytiques terrain – Informatique – Data Science\n\nUne session de travail Métier-Data Science-Informatique sans une culture data dans l’entreprise.\n\n\\> Un passé d’initiatives déconnectées\n\nLes synergies interpersonnelles doivent être complétées par des synergies interéquipes. Cela apparaît comme une évidence, mais nous continuons à voir des schémas de fonctionnement non propices à des collaborations allant au-delà de la relation client/fournisseur entre les différents départements intervenant sur la chaîne analytique.\n\nLes vagues de digitalisation successives ont vu par exemple le lancement de programmes souvent déconnectés :\n\n– les formations ou l’acculturation des équipes dirigeantes sur des approches de pointe à travers des formations exécutives ou des séries de conférence ;\n\n– les investissements lourds en nouvelles technologies par les services techniques ;\n\n– la création ex nihilo d’équipes digitales ou de Data Science ;\n\n– l’équipement de tous les collaborateurs avec des outils BI non maîtrisés ;\n\n– la prise de position affichée de faire du digital par l’achat de technologies gadgets dans les équipes terrain.\n\nCes stratégies manquent l’objectif principal de la transformation digitale : gagner en temps et en visibilité pour développer l’entreprise de manière durable. Elles ne contribuent pas à bâtir une culture data universelle (data culture at scale) dans l’entreprise, qui permettra à toutes les forces vives de l’organisation d’apporter leur valeur. Elles se concentrent sur chacun des étages de la pyramide de l’organisation data sans se préoccuper des courroies de transmission entre les niveaux. Elles restent sur le modèle : l’informatique choisit la technologie, les équipes data font les analyses et la majorité des salariés de l’entreprise sont cantonnés à être servis en analytique, sans vraiment contribuer.\n\nComment cette structure va-t-elle pouvoir changer de dynamique ? Le démarrage de cette dernière ne pourra pas être décrété par la création d’équipes dédiées ou l’investissement en matériel : ce sont les data pionniers, nous tous, qui allons être la fondation de cette transformation.\n\nFigure 16. Dynamique synergétique d’une data culture at scale\n\n\\> Le Pionnier catalyseur\n\n« Avec de grands pouvoirs viennent de grandes responsabilités. » Oncle Ben à Spider Man\n\nÉquipé des compétences partagées dans ce livre, le Pionnier acquiert une nouvelle autonomie. Au quotidien, nous pouvons mettre à jour nos analyses, oser poser de nouvelles questions et bâtir des chaînes analytiques légères et agiles qui vont pouvoir traiter la majorité des problèmes posés au quotidien. Nous allons appuyer nos initiatives sur les fondements d’une donnée fluide et durable en travaillant à partir de tables et d’outils décisionnels. Ce comportement responsable et professionnel va créer de nouvelles relations avec les équipes data et informatiques qui supportent les métiers :\n\n– nous sommes des interlocuteurs éclairés, qui connaissent les limites et les opportunités de la technologie ;\n\n– nous travaillons notre donnée avec des normes et des méthodologies compatibles. Même si nous devons travailler en local indépendamment des systèmes centraux pour des raisons pratiques, nous savons conserver notre donnée fluide et nos processus analytiques clairs ;\n\n– nous venons demander du support technique moins souvent et, quand nous le faisons, nous sommes souvent matures sur notre problème et avons probablement déjà testé de nombreuses options ;\n\n– nous savons nous aligner sur une gouvernance de la donnée que nous comprenons et à laquelle nous contribuons ;\n\n– nous ne sommes plus attentistes ou renfermés, nous sommes devenus de vrais partenaires des équipes support.\n\n\\> Les équipes data et informatiques libérées\n\nLes équipes techniques vont (enfin) échanger avec des interlocuteurs qui connaissent les bases de l’analytique :\n\n– elles vont être engagées sur des projets initialement testés par les métiers et dont la valeur a été validée ;\n\n– elles vont reprendre des maquettes ou des processus propres, facilement transposables dans leurs solutions ;\n\n– elles vont pouvoir profiter de vrais débats sur des arbitrages que les Pionniers comprennent enfin (limites transactionnel/décisionnel, modélisation et jointure, technique ETL, etc.).\n\nMoins sollicitées et interrompues par des demandes pas toujours pertinentes, les équipes support vont pouvoir se concentrer sur leur mission à valeur ajoutée ou sur les défis de demain tels que :\n\n– la cybersecurité et la protection des données ;\n\n– la mise à l’échelle des infrastructures et la maîtrise des coûts pour relever les défis d’intelligence artificielle de demain ;\n\n– la gouvernance sur des données de plus en plus variées ;\n\n– le respect de la vie privée dans le stockage et le traitement des données.\n\nEn tandem avec les métiers, les équipes data et informatiques vont permettre le développement de recherche et d’analyses toujours plus ambitieuses. Pionniers, informatique et équipe data vont pouvoir se tourner vers la Data Science avec non pas des illusions en tête, mais des questions très précises, soutenues par une donnée claire et des systèmes puissants.\n\n\\> La Data Science impactante\n\nGrâce aux synergies Pionnier/informatique/équipe Data, les équipes de Data Science vont enfin bénéficier :\n\n– de questions métier intéressantes ;\n\n– de la donnée organisée et propre ;\n\n– de systèmes performants ;\n\n– de partenaires techniques et métier qui travaillent en équipe.\n\nC’est la fin du problème récurrent des équipes de Data Science qui parcouraient les couloirs à la recherche de projets et qui finissaient inévitablement par construire une maquette (proof of concept) sans réelle application.\n\nAccompagnées par toute l’entreprise, ces équipes peuvent bâtir des analyses avancées pertinentes, moins biaisées et surtout comprises par les acteurs concernés. Les scientists deviennent une partie intégrante de la chaîne.\n\nLa culture à l’échelle de l’entreprise est là."
        }
      },
      {
        "json": {
          "pageId": "PAGE121_1757157753085",
          "text": "## Vers une intelligence de l’essaim\n\n« Ce n’est pas la queue qui remue le chien, mais l’inverse. » Anonyme\n\nBâtir une culture large de la donnée, c’est offrir à chacun le bénéfice de la visibilité sur son activité et son environnement, et surtout le luxe du temps pour se poser, réfléchir et innover.\n\nQuand chacun des collaborateurs gagne en plus la capacité quasi instantanée, massive et sans effort de partager ses informations et ses analyses, alors il se crée naturellement un maillage de synergies sans limites.\n\nAucun contributeur individuel ne peut remplacer le fourmillement d’idées de data pionniers opérationnels débarrassés de leur limitation analytique et de leurs biais.\n\nL’autre vertu de l’intelligence collective est que, plus que jamais dans notre société de diversité, nous avons tous un rôle à jouer.\n\n------------------------------------------------------------------------\n\n23. Lire l’ouvrage de Daniel Kahneman, Prix Nobel d’économie, Les deux vitesses de la pensée (Thinking, Fast and Slow) (Flammarion, 2012).\n\nConclusion\n\nLe Pionnier part à l’assaut de nouveaux défis ! Il va leur manquer.\n\nNous venons de couvrir les bases essentielles pour avancer en confiance dans le monde de l’analytique. Vous avez tout ce qu’il faut pour réussir et prendre votre rôle et vos responsabilités de pionnier de la data.\n\nVotre aventure entre désormais dans une nouvelle phase : celle de la pratique et de l’amélioration continue. Forts de vos compétences, vous allez devoir vous reposer sur ces mêmes valeurs qui vous ont fait investir ce temps de lecture et d’apprentissage : le courage, la curiosité, la détermination.\n\nUne aventure de découverte et d’apprentissage vous attend et il vous faudra du cœur pour avancer, faire face aux obstacles techniques, data et humains. Ce chemin sera une succession d’essais, d’erreurs, d’échecs qui vous mèneront aux victoires, qui ouvriront de nouveaux défis ou poseront de nouvelles questions.\n\nIl vous faudra aussi du cœur pour vous attaquer aux longues phases de préparation et de nettoyage de données ou encore aux tests de vos processus pour vous assurer de leur stabilité. Même si ces tâches peuvent être réparties ou déléguées, il vous incombera de vous investir dans ces dernières. En tant qu’analystes responsables, nous nous devons de travailler en amont sur la qualité et la fluidité de la donnée : notre responsabilité ne peut plus se limiter à notre pré carré. En plus de votre maîtrise des outils analytiques, la qualité de vos résultats se joue autant dans votre travail avec vos contributeurs que dans votre influence sur vos lecteurs et dans vos échanges avec l’informatique. N’invoquons plus systématiquement leurs erreurs en cas d’échec : la réussite de vos analyses est de votre responsa­bilité.\n\nLe cœur sera également le moteur des qualités requises pour devenir un pionnier de la data. Plus qu’une intelligence brute ou académique, ce sont les « quotients » collaboration, curiosité ou encore empathie dans lesquels vous devrez puiser. Les règles techniques et data sont établies : la grande inconnue sera votre environnement organisationnel et humain.\n\nNous devrons également sortir de l’obsession digitale pour ne pas oublier que les vrais défis ne sont pas la data, mais bel et bien ceux de l’excellence opérationnelle, décisionnelle, de l’innovation et de la réinvention continue de nos métiers dans un monde en accélération perpétuelle.\n\nVotre humanité, enfin, sera votre plus précieuse alliée pour garantir une data éthique, humaine et juste. Les lois, les règlements ou le politiquement correct ne seront jamais suffisants pour s’ériger en garde-fous des abus de la data. Les options que vous choisirez devront l’être avec nuance. En complément des lois, vous devrez vous fonder sur votre culture, votre expérience et vos valeurs de respect, d’inclusion, de liberté. C’est un défi énorme qui se pose à nous tous.\n\nComprendre ou maîtriser les machines et le traitement de données est une priorité aujourd’hui pour que nous puissions nous consacrer beaucoup plus à éviter que 1984 soit demain.\n\nCe livre est une humble contribution pour donner à tous les clés pour réussir nos premiers pas dans le monde de la data. C’est notre responsabilité de pionnier de contribuer à cette data éthique, pertinente et humaine.\n\nÀ vous de jouer ! Rejoignez les data pionniers !\n\nANNEXES  \nTémoignages de data pionniers et paroles d’experts"
        }
      },
      {
        "json": {
          "pageId": "PAGE122_1757157753085",
          "text": "# Hayat Kuna, Pierre-Antoine Bolmont, Benjamin Butez"
        }
      },
      {
        "json": {
          "pageId": "PAGE123_1757157753085",
          "text": "## Témoignages croisés de data pionniers\n\nHayat Kuna est manager de la transformation digitale chez Atos. Elle intervient depuis plus de quinze ans sur des problématiques de transformation des entreprises dans le marketing et la supply chain, dans le secteur industriel ou public. Elle participe également au développement de la formation interne.\n\n« J’accompagne mes clients pour mettre en place des solutions digitales ou favoriser l’acculturation digitale. Dans mes missions, le management de la data prend une place de plus en plus importante, d’où la nécessité de bien gérer les données avec le respect des réglementations RGPD (Règlement général sur la protection des données) également. »\n\nPierre-Antoine Bolmont a six ans d’expérience chez Decathlon : quatre en magasins, puis deux en service RH international. Il a travaillé notamment pour le déploiement international d’outils de formation et de montée en compétences. Aujourd’hui, il est chef de projet People Data, le projet de data RH de Decathlon au niveau international, et leader des référentiels métiers et compétences.\n\n« J’accompagne l’acculturation data au sein de notre domaine RH toujours au niveau international. »\n\nBenjamin Butez, de formation métier RH avec de fortes appétences pour l’analytique et le digital, travaille depuis une dizaine d’années sur la mise en place de processus de reporting dans le domaine des ressources humaines.\n\n« Je travaille à la conception de solutions analytiques plus agiles et performantes, pour mieux accompagner le développement des talents de l’entreprise et présenter par l’exemple le potentiel d’une data maîtrisée. »\n\n\\> Quels sont les défis de la transformation ?\n\nH.K. Lors de projets d’accompagnement de transformation digitale ou de mise en place de solutions digitales, nous pouvons nous sentir limités pour discuter et challenger sur des problématiques de données entre métier et informatique par manque de vocabulaire et de connaissance sur comment bien gérer la data.\n\nP.-A. B. L’un de nos plus grands défis est d’engager toutes les parties prenantes du problème à résoudre. Il faut prouver et mettre en avant les bénéfices pour que chacun ait l’envie d’être acteur et promoteur de la démarche. Si un coéquipier comprend et identifie l’intérêt de récolter, de partager et de maintenir de la donnée fiable pour lui, il sera très facile de soutenir et de maintenir la démarche dans le temps.\n\nB. B. Les freins principaux qu’on retrouve sur le chemin de la mise en place d’une culture de la data sont à mon sens la fiabilité des données et la facilité d’utilisation. Il est essentiel de prendre en compte ces deux enjeux dans la construction des outils et des datas mis à disposition des équipes.\n\n\\> Quelles sont les missions du data pionnier qui vous tiennent à cœur ?\n\nChanger pour grandir\n\nB. B. Les pionniers doivent intégrer les nouveaux outils et les nouvelles méthodes pour optimiser leurs processus de reporting. Ils doivent progressivement abandonner les approches tableur, pour travailler avec des outils spécialisés dans le traitement ou la visualisation des données. Concrètement, cela permet de remplacer l’exécution de tâches répétitives et manuelles à fort risque d’erreur. L’évolution des processus apporte aussi un gain de temps de production et une meilleure qualité des analyses.\n\nChallenger pour progresser\n\nH.K. Le data pionnier doit faire le lien fonctionnel entre les métiers et l’informatique. Il doit pousser ou faciliter la formulation de questions pertinentes et contribuer à leur bonne interprétation par les services informatiques et data. Il va travailler aussi en amont sur la data pour vraiment identifier celle qui apportera de la valeur au métier, pour ses analyses, ses processus ou ses applications.\n\nStructurer pour unifier\n\nP.-A. B. Il est aussi primordial de connaître et de maîtriser un langage commun autour de la data et de le diffuser le plus largement possible. Le data pionnier doit prendre le temps pour poser les fondements du Master Data Management. Il doit connaître le niveau de qualité de ces données utiles et le maintenir à un haut niveau. Il devra savoir communiquer l’importance de la gouvernance de la donnée et anticiper les difficultés d’implémentation potentielles, sous peine de rendre caduque l’ensemble de la démarche.\n\nRester humain\n\nP.-A. B. L’intuition et l’expérience acquises puis la data permettent de garantir une prise de décision humaine et éclairée. Promouvons une data humaniste et non robotique. Il convient de ne pas réduire la décision finale via le prisme unique de la data. Être 100 % data-driven n’aurait que peu de sens et la force de ces démarches en serait fortement réduite. Tout est question d’équilibre !\n\n\\> Quelles révolutions apportent les data pionniers\n\nGagner du temps pour se consacrer aux nouveaux défis\n\nB. B. La prise de conscience générale sur l’importance de la data est venue bouleverser la manière dont les directions RH gèrent les données des collaborateurs. Historiquement, la grande majorité du reporting RH consistait à produire du reporting réglementaire. Désor­mais, de plus en plus de dirigeants demandent de produire et de suivre du reporting opérationnel afin de piloter l’activité avec des indicateurs clés. Même si les principaux indicateurs RH ont peu changé (suivi des effectifs, absentéisme, turnover, etc.), c’est leur analyse qui a évolué. En tant que Pionnier, je fais des analyses approfondies dans les différents niveaux de l’organisation en croisant différentes dimensions comme l’âge, l’ancienneté, la localisation, le secteur d’activité. Grâce aux dizaines d’heures gagnées chaque mois, je peux me consacrer à de nouveaux chantiers pour développer encore plus le rôle de la RH. Dans un environnement économique concurrentiel, je peux contribuer au développement du capital humain, autant pour des enjeux de productivité que dans un con­texte de guerre des talents.\n\nApporter une puissance d’action sur toute la chaîne de la data\n\nH.K. Le data pionnier possède la puissance de préparation de la donnée : il peut attaquer toutes sources pertinentes pour l’analyse. Grâce à une solide base méthodologique, il va également bien préparer les données, phase cruciale en analytique. Par exemple, dans le cadre d’une mission pour mettre en place une nouvelle application, il était nécessaire de faire une reprise des plusieurs millions de données clients. Il a été demandé au métier de valider l’extraction avant de charger les données dans la nouvelle base. Sur quels critères le métier pouvait-il s’appuyer et vérifier que ces données étaient propres ? Devait-il tester par échantillons ou vérifier ligne par ligne ? Quelle méthode statistique devait-il employer ? La technique et les outils du data pionnier ont permis d’analyser ce fichier plus sereinement, plus rapidement et in extenso… En quelques minutes, sans passer par des formules sur fichier Excel. Autre exemple, dans un contexte de marketing multicanal, le data pionnier va pouvoir accompagner le métier pour mieux utiliser toutes ses données « silotées » grâce à des développements agiles. Il l’accompagnera pour construire, joindre, agréger, calculer les données plus facilement. Il rendra le métier autonome par la mise en place d’analyses souples et rapides. Il pourra ensuite être un lien entre le data scientist et le métier pour les analyses avancées qui suivront.\n\n\\> Quels sont vos facteurs clés de succès ?\n\nApprenez !\n\nH.K. Il ne faut plus « avoir peur de la data ». C’est un premier cap à passer pour des fonctionnels comme moi. Démystifions le vocabulaire autour de la data. Apprenons à comprendre la data, comment choisir les indicateurs et comment mieux analyser pour répondre au plus près aux enjeux du métier. Cela permet d’être plus dans l’anticipation et de décider plus facilement.\n\nP.-A. B. Côté RH, Il faut prévoir un plan d’accompagnement, avec notamment des parcours complets de formation. La clé est de présenter la data comme un outil d’aide à la prise de décision ou permettant d’identifier des signaux faibles difficilement perceptibles, mais toujours en gardant en tête que la gestion de l’humain nécessite l’intuition et l’expérience métier. Il faut définir une stratégie progressive de mise à disposition d’outils pour des utilisateurs peu habitués à traiter des données.\n\nPratiquez !\n\nH.K. Comme tout nouveau domaine, par la répétition, nous devenons meilleurs. Le nombre de données devenant croissant de plus en plus, tout comme leurs combinaisons, une pratique au quotidien permet d’acquérir des automatismes.\n\nB. B. La performance du processus se traduit par l’automatisation pour réduire la charge de réalisation tout en garantissant la fiabilité. Il faut pratiquer pour optimiser le temps disponible en limitant les efforts nécessaires pour produire les reportings récurrents afin de garder du temps pour l’analyse des données.\n\nPartagez !\n\nB. B. il faut éviter de construire des outils destinés à des experts. Un outil très puissant permettant des analyses approfondies mais complexe ou peu ergonomique ne sera pas ou peu utilisé. L’expérience utilisateur est un facteur clé d’appropriation des outils de reporting.\n\nH.K. Être data pionnier est une attitude, une posture, une culture à acquérir au sein de l’entreprise. Cette nouvelle compétence s’ajoute à d’autres comme travailler en agile. Le rôle d’ambassadeurs des data pionniers dans les entreprises est clé pour pouvoir travailler dans le même sens.\n\nP.-A. B. Le plus grand défi pour nous est de faire de l’usage de la data un réflexe quotidien, tant dans le partage et le suivi de nos données que dans l’utilisation de ces données, et ce, pour tous nos collaborateurs, quel que soit leur rôle. L’enjeu n’est pas d’en faire un projet data à l’échelle de l’entreprise ; mais de distiller la composante data dans l’ensemble de nos processus métier. Cela prend forme pour nous par le cas d’usage du capital humain. Nous souhaitons continuer à développer, à monitorer et à veiller sur ce capital au quotidien et nous souhaitons y impliquer l’ensemble de nos collaborateurs. À nous de bien déployer cette culture du jeu pour que chacun d’entre nous puisse y prendre part.\n\n\\> Un dernier mot ?\n\nH.K. Dorénavant, la donnée ne sera plus du domaine exclusif des data scientists ou des business analysts : il y a encore souvent un monde entre ces derniers et les métiers. Qu’ils soient issus des départements techniques, data ou de l’opérationnel, les data pionniers sont le chaînon manquant.\n\nB. B. Accompagné pendant toutes ces années par Excel pour le traitement et l’analyse des données, je n’ai que récemment découvert de nouveaux outils et de nouvelles méthodes pour optimiser mes processus de reporting. L’opportunité de devenir data pionnier m’a permis de découvrir un nouveau monde, celui de la data et de redécouvrir mon monde : celui du capital humain.\n\nP.-A. B. Pensons d’abord métier avant de penser data ! La data est « au service de » et non un sujet à part entière. Pensons avec notre tête, notre cœur, nos tripes et nos datas. Ne pensons pas que data ! La data vient compléter, conforter et consolider notre intuition, notre background et notre réflexion. Partageons notre enthousiasme. Travaillons tous ensemble pour que la data ne vienne jamais remplacer notre background et pour qu’elle vienne servir uniquement nos enjeux."
        }
      },
      {
        "json": {
          "pageId": "PAGE124_1757157753085",
          "text": "# Capucine Ortoli"
        }
      },
      {
        "json": {
          "pageId": "PAGE125_1757157753085",
          "text": "## Nos attitudes face à la transformation digitale et aux changements\n\nCoach, consultante et accompagnatrice de transformation individuelle et systémique : « Le plus humain, c’est le plus efficace durablement. »\n\n« Je suis une actrice du changement profond et durable de la culture d’entreprise vers plus d’humanité et d’efficacité. Ma singularité s’inscrit dans le lien entre la recherche du sens, l’intention profonde de vos actions, plus d’humanisme en entreprise dans le concret opérationnel. J’aide les individus, les leaders et les organisations à défier le statu quo et à donner naissance à de nouvelles façons d’interagir, de faire, de penser, de collaborer et d’être et je les accompagne pour le vivre au quotidien. »\n\n\\> Quel point de départ d’une transformation : théorie X ou Y ?\n\nLa théorie X de Douglas McGregor présuppose que l’homme n’a pas envie de travailler. Il faut l’y obliger, et donc le contrôler. L’homme chercherait dans cette théorie la sécurité, à ne pas prendre de risque et à rester toujours dans sa zone de confort. Il ne mobiliserait son intelligence que pour contourner les règles qui le gênent. Face à ces croyances, le seul mode de management qui paraît possible est un management autoritaire, qui lui-même accentue l’aversion du travail.\n\nL’homme est vu comme « une ressource à gérer » et non comme un être humain plein de potentiel à réaliser.\n\nLa théorie Y pose un autre regard : l’homme a une motivation intrinsèque pour le travail et un désir de contribuer ; il se réalise s’il est impliqué profondément dans l’organisation. Plus le salarié est impliqué dans l’organisation, plus il fait ses propres choix, plus il est engagé dans ses tâches. Plus un salarié est engagé, plus il se responsabilise et s’auto-organise. Ainsi s’installent la confiance, la liberté et la responsabilité dans le cadre d’une vision partagée des buts de l’entreprise.\n\nNous retrouvons ces deux facettes dans les projets de transformation digitale !\n\n\\> Dans quel cadran vivons-nous notre projet digital ?\n\nIl y a quatre territoires dans une transformation durable, profonde et performante à la fois humainement et économiquement.\n\nFigure 17. Matrice de quatre territoires pour guider une transformation digitale humaine24\n\nUne transformation durable passe par chacun de ces espaces. Le plus important est la flèche circulaire au milieu. Il ne s’agit pas d’abord de changer les pratiques ou de travailler sur les individus, mais bien d’agir sur chaque cadran simultanément. Si un cadran n’est pas pris en compte dans la transformation, attendons-nous à des résistances !\n\nUne entreprise industrielle, avec laquelle j’ai travaillé, initiait sa transformation principalement sur deux axes : individuel qui ne se voit pas (développement personnel, leadership) et collectif (méthodes agiles, lean par exemple). Le résultat : une forme d’individualisme s’est développée et certains leaders sont partis, faute d’un projet commun inscrit dans la profondeur (le collectif qui ne se voit pas). N’appuyer que sur deux cadrans sans appuyer sur les autres crée des effets de bord contre-productifs.\n\n\\> Quelles sont nos postures individuelles lors de transformations (digitales) ?\n\nNous allons nous concentrer sur la partie « individu » sur laquelle le data pionnier peut travailler individuellement.\n\nDans tous les projets, de transformation digitale ou pas, nous allons adopter des postures. Le principe n’est pas d’imaginer que nous allons changer l’autre ou nous-même immédiatement, mais juste de voir le mécanisme en route. Notons que c’est toujours plus facile de voir l’autre dans sa posture que de se voir soi-même.\n\nNos postures peuvent alimenter deux cycles : le cycle du drame ou le cycle de l’innovation et de la transformation féconde. La première étape, c’est la prise de conscience de chaque individu et des équipes pour identifier la dynamique dans laquelle nous nous trouvons.\n\nLes postures qui recyclent les drames dans l’entreprise : le triangle de Karpman « victime-sauveur et persécuteur »\n\nPrenons pour exemple un projet de développement de reporting où trois équipes sont impliquées : une équipe de développement, une équipe data, deux managers d’équipe. L’équipe de développement est en retard sur le livrable. Le cycle du drame suivant peut se mettre en place :\n\n– les chefs d’équipe accusent l’équipe de développement : « C’est de votre faute, vous êtes en retard. » Ils prennent le rôle de persécuteur, ils accusent les autres ;\n\n– l’équipe de développement rétorque : « C’est de notre faute, on ne peut rien faire, nous n’avons pas les moyens de travailler plus vite : nous ne sommes pas assez nombreux, on nous a imposé des délais de livraison trop courts. » Elle prend le rôle de victime qui ne peut rien faire, qui est impuissante et désarmée. Elle subit ce qui arrive ;\n\n– l’équipe Data arrive et propose : « Ne vous inquiétez pas, nous allons vous aider, nous pouvons nous rendre disponibles pour vous aider à travailler, nous allons négocier avec le client interne de nouveaux délais. » Elle se pose en sauveur de la situation sans que personne n’ait rien demandé.\n\nLe cycle du drame va commencer et se perpétuer. Il sera alimenté par les changements de postures des protagonistes. Regardons par exemple les chefs d’équipe qui se posent en persécuteurs. Ils prennent aussi le rôle de victimes des deux autres équipes : ils ne peuvent rien faire pour les aider, ils se sentent impuissants, et ils pourraient à terme tenter de sauver la situation au côté de l’équipe Data.\n\nPour nous aider à identifier nos postures, voici quelques éléments de repère\n\nAfin d’éviter les spirales négatives, nous devons prendre conscience individuellement ou collectivement des postures que nous prenons. Nous devons sentir l’apparition de ces triangles dramatiques qui génèrent des peurs et sont parfois vécus comme des menaces.\n\nL’idée n’est pas de faire disparaître ces mécanismes, mais d’identifier si notre posture est adaptée à la situation : est-ce que cela sert la transformation digitale d’entreprise ?\n\n\\> Quelles sont les postures à adopter pour sortir du triangle du drame et entrer dans le cycle de l’Innovation ?\n\nPrendre conscience du triangle dramatique qui se joue et se rejoue permet de prendre le recul nécessaire et de voir que d’autres postures sont possibles et souhaitables dans l’entreprise pour plus d’épanouissement et de succès pour tous. Nous pouvons bâtir un autre triangle : le coach, le créateur et le challenger.\n\nPour nous ouvrir à ces postures de créateur, de challenger, de coach, nous pouvons utiliser ces questions.\n\nDans l’exemple précédent, les équipes peuvent créer une dynamique positive :\n\n– les chefs d’équipe peuvent se positionner en coach : « Nous allons profiter de cette situation pour revoir nos processus. Si nous ne prenons pas ce temps, la situation se reproduira » ;\n\n– l’équipe de développement se présente en challenger : « La fixation de délais trop courts que nous n’osons pas remettre en cause nous met une pression inutile » ;\n\n– l’équipe Data se positionne en créateur : « Nous allons intercéder auprès du client interne pour obtenir des délais plus raisonnables. Nous devons changer cette dynamique et nous allons proposer une nouvelle approche dans la préparation de la donnée. »\n\nLes projets data se gagnent et se perdent avec le facteur humain. Il ne s’agit pas de mettre des étiquettes sur nos rôles et de ranger nos initiatives dans des cases, mais de trouver des repères pour nous situer dans les différentes dynamiques que nous rencontrons. Ces axes d’analyse apportent un guide à nos réflexions dans des situations complexes qui peuvent rapidement dégénérer et augmentent nos chances de naviguer dans les écueils du travail en équipe."
        }
      },
      {
        "json": {
          "pageId": "PAGE126_1757157753085",
          "text": "# Chantal Buard"
        }
      },
      {
        "json": {
          "pageId": "PAGE127_1757157753085",
          "text": "## La chaîne analytique au-delà du monde de l’entreprise, pour contribuer à un monde meilleur\n\nChantal Buard est la cofondatrice et P.-D.G. d’Impact Atlas, une plateforme technologique permettant de prouver et d’améliorer les programmes sociaux et humanitaires à travers le monde. Auparavant, Chantal a été cofondatrice et directrice digitale chez Amplifier, une agence de stratégie et d’impact social travaillant avec des visionnaires et des investisseurs dans les programmes d’impact social à grande échelle. À travers son expérience chez Amplifier, Chantal a cocréé deux autres solutions numériques : l’une pour la gestion des océans et l’autre pour la pêche aux États-Unis, en mettant en place la gestion de quotas. Elle a aussi créé un des groupes de philanthropie parmi les plus grands et innovants au monde, ainsi qu’une initiative collaborative portant sur l’extrême pauvreté, propulsant au niveau planétaire « l’approche de Graduation » qui a reçu le prix Nobel en 2019. Née à Paris, Chantal vit depuis vingt-cinq ans dans la Silicon Valley.\n\n\\> Pourquoi avoir créé Impact Atlas ?\n\nImpact Atlas a pour but de contribuer à la résolution des grands problèmes économiques, sociaux et écologiques de notre époque par une contribution holistique des enjeux. C’est une plateforme technologique qui permet de gérer l’ensemble des données des programmes sociaux ou humanitaires dans des environnements isolés, souvent pauvres et à faible connectivité. Impact Atlas permet de vraiment connaître ce qui se passe sur le terrain, parfois en temps réel, et de remonter une information aux responsables de programmes pour les analyses, les chercheurs et les donateurs ou les sponsors. La solution est déployée dans quinze pays à travers le monde et apporte une véritable transparence sur les projets complexes.\n\n\\> À quels défis répondez-vous avec Impact Atlas ?\n\nNous faisons tout d’abord face à la grande distance physique et à la difficulté de connecter des données locales très fines et variées à des objectifs macro. Les objectifs et les indicateurs centraux émis par les instances internationales et les associations humanitaires se résument souvent à des mesures très agrégées qui n’expriment pas la complexité d’une situation locale et qui sont construites sur la base d’une donnée parcellaire du fait de la difficulté de collecte de l’information. Dans les pays sur lesquels nous intervenons, les environnements très faiblement équipés ainsi que les conditions de terrain difficiles (absence de budget, d’électricité, de personnel formé, etc.) rendent la chaîne analytique beaucoup moins triviale à gérer qu’en entreprise.\n\nL’éloignement, la diversité des données ainsi que le manque de formation « data » des personnels en local accentuent également les problèmes de qualité de données. La majorité des informations clés n’est pas encore digitalisée et va nécessiter une intervention manuelle pour être captée. Ces phases de récupération de données vont devoir être simples et rapides : il n’est absolument pas concevable de faire passer de longs questionnaires à des populations qui se battent au quotidien pour faire vivre leurs familles. Enfin, les saisies ou les captages d’informations vont être sujets à des risques d’erreur du fait de l’environnement humain et technique et vont devoir être modérés par des recoupements et des inférences avec d’autres données captées.\n\nEnfin, la valeur des données locales ne se matérialise que lorsque celles-ci sont rassemblées, homogénéisées et mises en relation dans des modèles qui vont aider les analystes à comprendre la situation dans son ensemble et dans la durée.\n\nImpact Atlas apporte une solution robuste et simple, compatible avec ces milieux peu propices au déploiement de technologies. Nous couvrons les derniers kilomètres de la donnée pour assurer la collecte durable et précise des informations réellement pertinentes pour la compréhension et le suivi des causes que nous supportons.\n\n\\> Comment cela marche concrètement ?\n\nImpact Atlas va tout d’abord limiter la barrière technique inhérente à chaque capture de données. Nous travaillons sur des appareils mobiles à grande autonomie, qui peuvent fonctionner sans réseau. Ils peuvent capturer la donnée partout où un humain peut aller. Nos collecteurs peuvent parfois faire plusieurs jours de trajet pour arriver dans les villages reculés : ils synchronisent leur application avant leur départ pour charger les questionnaires et feront de même à leur retour ou dès l’accès à un réseau pour remonter les informations collectées.\n\nLa barrière de la langue est aussi traitée par un design multilingue des interfaces pour prendre en compte les dialectes locaux.\n\nNous travaillons aussi à une collecte « intelligente » de l’information. Nous nous concentrons sur la donnée nécessaire et nous évitons les questionnaires à rallonge qui découragent et finissent par manquer de consistance. Nos systèmes de collecte s’adaptent aux données précédemment captées pour progressivement compléter les données manquantes ou éventuellement demander des données complémentaires suite à une observation particulière. Par exemple, si, lors d’une rencontre, la présence d’enfants est remarquée dans une ferme, alors des questions sur leur scolarisation pourront être proposées.\n\nNous sommes également soucieux de la qualité de l’information collectée. Compte tenu de la dissémination des endroits de collecte, des personnels que nous ne pouvons pas tous motiver et former de la même manière, nous apportons une attention particulière à la validation des prises de mesures. Nous ne pouvons pas imposer des contraintes complexes qui seraient trop lourdes et qui nuiraient à la fluidité du processus. Nous travaillons beaucoup avec la géolocalisation des saisies pour nous assurer que la mesure est bien faite in situ. Nous collectons également des photos prises par les appareils mobiles qui nous apportent non seulement une preuve visuelle et des coordonnées géographiques, une validation de l’identité des personnes sondées, mais également de nombreuses informations supplémentaires que nous pourrons interpréter et capturer par la suite, notamment grâce à des solutions de reconnaissance d’image.\n\nNous centralisons ensuite toutes ces informations en un modèle de données holistique, vraiment représentatif d’une situation terrain, témoignage des progrès concrets d’une initiative.\n\nFinalement, nous apportons la transparence nécessaire sur ces projets complexes, grâce à des mesures pertinentes, validées et régulières.\n\n\\> Quelles sont les clés de réussite d’un projet avec Impact Atlas ? Quelles leçons un data pionnier peut-il en retenir ?\n\nPosez la bonne question et choisissez les bonnes données ! Nous avons d’ailleurs souvent recours au Design Thinking pour cerner les problématiques correctement. Sans cette phase initiale, le programme peut perdre tout son impact. Des programmes captent trop de données et se noient dans leur analyse. D’autres mesurent ce qui n’est pas pertinent et n’ont aucune idée de l’impact de leurs actions.\n\nAdaptez-vous au contexte de votre audience. A-t-elle le temps, la formation, les moyens, la motivation ou la formation pour fournir la donnée ? Comment pouvez-vous l’aider ou lui rendre la vie plus facile ?\n\nSoyez créatif et innovant pour aller chercher l’information clé qui vous manque.\n\nRestez toujours respectueux de l’information que vous collectez. Au-delà de nos missions d’analyse, nous avons aussi un devoir moral et légal de respecter la vie privée et l’éthique."
        }
      },
      {
        "json": {
          "pageId": "PAGE128_1757157753085",
          "text": "# Corinne Plourde"
        }
      },
      {
        "json": {
          "pageId": "PAGE129_1757157753085",
          "text": "## Le data pionnier face aux défis de l’éthique en data\n\nCorinne Plourde, Docteur en informatique dans le domaine des bases de données déductives, partage son expérience entre la mise en place de la sécurité de la donnée et de ses processus et les audits de compliance, ISO 27001 comme RGPD. Formatrice et enseignante en université (université Paris 1 Sorbonne, université Jean Monnet Saint-Étienne), Corinne a mis l’humain au centre de ses passions. Elle partage ses compétences entre pédagogie, créativité et qualification dans un poste de directrice pédagogique. Elle allie la recher­che fondamentale, l’expérience dans la montée en compétences et la technique pure. L’informatique fut une de ses passions depuis toute petite et l’apprentissage dans ce domaine, une volonté constante.\n\n« Lorsqu’on utilise le mot « éthique », on vise ce qui est juste. L’éthique nécessite donc une confrontation entre des principes issus des valeurs, du droit et de la réalité vécue. Par conséquent et par définition, il n’y a pas de vérité éthique déjà établie, qui soit valable de la même façon partout et pour tout le monde. Nous devons utiliser des références pour nous guider.\n\nIl est nécessaire que la prise en compte de l’éthique vienne de l’intérieur de la profession. Une pression qui vient de l’intérieur peut permettre de forcer les entreprises à revoir leur politique éthique indépendamment des considérations économiques. L’intégration des considérations éthiques dans la formation des data pionniers va éviter de réduire l’analytique à ses aspects purement techniques. Voici quelques pistes pour nous guider. »\n\n\\> Équilibrer le cœur et le cerveau\n\nAvec le recul que nous possédons aujourd’hui, nous pouvons observer que les avancées dans le monde de l’analytique se composent de deux parties distinctes : celle qui vient du cerveau et celle qui vient du cœur.\n\nLes avancées commencent par la partie cerveau. Et une fois que la conception, la technique, le développement et le design sont réalisés, arrive la question du cœur, de l’humain. Pour parler de « Data éthique », que ce soit par la machine ou par l’humain, nous sommes confrontés aux mêmes défis : comment prédire, prévoir, analyser et construire de nouvelles données tout en contrôlant leur contenu et en ne mettant pas en cause la vie privée des personnes et, par conséquent, sans toucher à leur liberté ?\n\nLe data pionnier doit allier le cœur au cerveau, la morale à la technologie, l’éthique à la data. Il doit aborder toutes les dimensions des avancées humaines du futur sans perdre le sens de ses objectifs. Dans cette perspective, il sera nécessaire de réintroduire dans les projets data, au travers d’approches transdisciplinaires, tout ce qui est constitutif de notre savoir et qui nous préserve dans notre humanisme au sein des organisations.\n\n\\> Comprendre la force et les dangers du Smart Data\n\nLe Big Data proposait la collecte massive de données, sans forcément attacher des objectifs d’analyse précis. Aujourd’hui, on parle désormais de « donnée intelligente » (Smart Data). La donnée intelligente est produite par la construction de modèles de données riches et holistiques, par nos analyses et par nos algorithmes de Machine Learning. La Smart Data représente les données et les informations les plus intéressantes et pertinentes au regard des objectifs de l’entreprise.\n\nLes Smart Data permettent une connaissance très précise des personnes et de leur sociologie. Elles offrent des analyses très détaillées qui sont en mesure de conditionner la réalité de populations entières : profilage, fake news, élections, utilisation abusive des données à caractère personnel.\n\nLe data pionnier doit avoir conscience de la portée de ses pouvoirs en termes de collecte, de traitement, d’assemblage et d’analyse. Il doit être le premier garde-fou des abus.\n\n\\> S’appuyer sur l’éthique par défaut et par conception\n\nLe principe « d’éthique par défaut » reprend le concept du RGPD (Règlement général sur la protection des données) avec la protection de la vie privée par défaut. Il consiste à n’exploiter et donc à ne collecter que les données nécessaires aux traitements répondant aux objectifs établis par l’organisation ou la business unit.\n\nEn complément de l’éthique par défaut, l’éthique par conception (by design) va, elle, gérer la conception des processus. Elle répond à la problématique de contrôle et de maîtrise des effets occasionnés par les nouvelles technologies et analyses.\n\nCes réflexions éthiques en amont des techniques sont essentielles à la pensée d’un projet dans son ensemble. Elles peuvent être soutenues par des méthodologies que l’on retrouve dans le champ du design (scénarios d’anticipation, tests utilisateurs, etc.).\n\n\\> Craindre l’humain plus que la machine\n\nCe n’est pas tant la machine qui est à craindre que les humains qui sont derrière et qui en ont la maîtrise et le contrôle.\n\nLa mise en place de mécanismes de supervision des algorithmes permettant le respect de la vie privée et des résultats socialement acceptables demandent un cadre éthique pour réguler les risques de conception d’un système prédateur, tourné vers son seul intérêt. Ces systèmes sont d’autant plus dangereux qu’ils sont capables de prendre des décisions de moins en moins transparentes et de plus en plus rapidement.\n\nEn imitant les humains, les machines amplifient tous leurs comportements… Sachons faire preuve d’exemplarité dans l’exercice des valeurs qui caractérisent notre humanité et qui se retrouveront dans la programmation des machines.\n\n\\> Faire de la diversité un objectif central\n\nLe manque de diversité dans la communauté des analystes induit une homogénéisation des façons de penser. L’éthique est avant tout une discussion qui est enrichie par l’apport de visions différentes. Sans une diversité de profils, les projets analytiques seront invariablement exposés à des biais plus forts et rarement identifiés comme problématiques.\n\n\\> Maintenir loyauté et vigilance\n\nLe principe de loyauté : tout algorithme, qu’il traite ou non des données personnelles, doit être loyal envers ses utilisateurs, non pas seulement les consommateurs, mais également les citoyens, voire envers des communautés ou de grands intérêts collectifs dont l’existence pourrait être directement affectée.\n\nLe principe de vigilance : il s’agit d’organiser une forme de questionnement régulier, méthodique et délibératif à l’égard de ces objets mouvants, avec la constitution d’un comité éthique par exemple.\n\nEn intégrant ces réflexions dès le début de leur cheminement, les data pionniers se prépareront progressivement aux défis éthiques croissants de leurs analytiques."
        }
      },
      {
        "json": {
          "pageId": "PAGE130_1757157753085",
          "text": "# Sea Matilda Bez"
        }
      },
      {
        "json": {
          "pageId": "PAGE131_1757157753085",
          "text": "## Les défis de l’innovation du data pionnier\n\nMatilda Bez est maître de conférences à l’université de Montpellier. Elle a passé deux ans à Berkeley en tant que post-doc avec Henry Chesbrough, professeur considéré comme le père de l’Open Inno­vation. Durant ces deux années, elle a étudié les défis de l’Open Innovation dans des contextes coopétitifs (quand des concurrents coopèrent).\n\n« Grâce à une analytique performante, les Pionniers disposent de plus de temps pour réfléchir. Comment peut-on élever notre capacité à réfléchir et à innover ?\n\nNous pouvons commencer par voir plus loin dans nos réflexions et leur apporter une dimension sociétale ou environnementale.\n\nSouvent, quand on pense à l’innovation, on pense innovation incrémentale. Il faudrait passer plus de temps à réfléchir à ce qu’on appelle des Moonshots (viser la lune). Ce sont des innovations radicales qui ont pour but de résoudre un besoin sociétal (sustainable development goals). Et pendant longtemps, on a vu le profit et ces objectifs sociétaux comme deux buts opposés. On opposait l’entreprise qui ne veut faire que du profit et l’organisation à but non lucratif qui a pour but de faire du développement durable. Aujourd’hui, on réalise que ces deux buts ne sont pas opposés : la recherche du profit peut consister à chercher à résoudre un but de développement durable.\n\nPar exemple, Enel, une entreprise d’énergie italienne, a décidé de se consacrer à l’accès universel à l’énergie abordable. Cette stratégie ouvre deux opportunités :\n\n– c’est un énorme marché : 1,2 milliard de personnes n’ont pas accès à de l’énergie à l’heure actuelle, et 1,8 milliard de personnes n’y ont pas accès de manière fiable ou à un coût raisonnable ;\n\n– c’est un défi qui va attirer les talents qui ont un vrai potentiel innovant. Ces derniers ne sont plus intéressés par l’argent. Ils veulent que leur métier ait du sens. La recherche d’innovations en développement durable donne une raison d’être à leur engagement. »\n\n\\> Quelles sont les clés pour renforcer son potentiel d’innovation, que l’on soit un individu ou une organisation ?\n\nIl faut d’abord se rendre compte de l’énorme écosystème de start-up qui est présent et prêt à aider l’entreprise à accélérer sa transformation digitale à un coût plus faible et moins risqué. Il ne faut pas tomber dans le piège de vouloir tout faire tout seul, mais au contraire ouvrir les murs de son entreprise à ces start-up et accepter leur aide. Il faut mobiliser ou créer cet écosystème de start-up qui nous entourent pour innover.\n\nPar ailleurs, quand une organisation décide justement de s’engager avec des partenaires de l’écosystème, elle doit se rendre attractive. C’est à l’organisation de séduire ses partenaires dans une collabo­ration. Nous pouvons même dire qu’elle doit faire un « pitch » aux start-up sur les raisons de travailler avec elle.\n\nIl faut également considérer que la compétition n’est pas uniquement au niveau de l’entreprise, mais au niveau de l’industrie. Il y a des industries plus ou moins attractives pour les start-up, surtout celles qui ont le vent en poupe. Comment peuvent-elles attirer des start-up dans leurs industries considérées comme plus traditionnelles ? En collaborant avec leurs concurrents. Par exemple, en créant une base de données commune, elles vont attirer les entrepreneurs talentueux qui veulent tester et développer des algorithmes de Machine Learning ou d’automatisation. Ainsi, Telefonica, Orange, Deutsche Telekom et Singtel ont créé une initiative qui s’appelle Go Ignite, où ils ont mis en commun leur base de données, représentant plus d’un milliard de clients, particuliers et entreprises, sur cinq continents. Et avec cette énorme base de données, ils font des appels aux start-up pour innover sur la 5G, sur la Blockchain et autres sujets de pointe. Ce trésor de données leur permet d’attirer beaucoup plus de start-up qu’ils n’auraient pu le faire séparément.\n\nUn dernier point, c’est ce que j’appelle le multi-unit back-end problem : le problème de l’arrière-ligne des entreprises multi-départements/multifiliales. C’est la difficulté à connecter les équipes de l’entreprise et à les faire collaborer. On sous-estime la compétition qu’il y a entre les départements. Cette absence de collaboration met les transformations digitales en danger.\n\nQuand on réfléchit sur des projets data avec des start-up partenaires, on se focalise souvent sur la relation entreprise-start-up. Comment mettre en synergie deux entités de tailles différentes, avec des façons de fonctionner différentes ? Eh oui, c’est vrai que cela peut être une difficulté, et cette relation ne peut pas marcher si l’on n’a pas fait coopérer les départements en interne en amont.\n\nPour illustrer cela, en l’occurrence un échec, je pense à un groupe bancaire français, qui avait au départ beaucoup de difficulté à avoir des résultats dans leurs initiatives de transformation digitale avec des partenaires start-up. Pourtant, l’entreprise était dans une situation idéale. Les managers étaient impliqués et motivés par le projet. Les différents départements avaient conscience qu’il fallait faire de l’innovation digitale. Il y avait vraiment une demande client pour de meilleurs services ou produits, qui allait nécessiter le développement d’algorithmes de Machine Learning ou d’automatisation. Le groupe avait tout un écosystème de start-up motivées pour travailler avec lui et même un budget de 600 millions d’euros sur trois ans pour créer une équipe dédiée à la mise en place des initiatives. Tout aurait dû fonctionner. Mais, au final, il n’est pas arrivé à avoir de résultats.\n\nQuand ma collègue Thuy Seran et moi avons cherché à comprendre pourquoi, nous n’avons pas identifié de problèmes de collaboration avec les start-up. Tout venait des problèmes internes et de la compétition entre les départements. Nous avons trouvé trois exemples représentatifs :\n\n– dans une mise en commun des données au sein de l’entreprise, partager la donnée ne suffit pas. Il faut que ces données soient de qualité et données à temps. Les différentes équipes avaient peur de partager leurs données, comme leurs meilleurs clients ;\n\n– le deuxième problème était le manque de transparence des équipes et des agences. Pour qu’une collaboration dans la création d’algorithmes de Machine Learning pertinents et utiles réussisse, il faut que les objectifs soient clairs, que les problèmes soient partagés en toute transparence et que toute la donnée pertinente soit mise à disposition. Il faut que les équipes et les partenaires puissent échanger. Mais, là encore, ce partage n’existait pas pour des raisons de méfiance et de peur ;\n\n– en dernier lieu, les synergies ne se produiront que si les équipes prennent du temps pour codévelopper avec les partenaires et pour tester les algorithmes par itération. Si les résultats sont là, ils seront partagés avec l’ensemble des autres équipes, qui n’auront pas contribué à l’effort. Pourquoi devrions-nous prendre le temps et le budget de nos équipes pour des développements, quand d’autres vont en bénéficier gratuitement ?\n\nC’est que nous qualifions de coopétition interne un mélange de compétition et de coopération en interne qui nuit à l’innovation et au progrès.\n\n\\> Comment fait-on pour gérer cette compétition qui peut être si ancrée dans les organisations ?\n\nCela se gère. C’est ce que ce groupe a d’ailleurs entrepris. Ils ont choisi de gérer cette compétition interne avec plusieurs types d’actions :\n\n– ils ont mis en place un système d’anonymisation des données partagées pour éviter que des départements n’accèdent à trop de détails de leurs collègues ;\n\n– ils ont ensuite offert un budget bonus pour l’achat de solutions développées par les start-up pour les départements qui ont donné leurs données ;\n\n– ils se sont rendu compte que partager un besoin entre départements était trop sensible pour les besoins réels du moment. Le groupe a donc incité à partager les problèmes pour lesquels les équipes n’avaient ni d’argent ni de temps à investir pour trouver une solution, évitant ainsi les comportements individuels. Pour les besoins réels, ils proposent des solutions locales qui se limitent au département et à la start-up (les autres départements ne sont pas au courant). Avec cette distinction entre besoin du moment et besoin futur, le problème de la peur ou de la jalousie entre départements était traité ;\n\n– en dernier lieu, ils espèrent motiver les équipes à tester et à construire des solutions en leur donnant un avantage concurrentiel en termes de droit d’utilisation. Si un département développe un algorithme, il disposera d’une période d’exclusivité. Une fois un retour investissement obtenu par l’algorithme, les autres départements pourront utiliser ce dernier. Comme une sorte de brevet interne ! C’est un bénéfice aux Pionniers.\n\n\\> Quels conseils donnerais-tu aux Pionniers ?\n\nJ’aurais trois conseils à donner.\n\n– Acceptez l’attitude « innovation ouverte avec des concurrents » (open coopetitive mindset). Soyez prêt à collaborer avec des concurrents et à en faire des alliés pour être encore plus pertinent dans vos approches analytiques et plus apte à attirer des partenaires et des talents pour collaborer.\n\n– N’ayez pas peur de paraître stupide. C’est un concept que j’ai développé avec Henry Chesbrough lorsque j’étais à Berkeley : Fear of Looking Foolish (« peur d’avoir l’air stupide »). Il y a des idées extraordinaires qui vont rester sur « une étagère inutilisée » car les managers ne vont pas laisser cette idée être développée par quelqu’un d’autre à l’extérieur de l’entreprise. En effet, si cette personne extérieure réussit à transformer l’idée en succès commercial, le manager qui aura laissé l’idée sortir de l’entreprise aura l’air stupide. Le manager peut même avoir peur de représailles : « Comment n’as-tu pas pu voir le potentiel derrière l’idée ? » Alors que, au contraire, il devrait être fier et sa hiérarchie devrait le féliciter d’avoir réussi à trouver la personne adéquate pour réaliser ce succès et idéalement négocier des royalties sur ce dernier.\n\n– Agissez pour limiter les biais et les risques de vos algorithmes de Machine Learning ou d’automatisation, mais aussi des algorithmes de vos concurrents. Il suffit d’une entreprise avec un algorithme aux conséquences dramatiques pour gâcher la crédibilité de toute l’industrie. Comportez-vous comme une fratrie même avec vos concurrents, surveillez leur manière de construire les algorithmes, éduquez-les sur les biais et les risques des algorithmes, et collaborez pour trouver ensemble des solutions."
        }
      },
      {
        "json": {
          "pageId": "PAGE132_1757157753085",
          "text": "# Nolwenn Godard"
        }
      },
      {
        "json": {
          "pageId": "PAGE133_1757157753085",
          "text": "## Couvrez cette domination masculine que je ne saurais voir dans ma data\n\nNolwenn Godard, cadre dans la FinTech, est dotée d’une riche expérience de création de produits et de plateformes dans un environnement mondialisé, dynamique et collaboratif. Elle s’emploie à mettre la technologie au service de la création de valeur et de l’impact social. Diplômée de l’Essec, ses expériences de carrière incluent un passage chez Sofi, PayPal, Ubisoft et une ONG en microfinance au Proche-Orient. Elle a été distinguée comme faisant partie des cent femmes les plus influentes de la Silicon Valley en 2018.\n\n« Nous avons une crise de représentation des femmes dans l’intelligence artificielle », nous dit Fei Li, à quelque dix-huit mille femmes et à moi-même à Grace Hopper 2017. Ces mots m’interpellent. Comment puis-je aider à changer la donne ?\n\nLa quatrième révolution industrielle arrive avec ses grands enjeux de société et ses questions : quel futur commun voulons-nous ? Comment éviter une dystopie à la Black Mirror 25 et tendre vers une société au développement durable, inclusif et écologique ? Comment former et redéployer la main-d’œuvre dont les emplois actuels seront détruits demain par l’automatisation, la communication de machine à machine et l’Internet des objets ? Qui aura accès au développement de compétences nouvelles ? Quelle entreprise ou instance gouvernementale aura accès au rare talent technologique, dans un monde « anumérique », comme il a autrefois été analphabète, pour réaliser ses projets ou se protéger contre les cyberattaques ? Comment s’assurer que les algorithmes seront équitables, à l’embauche, dans l’attribution des peines judiciaires et jusque dans nos livraisons quotidiennes ? Comment faire pour que nous ne vivions pas encore plus dans des mondes parallèles, pour que nous restions « libres » de nos choix et pour que les discriminations passées ne soient amplifiées ni gravées pour toujours dans « le grand code planétaire », sans possibilité de s’en affranchir ni de comprendre son fonctionnement ?\n\nCertes, comme dans d’autres domaines de pointe (nucléaire, médecine, recherche, etc.), l’expertise technique ne sera pas universellement répartie, mais il faut penser et organiser un système assurant le bien-être de l’humanité. Nous devons enfanter le monde de demain ensemble : gouvernements, entreprises, ONG, femmes et hommes politiques, société civile du monde entier. « La technologie est trop importante pour qu’on la laisse aux seuls technologistes26 », comme le dit si justement Azeem Azhar. Les progrès technique et moral n’étant pas corrélés, nous ne pouvons être attentistes.\n\nLa réflexion dans nos entreprises est encore balbutiante et sans commune mesure avec le besoin de penser et d’amorcer le changement. On entend de si beaux discours sur la diversité, l’inclusion, l’appartenance, l’équité et leurs bénéfices au-delà d’une simple justice (d’ailleurs toujours dévalorisée, insuffisante) : de meilleurs résultats financiers, plus d’innovation et de brevets, une amélioration du bien-être et de satisfaction au travail.\n\nMais, au quotidien, rien ne semble bouger. Les femmes exécutives en Tech et Data Science que je rencontre, quels que soient leur grade ou leurs années d’expérience, sont lassées par le décalage entre la rhétorique et la pratique. Pourquoi cette usure ?\n\nLes lignes suivantes vous révèlent ce dont un « je » collectif pourrait témoigner. Partager cette voix plurielle, cet échantillon d’expériences vécues sous différentes formes par des milliers de femmes s’inscrit dans une démarche d’interruption du silence et parfois du déni qui contribuent à perpétuer le système établi.\n\n\\> Une journée d’Ivanna Denisova, data analyste\n\n« Dès l’aube, à l’heure où blanchit la campagne, j’enfile ma tenue de combat (tailleur, jeans ou sari). Comme chaque jour, je suis la seule femme pour environ dix hommes dans mon équipe et dans toutes mes interactions. Les réunions s’enchaînent et l’on me coupe cons­tamment la parole… Que puis-je bien avoir d’intéressant à dire ? À la fin des sessions, on me dit qu’on ne m’entend pas assez, que je ne suis pas assez sûre de moi, que je n’ai pas d’opinion, ou que, si j’apportais de la valeur, on m’écouterait. Sur la visioconférence suivante (par chance on m’écoute), on me dit qu’il faut que je sois plus claire, qu’on ne comprend pas mon propos. Lorsqu’un collègue masculin le reformule à l’identique, on ne manque pas de le féliciter.\n\nMa journée se poursuit avec des rencontres clients et fournisseurs. Ils n’osent pas me demander d’apporter le café, mais me considèrent de facto comme leur scribe. Mes analyses, mes rapports sont commentés “sans moi” : impossible d’interrompre la “partie de ping-pong” qui se joue sous mes yeux…\n\nLors d’un atelier technique en début d’après-midi, il y a forcément un confrère pour m’expliquer d’emblée le Master Data Management, les règles de jointures ou l’ajout d’un script Python : malgré mes dix années d’expérience en Data Science, je n’échappe pas au Mansplaining27. J’ai toujours du mal à m’y faire.\n\nPuis il faut se battre et garder la tête haute… J’ai d’autres projets à défendre. Malgré les succès de mon équipe, je dois insister auprès de ma hiérarchie et des RH pour qu’une de mes employées en congé maternité soit promue. Je me vois aussi refuser un congé pour être auprès de mon père convalescent.\n\nAlors que je demande un peu plus de ressources pour renforcer le contrôle de qualité de nos données, un pair ingénieur me hurle dessus devant mon équipe : “Comment oses-tu demander autant de ressources ? Tu ne vas tout de même pas avoir une équipe plus grosse que la mienne !”\n\nLes quelques femmes autour de moi, qui se plaisent à citer Madeleine Albright28, “il y a une place spéciale en enfer pour les femmes qui n’aident pas les femmes”, ne prennent pas ma défense : elles aussi doivent survivre dans le monde de la data. Comment leur en vouloir ?\n\nUne journée classique dans ma vie de data analyste, déjà parsemée de moqueries en public, d’humiliations et de menaces en entretiens hebdomadaires, ou de pression psychologique, se termine par l’inévitable harcèlement frontal lorsque nous nous retrouvons en dehors du bureau pour célébrer un succès en équipe.\n\nIl se fait tard, mon patron m’annonce en off que l’on va me retirer ce que j’ai construit à partir de rien. Mes systèmes d’analyse, mes algorithmes, mes travaux de gouvernance de données dont personne ne voulait, les talents que j’aurai embauchés et formés à la data se sont tous transformés en « objet de désir » que plusieurs désormais convoitent. Il me dit qu’il faut laisser la place à Bob sous prétexte qu’il est une star, que je manque de charisme, que je suis la seule du département à ne pas avoir d’enfants, et que je serai donc parfaite pour ce nouveau rôle (un placard) qui nécessite de nombreux déplacements, ou encore que mon succès a offensé un supérieur… La nouvelle de ma « mutation » annoncée, mon patron déclare son aspiration à améliorer la diversité de son équipe.\n\nMe sentant en retrait au terme d’une journée qui s’éternise, il me conseille sous le couvert du conseil amical, de quitter la Tech pour ouvrir un restaurant. Je me prête au jeu et je finis par lui annoncer que je pars rejoindre une start-up en pointe dans l’analytique avancée. Mon manager s’étrangle de rage et dit : “Si j’étais ton père et que tu étais ma fille, je j’interdirais de quitter notre société.”\n\nJe termine cette journée, qui comme les précédentes, m’a montré que je n’étais pas à ma place. Avant d’éteindre ordinateur et téléphone, je prends le temps de revoir les résultats d’une reconnaissance d’images lancée il y deux jours pour la détection de défaut sur des pièces avec les équipes de production. Je viens en renfort boucler les dernières lignes de code sur le développement de l’application de scoring client développé avec la Finance. Je passe encore un peu de temps en ligne avec ma communauté d’analystes pour “cracker” un problème de visualisation de données. Je souris, j’aime mon métier, ces défis et toutes ces collaborations qu’il permet. C’est le domaine dans lequel j’excelle. »\n\n\\> Demain sera-t-il un autre jour ?“\n\nAujourd’hui, 50 % des femmes quittent la Tech avant 35 ans, et à un taux de 45 % plus élevé que les hommes29. Interrogées, seulement 21 % des femmes pensent qu’elles peuvent prospérer dans cette industrie, un chiffre qui tombe à 8 % pour les femmes de couleur. Confronter la constante présomption d’incompétence, le sentiment d’illégitimité, les micro-agressions ou le harcèlement sexuel des managers ou des VC (venture capitalist ou capital-risqueur), le manque d’avancement, d’opportunités de carrière ou de financement, et les journées à rallonge couvrant trois continents, c’est épuisant. Elles se reconvertissent ou quittent le marché du travail. Dommage, car on manque justement de talents en Tech.\n\n\\> Quelles solutions ?\n\nPour s’attaquer aux problèmes structurels de sexisme, de racisme, de classes/castes, il faut des changements structurels. Les déclarations d’intention ne suffisent pas. La volonté politique (publique, privée) incarnée dans l’action est primordiale. Dans son discours de l’université de Rice en 1962, Kennedy déclare un objectif très ambitieux, défini et délimité dans le temps : « Nous avons choisi d’aller sur la Lune au cours de cette décennie \\[…\\], non pas parce que c’est facile, mais justement parce que c’est difficile. Parce que cet objectif \\[…\\] est le défi que nous sommes prêts à relever, celui que nous refusons de remettre à plus tard, celui que nous avons la ferme intention de remporter. » Kennedy n’a pas dit : « Nous aimerions aller sur la Lune au cours de cette décennie » ni « nous allons former un comité sur l’espace dans cette décennie ». L’objectif fut atteint en sept ans.\n\n\\> Organisations pionnières de la data : que pouvons-nous faire ?\n\nComme nous y invite Paloma Medina30, remplaçons « aller sur la lune » par « atteindre l’égalité des genres et des origines ethniques » et formulons ces mêmes objectifs ambitieux : atteignons l’objectif d’égalité dans notre processus de recrutement d’ici à trois ans ; accroissons de 50 % la représentation des femmes dans notre équipe exécutive en trois ans ; faisons un audit et atteignons l’égalité des salaires et des promotions en deux ans… Il faut tracer, mesurer, rectifier, avoir des incitations à la clé (bonus si objectifs atteints), comme pour tout autre indicateur business.\n\nAdoptons une proactivité systématique. Les RH et les hautes sphères de l’entreprise devront recadrer les comportements discriminants, former au repérage des biais et donner aux managers des outils pour les réduire. Alors que les minorités sont de plus en plus conscientes des biais cognitifs à leur égard, la majorité est très peu formée et donc assez ignorante sur les questions d’équité, d’inclusion et de diversité. Le décentrement est difficile à acquérir pour la majorité qui a été habituée à être le référant universel. La sensibilisation à l’intersectionnalité et l’utilisation du Design Thinking pour créer une expérience de travail adaptée à celle-ci garantiront une meilleure expérience pour tous. « Si vous travaillez dur et que vous demandez de l’aide, vous y arriverez » : on aimerait que cette promesse méritocratique de Maria Klawe à ses élèves du Harvey Mudd College soit étendue au monde de l’entreprise.\n\n\\> Faisons de la place"
        }
      },
      {
        "json": {
          "pageId": "PAGE134_1757157753085",
          "text": "Les groupes de salariés représentant les femmes et les différentes minorités au sein de l’entreprise sont utiles comme réseaux de soutien, d’inspiration, ou comme plateformes d’enseignement des codes nécessaires à adopter pour arpenter l’univers de la culture dominante, mais ils ne doivent pas être perçus comme l’outil de changement dans l’organisation. Le cœur du problème est la volonté d’inclure et de garantir l’égalité. Pensons et orchestrons le difficile partage de pouvoir. Appuyons-nous sur des objectifs individuels motivant le changement (promouvoir un homme qui prend un congé paternité, bonus si objectifs de diversité atteints, etc.), car ceux de l’entreprise ne suffisent pas.\n\nTels les Stoïciens, anticipons le pire et construisons des processus pour pallier les défaillances humaines et limiter les abus de pouvoir. Des plateformes tierces pourraient faciliter le signalement du harcèlement sexuel. Formons à la communication non violente, au respect, à la bienveillance, à la bientraitance, tout en gardant à l’esprit que certains groupes démographiques sont statistiquement plus exposés aux abus et aux sujets à l’état de stress post-traumatique. Ces pratiques font cruellement défaut dans nos entreprises aujourd’hui.\n\nEn tant que data pionnier, nous avons tous un rôle à jouer. Soyons des pionnières pour d’autres femmes, d’autres minorités. Embauchons-les, promouvons-les. Que les hommes soient ceux par qui le changement adviendra. Les femmes ne leur demandent pas une discrimination positive, mais juste de ne pas entraver leur carrière.\n\nEn tant qu’individus, utilisons notre pouvoir collectif pour transformer le pouvoir institutionnel par des lois, des choix d’investissement public, des expérimentations sociales, condition pour que notre travail d’équité et d’inclusion ait un impact à grande échelle. Soyons des acteurs délibérés du changement. Comme nous y invite Victoria Dimitrakopulos, catalyseur Social and Emotional Learning (SEL) dans l’éducation primaire et secondaire, « c’est à notre tour de combler les écarts, d’être les gardiens des valeurs humaines, de poursuivre nos aspirations pour le monde sans attendre d’encouragement, (et) de nous sauver »."
        }
      },
      {
        "json": {
          "pageId": "PAGE135_1757157753085",
          "text": "# Odile Roujol"
        }
      },
      {
        "json": {
          "pageId": "PAGE136_1757157753085",
          "text": "## Fédérer la donnée entreprise : les premiers pas d’une Chief Data Officer (CDO)\n\nAprès quinze ans dans des grands groupes comme L’Oréal, Chanel, Saint Laurent, Odile a passé six ans en tant que Chief Data Officer chez Orange France. Elle a ensuite rejoint la Silicon Valley et dirige un fond de consumers brands créé pour des communautés social media et e-commerce engagées sur des problématiques data.\n\n\\> Quels éléments ont été clés pour votre position de CDO ?\n\nCe qui a primé pour moi, en tant que Chief Data Officer, c’est avant tout la connaissance métier. Je ne suis pas ingénieure et ma force (ou mon avantage), c’était la compréhension opérationnelle de ce que nous cherchions à mesurer.\n\nIl m’a fallu aussi un appui au plus haut niveau de l’entreprise, car la gouvernance de données dans un groupe mondial est une entreprise qui va nécessiter la fédération d’équipes variées. Je devais composer avec le marketing, le commercial, l’e-commerce, la production, etc, qui étaient des fonctions qui ne me reportaient pas. Ma position était rattachée au comité exécutif.\n\nJ’ai aussi veillé à entretenir une synergie durable avec ces équipes en combinant ma vision holistique à leur expertise terrain.\n\nEnfin, j’ai poussé l’agilité dans nos projets data. Certes, c’est un mot devenu « tarte à la crème », mais en l’occurrence, pour coller au terrain, comprendre, ajuster, corriger notre compréhension de marchés en constante évolution, je me suis appuyée sur des équipes compactes et rapides, les fameuses « équipes pizza » (pizza team), dont le nombre d’individus doit rester limité à celui d’un groupe qui aurait à se partager une pizza.\n\n\\> Quels défis avez-vous rencontrés ?\n\nJ’ai beaucoup travaillé sur la démocratisation du sujet. Aujourd’hui, toutes les entreprises réalisent que la data n’est pas que l’affaire d’une petite équipe de techniciens ou d’analystes. Tout le monde a un rôle à jouer tout au long de la chaîne analytique. Il y a dix ans, c’était moins commun. J’ai donc développé des programmes d’acculturation avec les ressources humaines, notamment par le jeu.\n\nJ’ai aussi contribué à donner un sens à notre gouvernance de données. Nous avons expliqué notre objectif : au delà d’être data-driven, c’était de mieux servir le client, de mieux le comprendre, de lui proposer de meilleurs produits et des solutions adaptées. Il ne s’agit pas de faire de la data pour de la data.\n\nL’autre grand défi a été celui de la transition de la donnée « froide » que nous utilisions traditionnellement à la donnée « chaude » plus proche du temps réel. Les questionnaires et les enquêtes nous donnaient auparavant des informations avec trois à six mois de décalage. En l’espace d’une décennie, nous sommes devenus capables d’avoir de la donnée très proche du temps réel. L’agilité évoquée plus haut est devenue critique pour constamment s’adapter aux nouvelles opportunités.\n\nEnfin, nous avons dès le départ ancré, si ce n’est sacralisé, le respect de la donnée privée.\n\n\\> Quelles qualités demandez-vous aux data pionniers ?\n\nJ’ai eu la chance de travailler avec D.J. Patel, ancien conseiller data de l’administration Obama et je le rejoins sur la qualité première de l’analyste : la curiosité.\n\nApprendre la data, ce n’est pas compliqué : par contre, avoir l’esprit inquisiteur, la vision critique est une qualité qui est plus personnelle et… plus rare.\n\nFormez-vous, préparez-vous, apprenez à reconnaître vos biais. Ne laissez pas l’IA entre les mains d’équipes seulement techniques et invitez la diversité dans vos groupes de travail."
        }
      },
      {
        "json": {
          "pageId": "PAGE137_1757157753085",
          "text": "# Virginie Pez"
        }
      },
      {
        "json": {
          "pageId": "PAGE138_1757157753085",
          "text": "## Les nouvelles opportunités data dans la connaissance du client\n\nVirginie Pez est maître de conférences à l’université Paris II Panthéon-Assas et professeure chargée de cours à l’École polytechnique. Elle est titulaire d’un doctorat de l’université Paris-Dauphine (2010) et d’une habilitation à diriger des recherches de l’université Paris II Panthéon-Assas (2018). Celles-ci portent sur les stratégies clients et s’intéressent en particulier au comportement du consommateur et aux aspects psychologiques liés à la consommation. Elle accompagne régulièrement les entreprises sur ces problématiques (conférences de lancement d’ateliers, animation de groupes de travail, participation à des livres blancs, etc.). Elle a, entre autres, coordonné avec ses collègues l’écriture de l’ouvrage Stratégie clients augmentée (ISTE 2019). Ses travaux font l’objet de publications scientifiques dans des ouvrages, des revues académiques (Journal of Business Research, Recherche et Applications en Marketing, Journal of Marketing Management, Journal of Retailing and Consumer Services, Décisions Marketing, etc.), ou des revues professionnelles ou grand public. Virginie Pez est membre du laboratoire de recher­che LARGEPA (université Paris II Panthéon-Assas), du CCM (université Paris-Dauphine) et de i3-CRG (École polytechnique). Elle dirige le Master Marketing et Communication de Paris 2 (Master MC2) au sein de l’université Paris II Panthéon-Assas.\n\n\\> Qu’est-ce qui a changé pour les services marketing et ventes avec les nouvelles capacités d’acquisition et de traitement des data ?\n\nAu départ, rien ne change : les fondements restent les mêmes. Depuis la nuit des temps, les bons vendeurs savent combien il est important de développer un lien particulier avec les clients pour les fidéliser. Même si le fait d’être customer centric est devenu ces dernières décennies le mantra de bon nombre d‘entreprises, le fait de chercher à mieux connaître le client et à le mettre au centre des décisions n’est pas nouveau. Les archéologues ont par exemple retrouvé dans les fouilles de la Rome Antique des tablettes à écrire et des registres faisant état de manière très fine des transactions avec les clients, esquisse de nos bases de données modernes. Gandhi rappelait en son temps à quel point le client était la source de valeur essentielle des relations d’affaires et méritait d’être placé au centre de toutes les attentions : « Un client est le visiteur le plus important de nos locaux. Il ne dépend pas de nous. Nous sommes dépendants de lui. Il n’est pas une gêne dans notre travail. Il en est l’objectif. Il n’est pas étranger à nos affaires. Il en fait partie. Nous ne lui faisons pas une faveur en le servant. C’est lui qui nous fait cette faveur en nous en donnant l’occasion. ». Vous l’aurez donc compris, les fondamentaux restent les mêmes ! L’objectif est de générer une valeur mutuelle et de créer des relations saines, justes et équitables, seuls gages de fidélité. Mais, si cette philosophie reste inchangée, les avancées technologiques contemporaines nous offrent cependant des opportunités d’acquisition et d’exploitation des données clients inédites.\n\nIl y a désormais beaucoup d’informations disponibles, accessibles plus simplement, massivement et de manière plus économique\n\nOn peut mieux connaître son client et plus vite. Les entreprises profitent de chaque occasion de contact pour collecter des données sur leurs clients, et les enrichissent en continu via de multiples démarches.\n\nSur les canaux digitaux, chaque connexion peut être facilement tracée (sous réserve que l’utilisateur y ait consenti bien entendu). Les pages visitées, le parcours suivi, le temps passé par page, les produits visualisés, les informations lues, les produits mis au panier, les produits réellement achetés, etc. sont des informations accessibles et facilement collectées par l’intermédiaire des cookies installés sur les appareils. Il est également aisé désormais de collecter des informations liées à la localisation ou aux usages.\n\nEn magasin physique, la carte de fidélité ou le fichier client sont également des sources d’informations intarissables. Pour y adhérer, les consommateurs se voient demander leur identité, leur adresse ou code postal, leur adresse email, leurs coordonnées téléphoniques, leur date de naissance, voire la composition de leur foyer ou le prénom de leurs enfants. Chaque achat y est ensuite minutieusement enregistré : produits, date, heure, magasin. Ces données sont ensuite croisées pour dresser un profil le plus complet possible du client.\n\n\\> Quelles grandes approches ont changé la donne ?\n\nL’usage des réseaux sociaux : les partages d’informations sociales permettent de comprendre plus de nos clients.\n\nLes informations obtenues de façon directe par la collecte d’informations sur les canaux digitaux et/ou physiques peuvent être complétées par des données issues des réseaux sociaux, de manière plus ou moins automatisée en fonction de la maturité relationnelle des entreprises. Même si cette pratique n’est pas généralisée aujour­d’hui (seulement 40 % des entreprises intègrent des données issues des réseaux sociaux dans leur base de données clients), il est très probable que ces informations soient plus largement utilisées à l’avenir au fur et à mesure que les entreprises se dotent d’outils performants. Les informations issues des réseaux sociaux sont en effet très riches. Chaque individu laisse sur la toile des informations sur son mode de vie, ses goûts, ses inspirations, etc., et ces éléments sont une vraie mine d’or pour les entreprises. Lorsque ces données qualitatives sont rapprochées des données clients issues des autres canaux de distribution (magasins physiques, site web, SAV, etc.), les marques obtiennent une véritable vision à 360° de leur client. Cette mine d’or d’informations, parfois très intimes, est encore peu exploitée aujourd’hui, faute de savoir en industrialiser le traitement. Mais leur utilisation devrait s’intensifier dans les années à venir.\n\nL’application du Machine Learning nous permet de voir des signaux plus fins, et ce, plus rapidement\n\nLes entreprises n’ont pas tardé à s’appuyer sur des machines pour automatiser le traitement des données clients, potentiellement en temps réel, afin d’en tirer du sens efficacement et d’optimiser leur potentiel. Parmi ces techniques, le Machine Learning figure en première ligne. Cette méthode permet à l’ordinateur d’apprendre des données clients sans être programmé de manière explicite. Concrè­tement, il s’agit de faire émerger des modèles de nature à prédire les événements à partir des données. Prenons l’exemple d’un opérateur télécom qui souhaiterait prévenir le départ de ses clients (ce que l’on appelle dans le secteur des services le churn, ou la résiliation client). Il s’agit pour lui de réussir à anticiper le départ de ses clients pour les traiter de manière préventive. En s’appuyant sur le Machine Learning, l’opérateur en question pourrait alimenter l’ordinateur avec une base de données de ce qu’il s’est produit dans le passé, détaillant le profil des clients (en termes de caractéristiques socio-démographiques et de comportements de consommation) et leur fidélité (ici de manière binaire : sont-ils restés chez l’opérateur ou ont-ils résilié leur contrat ?). En analysant l’ensemble de ces données, l’ordinateur va trouver des modèles (ou patterns) prédictifs de la fidélité ou de l’infidélité. L’opérateur pourra ainsi identifier en amont les clients susceptibles de résilier leur contrat, et les traiter de manière adéquate (réduction de prix, offre spéciale, etc.). De cette façon, le Machine Learning permet d’identifier des signaux de manière très fine, si besoin en temps réel, pour aider les managers à la prise de décision.\n\nLa démocratisation de l’accès à l’information dans les réseaux marketing et commerciaux apporte une meilleure connaissance client à tous les niveaux pour un meilleur service\n\nLes données clients sont autant de pépites qui permettent d’offrir un meilleur service aux clients et de développer de la valeur. Mais, pour optimiser leur potentiel, encore faut-il qu’elles soient partagées entre les différentes parties prenantes de l’entreprise. Pour un hôtel par exemple, inutile de savoir que son client habitué préférera une chambre avec douche plutôt que baignoire et qu’il commandera toujours la même chose pour son petit déjeuner (qu’il préférera livré en chambre), si cette information n’est pas accessible et disponible pour le personnel au contact. Lui seul sera en effet en capacité de délivrer et de mettre en musique un tel service personnalisé. La question de l’organisation des données clients et de sa bonne diffusion dans l’entreprise est ainsi clé ! Et la réussite d’une stratégie de marque à l’appui des données clients est étroitement liée à la performance de ses systèmes d’information.\n\n\\> Quels sont les pièges à éviter ?\n\nSavoir naviguer l’éthique et les réglementations\n\nEn analysant et en exploitant les données collectées, les marques ont une responsabilité sociétale dont elles doivent prendre toute la mesure pour que leur approche des marchés fonctionne sur le long terme. Au-delà des réglementations qui s’appliquent (RGPD en tête en Europe), nombreuses sont celles qui se fixent un cadre déontologique et éthique de règles à suivre ou de limites à ne pas franchir. À laisser les algorithmes décider seuls, les questions éthiques qu’ils soulèvent ne tarderaient pas. Par exemple, l’algorithme sur lequel s’appuient les banquiers pour évaluer le risque qu’un prêt ne soit pas remboursé par son contractant aurait vite fait de considérer qu’un habitant de Seine-Saint-Denis a une probabilité plus forte que les autres de se retrouver en impayé… ce qui pourrait injustement stigmatiser l’ensemble des habitants du secteur. Autre exemple, il est fort probable qu’un algorithme de classification des clients qui aurait pour objectif d’identifier les clients les plus susceptibles d’être intéressés par un bon de réduction pour des sucreries recommande de les adresser à des consommateurs suivant un régime, car leur probabilité d’y succomber est forte (en raison de la frustration liée à leur cure, il leur serait difficile de résister, et ils pourraient saisir l’opportunité de ce bon de réduction inopiné pour légitimer un écart !). Les données ainsi collectées permettent de trouver les « failles » permettant de faire succomber les consommateurs.\n\nMais ces questions vont au-delà des enjeux réglementaires et éthiques. Ce n’est pas par philanthropie que les marques s’intéressent à ces dimensions. Un manque de transparence dans les pratiques pourrait en effet altérer la confiance des clients envers la marque (et on sait toute l’importance de ce levier dans la fidélité !). Il pourrait aussi contribuer au rejet des pratiques par les clients, via un mécanisme de réactance psychologique par lequel un consommateur qui a le sentiment de perdre le contrôle sur ses données personnelles va s’opposer aux marques. Une opacité trop importante des pratiques aurait ainsi l’effet délétère de susciter méfiance et défiance. L’éva­luation de la manière dont sont collectées les données clients conduit à la formation d’une attitude (favorable ou non) face à la communication des données demandées et influence positivement l’intention du consommateur de poursuivre la relation avec l’entreprise.\n\nNe pas confondre volume et qualité des données"
        }
      },
      {
        "json": {
          "pageId": "PAGE139_1757157753085",
          "text": "La grande nouveauté des stratégies clients modernes trouve sans doute sa source dans la capacité inédite des entreprises à organiser et à structurer leurs données clients. Mais, s’il est une chose de collecter des données clients, savoir les trier, les consolider et les rattacher aux « bons » clients en est une autre. Collecter les données clients tous azimuts peut être à double tranchant : plus le volume de données recueillies est important, plus il est complexe de repérer à l’intérieur l’information utile, qui permettra un meilleur service rendu aux clients et une plus grande création de valeur pour l’entreprise. Confondre volume et qualité des données est ainsi un écueil duquel il est essentiel de se préserver.\n\n\\> Votre recommandation personnelle pour les data pionniers ?\n\nLes entreprises investissent des sommes et des efforts considérables au service d’une meilleure connaissance client, permettant une création de valeur plus importante pour les utilisateurs. Mais, lorsque le consommateur ne se sent pas justement valorisé du fait de cette meilleure connaissance de son profil ou qu’il se sent mis sous pression par des sollicitations commerciales disproportionnées, celui-ci peut ressentir un malaise psychologique qui va à l’encontre d’un objectif de niveau supérieur à la vente transactionnelle simple et court-termiste : construire des relations harmonieuses entre les consommateurs et les entreprises qui les servent.\n\nAu final, c’est en (re)connaissant le consommateur sous toutes ses formes que des relations harmonieuses et durables pourront être construites de concert avec lui. Protéger les données des consommateurs, c’est préserver la croissance et poser le socle de relations saines, durables, justes et équitables. Agir de manière plus responsable en matière de connaissance client permettrait également de ne pas trop aiguiser l’intérêt du législateur sur ces questions. Sur­endettement, addiction aux jeux, pratiques de ventes forcées ou encore clauses abusives, le législateur s’est déjà saisi de nombreux sujets dans une optique de défense des utilisateurs. Face à l’intérêt grandissant du législateur sur ces questions, il est aussi dans l’intérêt des entreprises de faire preuve de responsabilité et de modération sur la question. Au final, en matière de connaissance client, il faut sans doute réussir à faire moins, mais mieux !\n\n------------------------------------------------------------------------\n\n24. Cette matrice est inspirée des travaux de Toscane Accom­pa­gnement, de Frédéric Laloux et de Ken Wilber.\n\n25. Black mirror est une série télévisée interrogeant les conséquences possibles des nouvelles technologies.\n\n26. Azhar A. Technology is too important to be left to technologists, SiliconRepublic, mars 2019.\n\n27. De l’anglais man, homme, et explaining, explication, ce concept désigne une situation où un homme explique à une femme quel­que chose qu’elle sait déjà, voire dont elle est experte, sur un ton condescendant.\n\n28. Madeleine Albright est diplomate, femme politique et femme d’affaires américaine, ambassadrice américaine auprès des Nations unies de 1993 à 1997, puis secrétaire d’État des États-Unis entre 1997 et 2001 dans l’administration du président Bill Clinton.\n\n29. Maynard P., Sommes-nous vraiment en train de combler l’écart entre les sexes dans la technologie ?, Forbes, mars 2021. Resetting Tech Culture 5 strategies to keep women in tech, Accenture – Girls Who Code.\n\n30. Paloma Median est une experte de l’équité et de l’inclusion, avec une expérience dans la Tech (Etsy, Digital Ocean, and Squarespace) et dans les ONG.\n\nRemerciements\n\nÀ Jan et Paul pour leurs illustrations.\n\nÀ Nolwenn pour la supervision des entretiens des contributeurs de cet ouvrage.\n\nAux éditions Mardaga pour avoir cru en ce projet.\n\nÀ Pierre Desproges pour l’inspiration humoristique de la photo récurrente.\n\nÀ propos de l’auteur\n\n« Quand on me demande pourquoi j’ai cette passion pour la data, je réponds qu’au-delà de l’inspiration de mes parents et de mes mentors, c’est l’attraction pour l’apprentissage : toujours apprendre et découvrir. C’est ce que la Berkeley Haas School of Business définit comme rester Student Always. Et c’est aussi ce que je retrouve dans mes autres passions : le surf, l’astrophotographie, la mixologie (et finalement toutes les disciplines que l’on pratique avec engagement) sont des domaines où chaque jour, chaque nuit me confrontent à de nouveaux challenges, à des erreurs et à des découvertes. Je vis pour cette vie riche et surprenante, même si elle me tire souvent de ma zone de confort. »\n\nGauthier Vasseur a commencé sa carrière en audit, finance d’entreprise et trésorerie, et a relevé de nombreux défis avec ses équipes : mise en conformité à de nouvelles normes internationales comptables, passage à l’an 2000, à l’euro, gestion de crise, agilité et performance de processus d’analyse et de reporting. Mais c’est la maîtrise de la data qui a créé un déclic, lorsque, au début des années 2000, il a compris que l’on pouvait travailler plus vite et surtout plus justement grâce à des techniques simples. Il se lance alors avec l’AFTE, l’Association française des trésoriers d’entreprises, dans le développement de cours et de conférences sur les systèmes d’information en Finance. En 2004, il bascule dans le monde de la technologie : directeur marketing produit pour les solutions de gestion de la performance chez Hyperion en Silicon Valley, racheté plus tard par Oracle ; en 2007, directeur de la gestion de la data financière chez Google ; puis vice-président des solutions data de TriNet, à la veille de son entrée en Bourse ; enfin Chief Operating Officer de la filiale américaine de Semarchy. Le dernier tournant de sa carrière est amorcé dès 2008 avec ses premiers cours data à Stanford. C’est ainsi qu’il s’engage définitivement dans la voie de la formation et crée en 2017 Data Wise Academy à Menlo Park en Californie. En 2018, il rejoint l’Université de Berkeley et prend la direction du Fisher Center for Business Analytics à la Haas School of Business. Il se concentre désormais à la recherche et à l’enseignement académique et professionnel : projets entrepreneuriaux avec Le Pont, un leader de la formation Data/IA en France, missions d’inclusion et d’accès à la formation, et création en 2018 de la Berkeley Alliance for Inclusive AI dont il est le coprésident avec Nolwenn Godard.\n\nCrédits photographiques\n\nPages 17, 18, 22, 29, 53, 65, 93, 119, 125, 172, 207, 222, 225, 231, 249, 275, 279 : Jean-Paul Laurens, L’Empereur Maximilien Ier du Mexique avant son exécution (1882), Wikimedia Commons, domaine public.\n\nPage 115 : Free material from [www.gapminder.org](http://www.gapminder.org)\n\nPage 130 : Peter Kaminski, Wikimedia Commons, CC BY 2.0.\n\nPage 325 : Jenny Freiermuth, Ford Photography.\n\n© 2021 Éditions Mardaga\n\nB-1160 Bruxelles (Belgique)\n\n[www.editionsmardaga.com](http://www.editionsmardaga.com)\n\nCoordination éditoriale : Véronique Dupas et Laura Vanham  \nMaquette intérieure : Carine Thurion\n\nDesign de couverture : Emmanuel Bonaffini\n\nDépôt légal : 2021/0024/85  \nISBN numérique : 9782804720339\n\nToute reproduction ou représentation intégrale ou partielle, par quelque procédé que ce soit, du présent ouvrage est strictement interdite."
        }
      }
    ],
    "When clicking ‘Execute workflow’": [
      {
        "json": {}
      }
    ],
    "List files": [
      {
        "json": {
          "name": "Epub to md.ipynb",
          "path": "data pionnier/Epub to md.ipynb",
          "sha": "7162f96ddadd4fcbdcf346bc8f93c53b1b18a341",
          "size": 9392,
          "url": "https://api.github.com/repos/gauthiervasseur/Md_Books/contents/data%20pionnier/Epub%20to%20md.ipynb?ref=main",
          "html_url": "https://github.com/gauthiervasseur/Md_Books/blob/main/data%20pionnier/Epub%20to%20md.ipynb",
          "git_url": "https://api.github.com/repos/gauthiervasseur/Md_Books/git/blobs/7162f96ddadd4fcbdcf346bc8f93c53b1b18a341",
          "download_url": "https://raw.githubusercontent.com/gauthiervasseur/Md_Books/main/data%20pionnier/Epub%20to%20md.ipynb?token=BM726QQGNKDIUSGR6YDHEGTIXQKQW",
          "type": "file",
          "_links": {
            "self": "https://api.github.com/repos/gauthiervasseur/Md_Books/contents/data%20pionnier/Epub%20to%20md.ipynb?ref=main",
            "git": "https://api.github.com/repos/gauthiervasseur/Md_Books/git/blobs/7162f96ddadd4fcbdcf346bc8f93c53b1b18a341",
            "html": "https://github.com/gauthiervasseur/Md_Books/blob/main/data%20pionnier/Epub%20to%20md.ipynb"
          }
        }
      },
      {
        "json": {
          "name": "Vasseur_Data pionnier.epub",
          "path": "data pionnier/Vasseur_Data pionnier.epub",
          "sha": "e0febdeef81a032f915e4956c9362f5fb913b978",
          "size": 26396791,
          "url": "https://api.github.com/repos/gauthiervasseur/Md_Books/contents/data%20pionnier/Vasseur_Data%20pionnier.epub?ref=main",
          "html_url": "https://github.com/gauthiervasseur/Md_Books/blob/main/data%20pionnier/Vasseur_Data%20pionnier.epub",
          "git_url": "https://api.github.com/repos/gauthiervasseur/Md_Books/git/blobs/e0febdeef81a032f915e4956c9362f5fb913b978",
          "download_url": "https://raw.githubusercontent.com/gauthiervasseur/Md_Books/main/data%20pionnier/Vasseur_Data%20pionnier.epub?token=BM726QWY54EIMTYUKHHAE7LIXQKQW",
          "type": "file",
          "_links": {
            "self": "https://api.github.com/repos/gauthiervasseur/Md_Books/contents/data%20pionnier/Vasseur_Data%20pionnier.epub?ref=main",
            "git": "https://api.github.com/repos/gauthiervasseur/Md_Books/git/blobs/e0febdeef81a032f915e4956c9362f5fb913b978",
            "html": "https://github.com/gauthiervasseur/Md_Books/blob/main/data%20pionnier/Vasseur_Data%20pionnier.epub"
          }
        }
      },
      {
        "json": {
          "name": "Vasseur_Data pionnier.md",
          "path": "data pionnier/Vasseur_Data pionnier.md",
          "sha": "5fa6611461e54d603b2efdfa0b10e98a03b3106a",
          "size": 573490,
          "url": "https://api.github.com/repos/gauthiervasseur/Md_Books/contents/data%20pionnier/Vasseur_Data%20pionnier.md?ref=main",
          "html_url": "https://github.com/gauthiervasseur/Md_Books/blob/main/data%20pionnier/Vasseur_Data%20pionnier.md",
          "git_url": "https://api.github.com/repos/gauthiervasseur/Md_Books/git/blobs/5fa6611461e54d603b2efdfa0b10e98a03b3106a",
          "download_url": "https://raw.githubusercontent.com/gauthiervasseur/Md_Books/main/data%20pionnier/Vasseur_Data%20pionnier.md?token=BM726QUUGHQDFP6TAEIXDD3IXQKQW",
          "type": "file",
          "_links": {
            "self": "https://api.github.com/repos/gauthiervasseur/Md_Books/contents/data%20pionnier/Vasseur_Data%20pionnier.md?ref=main",
            "git": "https://api.github.com/repos/gauthiervasseur/Md_Books/git/blobs/5fa6611461e54d603b2efdfa0b10e98a03b3106a",
            "html": "https://github.com/gauthiervasseur/Md_Books/blob/main/data%20pionnier/Vasseur_Data%20pionnier.md"
          }
        }
      },
      {
        "json": {
          "name": "Vasseur_Data pionnier.md.bak",
          "path": "data pionnier/Vasseur_Data pionnier.md.bak",
          "sha": "d9c8dba9722416e6e838b91b36b956216b530612",
          "size": 633849,
          "url": "https://api.github.com/repos/gauthiervasseur/Md_Books/contents/data%20pionnier/Vasseur_Data%20pionnier.md.bak?ref=main",
          "html_url": "https://github.com/gauthiervasseur/Md_Books/blob/main/data%20pionnier/Vasseur_Data%20pionnier.md.bak",
          "git_url": "https://api.github.com/repos/gauthiervasseur/Md_Books/git/blobs/d9c8dba9722416e6e838b91b36b956216b530612",
          "download_url": "https://raw.githubusercontent.com/gauthiervasseur/Md_Books/main/data%20pionnier/Vasseur_Data%20pionnier.md.bak?token=BM726QWOC4E2NVPVZKPXD43IXQKQW",
          "type": "file",
          "_links": {
            "self": "https://api.github.com/repos/gauthiervasseur/Md_Books/contents/data%20pionnier/Vasseur_Data%20pionnier.md.bak?ref=main",
            "git": "https://api.github.com/repos/gauthiervasseur/Md_Books/git/blobs/d9c8dba9722416e6e838b91b36b956216b530612",
            "html": "https://github.com/gauthiervasseur/Md_Books/blob/main/data%20pionnier/Vasseur_Data%20pionnier.md.bak"
          }
        }
      },
      {
        "json": {
          "name": "Vasseur_Data pionnier_media",
          "path": "data pionnier/Vasseur_Data pionnier_media",
          "sha": "38e2453bb0f53d5de8d0e66e17c6bb8bc3ee6af0",
          "size": 0,
          "url": "https://api.github.com/repos/gauthiervasseur/Md_Books/contents/data%20pionnier/Vasseur_Data%20pionnier_media?ref=main",
          "html_url": "https://github.com/gauthiervasseur/Md_Books/tree/main/data%20pionnier/Vasseur_Data%20pionnier_media",
          "git_url": "https://api.github.com/repos/gauthiervasseur/Md_Books/git/trees/38e2453bb0f53d5de8d0e66e17c6bb8bc3ee6af0",
          "download_url": null,
          "type": "dir",
          "_links": {
            "self": "https://api.github.com/repos/gauthiervasseur/Md_Books/contents/data%20pionnier/Vasseur_Data%20pionnier_media?ref=main",
            "git": "https://api.github.com/repos/gauthiervasseur/Md_Books/git/trees/38e2453bb0f53d5de8d0e66e17c6bb8bc3ee6af0",
            "html": "https://github.com/gauthiervasseur/Md_Books/tree/main/data%20pionnier/Vasseur_Data%20pionnier_media"
          }
        }
      }
    ],
    "Get a file": [
      {
        "json": {
          "name": "Vasseur_Data pionnier.md",
          "path": "data pionnier/Vasseur_Data pionnier.md",
          "sha": "5fa6611461e54d603b2efdfa0b10e98a03b3106a",
          "size": 573490,
          "url": "https://api.github.com/repos/gauthiervasseur/Md_Books/contents/data%20pionnier/Vasseur_Data%20pionnier.md?ref=main",
          "html_url": "https://github.com/gauthiervasseur/Md_Books/blob/main/data%20pionnier/Vasseur_Data%20pionnier.md",
          "git_url": "https://api.github.com/repos/gauthiervasseur/Md_Books/git/blobs/5fa6611461e54d603b2efdfa0b10e98a03b3106a",
          "download_url": "https://raw.githubusercontent.com/gauthiervasseur/Md_Books/main/data%20pionnier/Vasseur_Data%20pionnier.md?token=BM726QUUGHQDFP6TAEIXDD3IXQKQW",
          "type": "file",
          "_links": {
            "self": "https://api.github.com/repos/gauthiervasseur/Md_Books/contents/data%20pionnier/Vasseur_Data%20pionnier.md?ref=main",
            "git": "https://api.github.com/repos/gauthiervasseur/Md_Books/git/blobs/5fa6611461e54d603b2efdfa0b10e98a03b3106a",
            "html": "https://github.com/gauthiervasseur/Md_Books/blob/main/data%20pionnier/Vasseur_Data%20pionnier.md"
          }
        }
      }
    ],
    "Filter the book with his Name": [
      {
        "json": {
          "name": "Vasseur_Data pionnier.md",
          "path": "data pionnier/Vasseur_Data pionnier.md",
          "sha": "5fa6611461e54d603b2efdfa0b10e98a03b3106a",
          "size": 573490,
          "url": "https://api.github.com/repos/gauthiervasseur/Md_Books/contents/data%20pionnier/Vasseur_Data%20pionnier.md?ref=main",
          "html_url": "https://github.com/gauthiervasseur/Md_Books/blob/main/data%20pionnier/Vasseur_Data%20pionnier.md",
          "git_url": "https://api.github.com/repos/gauthiervasseur/Md_Books/git/blobs/5fa6611461e54d603b2efdfa0b10e98a03b3106a",
          "download_url": "https://raw.githubusercontent.com/gauthiervasseur/Md_Books/main/data%20pionnier/Vasseur_Data%20pionnier.md?token=BM726QUUGHQDFP6TAEIXDD3IXQKQW",
          "type": "file",
          "_links": {
            "self": "https://api.github.com/repos/gauthiervasseur/Md_Books/contents/data%20pionnier/Vasseur_Data%20pionnier.md?ref=main",
            "git": "https://api.github.com/repos/gauthiervasseur/Md_Books/git/blobs/5fa6611461e54d603b2efdfa0b10e98a03b3106a",
            "html": "https://github.com/gauthiervasseur/Md_Books/blob/main/data%20pionnier/Vasseur_Data%20pionnier.md"
          }
        }
      }
    ]
  },
  "repo": {
    "owner": "abqarimind",
    "name": "n8n-workflow"
  },
  "settings": {
    "executionOrder": "v1"
  },
  "staticData": null,
  "tags": [],
  "triggerCount": 0,
  "updatedAt": "2025-09-06T11:31:48.000Z",
  "versionId": "feedc7a5-dc49-4485-a914-4c480a15fe9e"
}